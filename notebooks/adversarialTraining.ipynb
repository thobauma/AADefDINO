{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchattacks\n",
    "from torchattacks import *\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "# from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino, ViTWrapper\n",
    "from src.model.data import *\n",
    "from src.model.train import *\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "MAX_PATH = Path('/','cluster', 'scratch', 'mmathys', 'dl_data')\n",
    "\n",
    "LOG_BASE_PATH = Path(MAX_PATH, 'logs')\n",
    "TB_LOGS_PATH = Path(LOG_BASE_PATH, 'tb_logs')\n",
    "\n",
    "# DamageNet\n",
    "DN_PATH = Path(DATA_PATH, 'damageNet')\n",
    "DN_LABEL_PATH = Path(DN_PATH, 'val_damagenet.txt')\n",
    "DN_IMAGES_PATH = Path(DN_PATH, 'images')\n",
    "\n",
    "# Image Net\n",
    "ORI_PATH = Path(DATA_PATH, 'ori_data')\n",
    "CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "\n",
    "VAL_PATH = Path(ORI_PATH, 'validation')\n",
    "VAL_IMAGES_PATH = Path(VAL_PATH,'images')\n",
    "VAL_LABEL_PATH = Path(VAL_PATH, 'correct_labels.txt')\n",
    "\n",
    "TRAIN_PATH = Path(ORI_PATH, 'train')\n",
    "TRAIN_IMAGES_PATH = Path(TRAIN_PATH,'images')\n",
    "TRAIN_LABEL_PATH = Path(TRAIN_PATH, 'correct_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If CLASS_SUBSET is specified, INDEX_SUBSET will be ignored. Set CLASS_SUBSET=None if you want to use indexes.\n",
    "# INDEX_SUBSET = get_random_indexes(number_of_images = 50000, n_samples=1000)\n",
    "# CLASS_SUBSET = get_random_classes(number_of_classes = 10, min_rand_class = 1, max_rand_class = 1001)\n",
    "\n",
    "\n",
    "CLASS_SUBSET = np.load(CLASS_SUBSET_PATH)\n",
    "\n",
    "NUM_WORKERS= 0\n",
    "PIN_MEMORY=True\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_SUBSET = CLASS_SUBSET[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SummaryWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Create a SummaryWriter instance\n",
    "# SummaryWriter writes event files to log_dir\n",
    "writer = SummaryWriter(TB_LOGS_PATH)\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:      3872\n",
      "validation:  150\n"
     ]
    }
   ],
   "source": [
    "# Remember to set the correct transformation\n",
    "\n",
    "train_dataset = AdvTrainingImageDataset(TRAIN_IMAGES_PATH, TRAIN_LABEL_PATH, ADVERSARIAL_TRAINING_TRANSFORM, CLASS_SUBSET, index_subset=None)\n",
    "val_dataset = AdvTrainingImageDataset(VAL_IMAGES_PATH, VAL_LABEL_PATH, ORIGINAL_TRANSFORM, CLASS_SUBSET, index_subset=None)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=False)\n",
    "\n",
    "print(f'train:      {len(train_dataset)}\\nvalidation:  {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Model, Classifier\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights.\n"
     ]
    }
   ],
   "source": [
    "model, base_linear_classifier = get_dino()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and load pretrained weights for linear classifier on ImageNet\n",
    "from torch import nn\n",
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000, hidden_size=512):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(dim, hidden_size) \n",
    "        self.linear2 = nn.Linear(hidden_size, num_labels) \n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear2.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear2.bias.data.zero_()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # linear layer\n",
    "        x = self.relu(self.linear(x))\n",
    "        return self.linear2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier = LinearClassifier(base_linear_classifier.linear.in_features, num_labels=len(CLASS_SUBSET))\n",
    "linear_classifier = linear_classifier.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vits = ViTWrapper(model, linear_classifier, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attack used for adversarial training\n",
    "train_attack = PGD(vits, eps=0.3, alpha=2/255, steps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = Path(LOG_BASE_PATH, 'pgd','03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/242]  eta: 0:11:29  lr: 0.000063  loss: 3.358485 (3.358485)  time: 2.849257  data: 0.764331  max mem: 1407\n",
      "Epoch: [0]  [ 20/242]  eta: 0:09:15  lr: 0.000063  loss: 2.586604 (2.658992)  time: 2.484110  data: 0.533290  max mem: 1431\n",
      "Epoch: [0]  [ 40/242]  eta: 0:08:24  lr: 0.000063  loss: 1.787498 (2.233511)  time: 2.494883  data: 0.534228  max mem: 1431\n",
      "Epoch: [0]  [ 60/242]  eta: 0:07:34  lr: 0.000063  loss: 1.598646 (2.025351)  time: 2.496371  data: 0.530708  max mem: 1431\n",
      "Epoch: [0]  [ 80/242]  eta: 0:06:44  lr: 0.000063  loss: 1.466795 (1.889623)  time: 2.494945  data: 0.527911  max mem: 1431\n",
      "Epoch: [0]  [100/242]  eta: 0:05:54  lr: 0.000063  loss: 1.403662 (1.791615)  time: 2.492798  data: 0.525898  max mem: 1431\n",
      "Epoch: [0]  [120/242]  eta: 0:05:04  lr: 0.000063  loss: 1.342546 (1.718228)  time: 2.491139  data: 0.524081  max mem: 1431\n",
      "Epoch: [0]  [140/242]  eta: 0:04:14  lr: 0.000063  loss: 1.295247 (1.657803)  time: 2.499468  data: 0.532859  max mem: 1431\n",
      "Epoch: [0]  [160/242]  eta: 0:03:24  lr: 0.000063  loss: 1.250396 (1.607489)  time: 2.497459  data: 0.531061  max mem: 1431\n",
      "Epoch: [0]  [180/242]  eta: 0:02:34  lr: 0.000063  loss: 1.221807 (1.564457)  time: 2.519520  data: 0.553233  max mem: 1431\n",
      "Epoch: [0]  [200/242]  eta: 0:01:44  lr: 0.000063  loss: 1.197404 (1.527763)  time: 2.500124  data: 0.533623  max mem: 1431\n",
      "Epoch: [0]  [220/242]  eta: 0:00:54  lr: 0.000063  loss: 1.169809 (1.495556)  time: 2.504775  data: 0.538572  max mem: 1431\n",
      "Epoch: [0]  [240/242]  eta: 0:00:04  lr: 0.000063  loss: 1.157444 (1.467644)  time: 2.495195  data: 0.529629  max mem: 1431\n",
      "Epoch: [0]  [241/242]  eta: 0:00:02  lr: 0.000063  loss: 1.156301 (1.466307)  time: 2.497206  data: 0.531515  max mem: 1431\n",
      "Epoch: [0] Total time: 0:10:04 (2.499284 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 1.156301 (1.466307)\n",
      "Test:  [ 0/10]  eta: 0:00:25  loss: 1.079725 (1.079725)  acc1: 75.000000 (75.000000)  adv_loss: 1.146046 (1.146046)  adv_acc1: 0.000000 (0.000000)  time: 2.557616  data: 0.546483  max mem: 1441\n",
      "Test:  [ 9/10]  eta: 0:00:02  loss: 1.079725 (1.080577)  acc1: 75.000000 (79.333334)  adv_loss: 1.148660 (1.150700)  adv_acc1: 0.000000 (0.666667)  time: 2.411361  data: 0.513673  max mem: 1441\n",
      "Test: Total time: 0:00:24 (2.411678 s / it)\n",
      "* Acc@1 79.333 loss 1.081\n",
      "* adv_Acc@1 0.667 adv_loss 1.151\n",
      "Accuracy at epoch 0 of the network on the 10 test images: 79.3%\n",
      "Max accuracy so far: 79.33%\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/cluster/scratch/mmathys/dl_data/logs/pgd/03/log.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-51ce26f1cbf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m loggers = train(model, \n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0mlinear_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mLOG_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AADefDINO/src/model/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, classifier, train_loader, validation_loader, log_dir, tensor_dir, optimizer, adversarial_attack, epochs, val_freq, batch_size, lr, to_restore, n, avgpool_patchtokens, writer)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_main_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"log.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_stats\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             save_dict = {\n",
      "\u001b[0;32m/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/lib64/python3.8/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1218\u001b[0;31m         return io.open(self, mode, buffering, encoding, errors, newline,\n\u001b[0m\u001b[1;32m   1219\u001b[0m                        opener=self._opener)\n\u001b[1;32m   1220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/lib64/python3.8/pathlib.py\u001b[0m in \u001b[0;36m_opener\u001b[0;34m(self, name, flags, mode)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0o666\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0;31m# A stub for the opener argument to built-in open()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_raw_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0o777\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/cluster/scratch/mmathys/dl_data/logs/pgd/03/log.txt'"
     ]
    }
   ],
   "source": [
    "loggers = train(model, \n",
    "                linear_classifier,\n",
    "                train_loader,\n",
    "                val_loader, \n",
    "                LOG_PATH, \n",
    "                epochs=2,\n",
    "                adversarial_attack=train_attack,\n",
    "                writer=writer\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_attack = PGD(vits, eps=0.6, alpha=2/255, steps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict, logs = validate_network(model, \n",
    "                linear_classifier, \n",
    "                val_loader,\n",
    "                tensor_dir=None,\n",
    "                adversarial_attack=validation_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, target, name = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = inp.cuda(non_blocking=True)\n",
    "target = target.cuda(non_blocking=True)\n",
    "out = train_attack(inp, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(img, title):\n",
    "    img = img.to('cpu')\n",
    "    npimg = img.numpy()\n",
    "    fig = plt.figure(figsize = (5, 15))\n",
    "    plt.imshow(np.transpose(npimg,(1,2,0)))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(out[0], 'adversarial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(inp[0], 'original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict, logs = validate_network(model, \n",
    "                linear_classifier, \n",
    "                val_loader,\n",
    "                tensor_dir=None,\n",
    "                adversarial_attack=train_attack)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms as pth_transforms\n",
    "\n",
    "from dino import utils\n",
    "\n",
    "\n",
    "def train(model, classifier, train_loader, validation_loader, log_dir=None, tensor_dir=None, optimizer=None, adversarial_attack=None, epochs=5, val_freq=1, batch_size=16,  lr=0.001, to_restore = {\"epoch\": 0, \"best_acc\": 0.}, n=4, avgpool_patchtokens=False):\n",
    "    \"\"\" Trains a classifier ontop of a base model. The input can be perturbed by selecting an adversarial attack.\n",
    "        \n",
    "        :param model: base model (frozen)\n",
    "        :param classifier: classifier to train\n",
    "        :param train_loader: dataloader of the train dataset\n",
    "        :param validation_loader: dataloader of the validation dataset\n",
    "        :param log_dir: path to the log directory.\n",
    "        :param tensor_dir: if set saves the output of the model in the dir\n",
    "        :param optimizer: optimizer for the training process. Default: None -> uses the SGD as defined by DINO.\n",
    "        :param adversarial_attack: adversarial attack for adversarial training. Default: None -> the classifier is trained without adversarial perturbation.\n",
    "        :param epochs: number of epochs to train the classifier on. Default: 5\n",
    "        :param val_freq: frequency (in epochs) in which the classifier is validated.\n",
    "        :param batch_size: batch_size for training and validation. Default: 16\n",
    "        :param lr: the learning rate of the optimizer if the DINO optimizer is used. Default: 0.001\n",
    "        :param to_restore:\n",
    "        :param n: from DINO. Default: 4\n",
    "        :param avgpool_patchtokens: from DINO. Default: False\n",
    "        \n",
    "    \"\"\"\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.SGD(\n",
    "            classifier.parameters(),\n",
    "            lr * (batch_size * utils.get_world_size()) / 256., # linear scaling rule\n",
    "            momentum=0.9,\n",
    "            weight_decay=0, # we do not apply weight decay\n",
    "        )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, eta_min=0)\n",
    "    \n",
    "    # Optionally resume from a checkpoint\n",
    "    utils.restart_from_checkpoint(\n",
    "        Path(log_dir, \"checkpoint.pth.tar\"),\n",
    "        run_variables=to_restore,\n",
    "        state_dict=classifier,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "    start_epoch = to_restore[\"epoch\"]\n",
    "    best_acc = to_restore[\"best_acc\"]\n",
    "    \n",
    "    # train loop\n",
    "    loggers = {'train':[], 'validation':[]}\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        if 'set_epoch' in dir(train_loader.sampler):\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        # train epoch\n",
    "        train_stats, metric_logger = train_epoch(model, classifier, optimizer, train_loader, tensor_dir, adversarial_attack, epoch, n, avgpool_patchtokens)\n",
    "        loggers['train'].append(metric_logger)\n",
    "        scheduler.step()\n",
    "\n",
    "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                     'epoch': epoch}\n",
    "        \n",
    "        # validate\n",
    "        if epoch % val_freq == 0 or epoch == epochs - 1:\n",
    "            test_stats, metric_logger = validate_network(model, classifier, validation_loader, tensor_dir, adversarial_attack, n, avgpool_patchtokens)\n",
    "            loggers['validation'].append(metric_logger)\n",
    "            print(f\"Accuracy at epoch {epoch} of the network on the {len(validation_loader)} test images: {test_stats['acc1']:.1f}%\")\n",
    "            best_acc = max(best_acc, test_stats[\"acc1\"])\n",
    "            print(f'Max accuracy so far: {best_acc:.2f}%')\n",
    "            log_stats = {**{k: v for k, v in log_stats.items()},\n",
    "                         **{f'test_{k}': v for k, v in test_stats.items()}}\n",
    "        # log\n",
    "        if utils.is_main_process():\n",
    "            with (Path(log_dir) / \"log.txt\").open(\"a\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "            save_dict = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"state_dict\": classifier.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "                \"best_acc\": best_acc,\n",
    "            }\n",
    "            torch.save(save_dict, Path(log_dir, \"checkpoint.pth.tar\"))\n",
    "    print(\"Training of the supervised linear classifier on frozen features completed.\\n\"\n",
    "                \"Top-1 test accuracy: {acc:.1f}\".format(acc=best_acc))\n",
    "    return loggers\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def train_epoch(model, classifier, optimizer, train_loader, tensor_dir=None, adversarial_attack=None, epoch=0, n=4, avgpool=False):\n",
    "    \"\"\" Trains a classifier ontop of a base model. The input can be perturbed by selecting an adversarial attack.\n",
    "        \n",
    "        :param model: base model (frozen)\n",
    "        :param classifier: classifier to train\n",
    "        :param optimizer: optimizer for the training process.\n",
    "        :param train_loader: dataloader of the train dataset\n",
    "        :param tensor_dir: if set saves the output of the model in the dir\n",
    "\n",
    "        :param adversarial_attack: adversarial attack for adversarial training. Default: None -> the classifier is trained without adversarial perturbation.\n",
    "        :param epochs: The current epch\n",
    "        :param n: from DINO. Default: 4\n",
    "        :param avgpool_patchtokens: from DINO. Default: False\n",
    "        \n",
    "    \"\"\"\n",
    "    classifier.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    for (inp, target, names) in metric_logger.log_every(train_loader, 20, header):\n",
    "        \n",
    "        # move to gpu\n",
    "        inp = inp.cuda(non_blocking=True)\n",
    "        target = target.cuda(non_blocking=True)\n",
    "\n",
    "        # adversarial attack  \n",
    "        if adversarial_attack is not None:\n",
    "            inp = adversarial_attack(inp, target)\n",
    "        \n",
    "        # Normalize\n",
    "        transform = pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        inp = transform(inp)\n",
    "        \n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            if 'get_intermediate_layers' in dir(model):\n",
    "                intermediate_output = model.get_intermediate_layers(inp, n)\n",
    "                output = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
    "                if avgpool:\n",
    "                    output = torch.cat((output.unsqueeze(-1), torch.mean(intermediate_output[-1][:, 1:], dim=1).unsqueeze(-1)), dim=-1)\n",
    "                    output = output.reshape(output.shape[0], -1)\n",
    "            else:\n",
    "                output = model(inp)\n",
    "\n",
    "        # save output      \n",
    "        if tensor_dir is not None and epoch == 0:\n",
    "            save_output_batch(output, names, tensor_dir)\n",
    "        \n",
    "        output = classifier(output)\n",
    "\n",
    "        # compute cross entropy loss\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "\n",
    "        # compute the gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # step\n",
    "        optimizer.step()\n",
    "\n",
    "        # log \n",
    "        torch.cuda.synchronize()\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}, metric_logger\n",
    "\n",
    "\n",
    "def validate_network(model, classifier, validation_loader, tensor_dir=None, adversarial_attack=None, n=4, avgpool=False):\n",
    "    \"\"\" Validates a classifier\n",
    "        \n",
    "        :param model: base model (frozen)\n",
    "        :param classifier: classifier to train\n",
    "        :param validation_loader: dataloader of the validation dataset\n",
    "        :param tensor_dir: if set saves the output of the model in the dir\n",
    "        :param adversarial_attack: adversarial attack for adversarial training. Default: None -> the classifier is trained without adversarial perturbation.\n",
    "        :param n: from DINO. Default: 4\n",
    "        :param avgpool_patchtokens: from DINO. Default: False\n",
    "        \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Test:'\n",
    "    if 'num_labels' in dir(classifier):\n",
    "        num_labels = classifier.num_labels\n",
    "    else:\n",
    "        num_labels = classifier.module.num_labels\n",
    "    for inp, target, names in metric_logger.log_every(validation_loader, 20, header):\n",
    "\n",
    "        # move to gpu\n",
    "        inp = inp.cuda(non_blocking=True)\n",
    "        target = target.cuda(non_blocking=True) \n",
    "\n",
    "        # Normalize\n",
    "        transform = pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        norminp = transform(inp)  \n",
    "        \n",
    "        # benign\n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            if 'get_intermediate_layers' in dir(model):\n",
    "                intermediate_output = model.get_intermediate_layers(norminp, n)\n",
    "                output = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
    "                if avgpool:\n",
    "                    output = torch.cat((output.unsqueeze(-1), torch.mean(intermediate_output[-1][:, 1:], dim=1).unsqueeze(-1)), dim=-1)\n",
    "                    output = output.reshape(output.shape[0], -1)\n",
    "            else:\n",
    "                output = model(norminp)\n",
    "                \n",
    "            # save output\n",
    "            if tensor_dir is not None:\n",
    "                save_output_batch(output, names, tensor_dir)\n",
    "\n",
    "            output = classifier(output)\n",
    "            loss = nn.CrossEntropyLoss()(output, target)\n",
    "        \n",
    "        if num_labels >= 5:\n",
    "            acc1, acc5 = utils.accuracy(output, target, topk=(1, 5))\n",
    "        else:\n",
    "            acc1, = utils.accuracy(output, target, topk=(1,))\n",
    "\n",
    "        batch_size = inp.shape[0]\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n",
    "        if num_labels >= 5:\n",
    "            metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n",
    "        \n",
    "        # adversarial attack\n",
    "        if adversarial_attack is not None:\n",
    "            inp = adversarial_attack(inp, target)\n",
    "\n",
    "            # Normalize\n",
    "            transform = pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "            inp = transform(inp)  \n",
    "\n",
    "            # forward\n",
    "            with torch.no_grad():\n",
    "                if 'get_intermediate_layers' in dir(model):\n",
    "                    intermediate_output = model.get_intermediate_layers(inp, n)\n",
    "                    output = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
    "                    if avgpool:\n",
    "                        output = torch.cat((output.unsqueeze(-1), torch.mean(intermediate_output[-1][:, 1:], dim=1).unsqueeze(-1)), dim=-1)\n",
    "                        output = output.reshape(output.shape[0], -1)\n",
    "                else:\n",
    "                    output = model(inp)\n",
    "\n",
    "                # save output\n",
    "                if tensor_dir is not None:\n",
    "                    save_output_batch(output, names, tensor_dir)\n",
    "\n",
    "                output = classifier(output)\n",
    "                adv_loss = nn.CrossEntropyLoss()(output, target)\n",
    "\n",
    "            if num_labels >= 5:\n",
    "                adv_acc1, adv_acc5 = utils.accuracy(output, target, topk=(1, 5))\n",
    "            else:\n",
    "                adv_acc1, = utils.accuracy(output, target, topk=(1,))\n",
    "\n",
    "            batch_size = inp.shape[0]\n",
    "            metric_logger.update(adv_loss=adv_loss.item())\n",
    "            metric_logger.meters['adv_acc1'].update(adv_acc1.item(), n=batch_size)\n",
    "            if num_labels >= 5:\n",
    "                metric_logger.meters['adv_acc5'].update(adv_acc5.item(), n=batch_size)\n",
    "                \n",
    "    if num_labels >= 5:\n",
    "        print('* Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f} loss {losses.global_avg:.3f}'\n",
    "          .format(top1=metric_logger.acc1, top5=metric_logger.acc5, losses=metric_logger.loss))\n",
    "        if adversarial_attack is not None:\n",
    "            print('* adv_Acc@1 {top1.global_avg:.3f} adv_Acc@5 {top5.global_avg:.3f} adv_loss {losses.global_avg:.3f}'\n",
    "          .format(top1=metric_logger.adv_acc1, top5=metric_logger.adv_acc5, losses=metric_logger.adv_loss))\n",
    "    else:\n",
    "        print('* Acc@1 {top1.global_avg:.3f} loss {losses.global_avg:.3f}'\n",
    "          .format(top1=metric_logger.acc1, losses=metric_logger.loss))\n",
    "        if adversarial_attack is not None:\n",
    "            print('* adv_Acc@1 {top1.global_avg:.3f} adv_loss {losses.global_avg:.3f}'\n",
    "          .format(top1=metric_logger.adv_acc1, losses=metric_logger.adv_loss))\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}, metric_logger\n",
    "\n",
    "\n",
    "\n",
    "def save_output_batch(batch_out, batch_names, output_dir):\n",
    "    for out, name in zip(batch_out, batch_names):\n",
    "        torch.save(out, Path(output_dir,name.split('.')[0]+'.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
