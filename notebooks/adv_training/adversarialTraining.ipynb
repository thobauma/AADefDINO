{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchattacks\n",
    "from torchattacks import *\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "# from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino, ViTWrapper\n",
    "from src.model.data import *\n",
    "from src.model.train import *\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "MAX_PATH = Path('/','cluster', 'scratch', 'mmathys', 'dl_data')\n",
    "\n",
    "LOG_BASE_PATH = Path(MAX_PATH, 'logs')\n",
    "TB_LOGS_PATH = Path(LOG_BASE_PATH, 'tb_logs')\n",
    "\n",
    "# DamageNet\n",
    "DN_PATH = Path(DATA_PATH, 'damageNet')\n",
    "DN_LABEL_PATH = Path(DN_PATH, 'val_damagenet.txt')\n",
    "DN_IMAGES_PATH = Path(DN_PATH, 'images')\n",
    "\n",
    "# Image Net\n",
    "ORI_PATH = Path(DATA_PATH, 'ori_data')\n",
    "CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "\n",
    "VAL_PATH = Path(ORI_PATH, 'validation')\n",
    "VAL_IMAGES_PATH = Path(VAL_PATH,'images')\n",
    "VAL_LABEL_PATH = Path(VAL_PATH, 'correct_labels.txt')\n",
    "\n",
    "TRAIN_PATH = Path(ORI_PATH, 'train')\n",
    "TRAIN_IMAGES_PATH = Path(TRAIN_PATH,'images')\n",
    "TRAIN_LABEL_PATH = Path(TRAIN_PATH, 'correct_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If CLASS_SUBSET is specified, INDEX_SUBSET will be ignored. Set CLASS_SUBSET=None if you want to use indexes.\n",
    "# INDEX_SUBSET = get_random_indexes(number_of_images = 50000, n_samples=1000)\n",
    "# CLASS_SUBSET = get_random_classes(number_of_classes = 10, min_rand_class = 1, max_rand_class = 1001)\n",
    "\n",
    "\n",
    "CLASS_SUBSET = np.load(CLASS_SUBSET_PATH)\n",
    "\n",
    "NUM_WORKERS= 0\n",
    "PIN_MEMORY=True\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_SUBSET = CLASS_SUBSET[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SummaryWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Create a SummaryWriter instance\n",
    "# SummaryWriter writes event files to log_dir\n",
    "writer = SummaryWriter(TB_LOGS_PATH)\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:      3872\n",
      "validation:  150\n"
     ]
    }
   ],
   "source": [
    "# Remember to set the correct transformation\n",
    "\n",
    "train_dataset = AdvTrainingImageDataset(TRAIN_IMAGES_PATH, TRAIN_LABEL_PATH, ADVERSARIAL_TRAINING_TRANSFORM, CLASS_SUBSET, index_subset=None)\n",
    "val_dataset = AdvTrainingImageDataset(VAL_IMAGES_PATH, VAL_LABEL_PATH, ORIGINAL_TRANSFORM, CLASS_SUBSET, index_subset=None)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=False)\n",
    "\n",
    "print(f'train:      {len(train_dataset)}\\nvalidation:  {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Model, Classifier\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights.\n"
     ]
    }
   ],
   "source": [
    "model, base_linear_classifier = get_dino()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and load pretrained weights for linear classifier on ImageNet\n",
    "from torch import nn\n",
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000, hidden_size=512):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.linear1 = nn.Linear(dim, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, int(hidden_size / 2))\n",
    "        self.linear3 = nn.Linear(int(hidden_size / 2), int(hidden_size / 4))\n",
    "        self.linear4 = nn.Linear(int(hidden_size / 4), num_labels)\n",
    "#         self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "#         self.linear.bias.data.zero_()\n",
    "#         self.linear2.weight.data.normal_(mean=0.0, std=0.01)\n",
    "#         self.linear2.bias.data.zero_()\n",
    "#         self.initialize()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def initialize(self):\n",
    "        nn.init.normal_(self.linear1.weight, mean=0, std=1.0)\n",
    "        nn.init.normal_(self.linear1.bias, mean=0, std=1.0)\n",
    "        nn.init.normal_(self.linear2.weight, mean=0, std=1.0)\n",
    "        nn.init.normal_(self.linear2.bias, mean=0, std=1.0)\n",
    "        nn.init.normal_(self.linear3.weight, mean=0, std=1.0)\n",
    "        nn.init.normal_(self.linear3.bias, mean=0, std=1.0)\n",
    "        nn.init.normal_(self.linear4.weight, mean=0, std=1.0)\n",
    "        nn.init.normal_(self.linear4.bias, mean=0, std=1.0)\n",
    "        # nn.init.xavier_uniform(self.linear.weight.data)\n",
    "        # self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # linear layer\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        \n",
    "        return self.linear4(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier = LinearClassifier(base_linear_classifier.linear.in_features, num_labels=len(CLASS_SUBSET), hidden_size=2048)\n",
    "linear_classifier = linear_classifier.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vits = ViTWrapper(model, linear_classifier, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attack used for adversarial training\n",
    "train_attack = PGD(vits, eps=0.3, alpha=6/255, steps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = Path(LOG_BASE_PATH, 'pgd','03_nasib_v2')\n",
    "if not os.path.isdir(LOG_PATH):\n",
    "    os.mkdir(LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 30 20:25:00 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 27%   29C    P2    37W / 180W |    923MiB /  8119MiB |      0%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 1080    Off  | 00000000:06:00.0 Off |                  N/A |\n",
      "| 27%   29C    P8     5W / 180W |      2MiB /  8119MiB |      0%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 1080    Off  | 00000000:07:00.0 Off |                  N/A |\n",
      "| 27%   28C    P8     5W / 180W |      2MiB /  8119MiB |      0%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 1080    Off  | 00000000:0F:00.0 Off |                  N/A |\n",
      "| 28%   32C    P8     6W / 180W |      2MiB /  8119MiB |      0%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     11598      C   ....8.5/x86_64/bin/python3.8      921MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found checkpoint at /cluster/scratch/mmathys/dl_data/logs/pgd/03_nasib_v2/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/mmathys/dl_data/logs/pgd/03_nasib_v2/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      "=> loaded 'optimizer' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/pgd/03_nasib_v2/checkpoint.pth.tar'\n",
      "=> loaded 'scheduler' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/pgd/03_nasib_v2/checkpoint.pth.tar'\n",
      "Training of the supervised linear classifier on frozen features completed.\n",
      "Top-1 test accuracy: 87.3\n"
     ]
    }
   ],
   "source": [
    "loggers = train(model, \n",
    "                linear_classifier,\n",
    "                train_loader,\n",
    "                val_loader, \n",
    "                LOG_PATH, \n",
    "                epochs=20,\n",
    "                adversarial_attack=train_attack,\n",
    "                writer=writer\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_attack = PGD(vits, eps=0.6, alpha=2/255, steps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict, logs = validate_network(model, \n",
    "                linear_classifier, \n",
    "                val_loader,\n",
    "                tensor_dir=None,\n",
    "                adversarial_attack=validation_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, target, name = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = inp.cuda(non_blocking=True)\n",
    "target = target.cuda(non_blocking=True)\n",
    "out = train_attack(inp, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(img, title):\n",
    "    img = img.to('cpu')\n",
    "    npimg = img.numpy()\n",
    "    fig = plt.figure(figsize = (5, 15))\n",
    "    plt.imshow(np.transpose(npimg,(1,2,0)))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(out[0], 'adversarial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(inp[0], 'original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict, logs = validate_network(model, \n",
    "                linear_classifier, \n",
    "                val_loader,\n",
    "                tensor_dir=None,\n",
    "                adversarial_attack=train_attack)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms as pth_transforms\n",
    "\n",
    "from dino import utils\n",
    "\n",
    "\n",
    "def train(model, classifier, train_loader, validation_loader, log_dir=None, tensor_dir=None, optimizer=None, adversarial_attack=None, epochs=5, val_freq=1, batch_size=16,  lr=0.001, to_restore = {\"epoch\": 0, \"best_acc\": 0.}, n=4, avgpool_patchtokens=False):\n",
    "    \"\"\" Trains a classifier ontop of a base model. The input can be perturbed by selecting an adversarial attack.\n",
    "        \n",
    "        :param model: base model (frozen)\n",
    "        :param classifier: classifier to train\n",
    "        :param train_loader: dataloader of the train dataset\n",
    "        :param validation_loader: dataloader of the validation dataset\n",
    "        :param log_dir: path to the log directory.\n",
    "        :param tensor_dir: if set saves the output of the model in the dir\n",
    "        :param optimizer: optimizer for the training process. Default: None -> uses the SGD as defined by DINO.\n",
    "        :param adversarial_attack: adversarial attack for adversarial training. Default: None -> the classifier is trained without adversarial perturbation.\n",
    "        :param epochs: number of epochs to train the classifier on. Default: 5\n",
    "        :param val_freq: frequency (in epochs) in which the classifier is validated.\n",
    "        :param batch_size: batch_size for training and validation. Default: 16\n",
    "        :param lr: the learning rate of the optimizer if the DINO optimizer is used. Default: 0.001\n",
    "        :param to_restore:\n",
    "        :param n: from DINO. Default: 4\n",
    "        :param avgpool_patchtokens: from DINO. Default: False\n",
    "        \n",
    "    \"\"\"\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.SGD(\n",
    "            classifier.parameters(),\n",
    "            lr * (batch_size * utils.get_world_size()) / 256., # linear scaling rule\n",
    "            momentum=0.9,\n",
    "            weight_decay=0, # we do not apply weight decay\n",
    "        )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, eta_min=0)\n",
    "    \n",
    "    # Optionally resume from a checkpoint\n",
    "    utils.restart_from_checkpoint(\n",
    "        Path(log_dir, \"checkpoint.pth.tar\"),\n",
    "        run_variables=to_restore,\n",
    "        state_dict=classifier,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "    start_epoch = to_restore[\"epoch\"]\n",
    "    best_acc = to_restore[\"best_acc\"]\n",
    "    \n",
    "    # train loop\n",
    "    loggers = {'train':[], 'validation':[]}\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        if 'set_epoch' in dir(train_loader.sampler):\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        # train epoch\n",
    "        train_stats, metric_logger = train_epoch(model, classifier, optimizer, train_loader, tensor_dir, adversarial_attack, epoch, n, avgpool_patchtokens)\n",
    "        loggers['train'].append(metric_logger)\n",
    "        scheduler.step()\n",
    "\n",
    "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                     'epoch': epoch}\n",
    "        \n",
    "        # validate\n",
    "        if epoch % val_freq == 0 or epoch == epochs - 1:\n",
    "            test_stats, metric_logger = validate_network(model, classifier, validation_loader, tensor_dir, adversarial_attack, n, avgpool_patchtokens)\n",
    "            loggers['validation'].append(metric_logger)\n",
    "            print(f\"Accuracy at epoch {epoch} of the network on the {len(validation_loader)} test images: {test_stats['acc1']:.1f}%\")\n",
    "            best_acc = max(best_acc, test_stats[\"acc1\"])\n",
    "            print(f'Max accuracy so far: {best_acc:.2f}%')\n",
    "            log_stats = {**{k: v for k, v in log_stats.items()},\n",
    "                         **{f'test_{k}': v for k, v in test_stats.items()}}\n",
    "        # log\n",
    "        if utils.is_main_process():\n",
    "            with (Path(log_dir) / \"log.txt\").open(\"a\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "            save_dict = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"state_dict\": classifier.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "                \"best_acc\": best_acc,\n",
    "            }\n",
    "            torch.save(save_dict, Path(log_dir, \"checkpoint.pth.tar\"))\n",
    "    print(\"Training of the supervised linear classifier on frozen features completed.\\n\"\n",
    "                \"Top-1 test accuracy: {acc:.1f}\".format(acc=best_acc))\n",
    "    return loggers\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def train_epoch(model, classifier, optimizer, train_loader, tensor_dir=None, adversarial_attack=None, epoch=0, n=4, avgpool=False):\n",
    "    \"\"\" Trains a classifier ontop of a base model. The input can be perturbed by selecting an adversarial attack.\n",
    "        \n",
    "        :param model: base model (frozen)\n",
    "        :param classifier: classifier to train\n",
    "        :param optimizer: optimizer for the training process.\n",
    "        :param train_loader: dataloader of the train dataset\n",
    "        :param tensor_dir: if set saves the output of the model in the dir\n",
    "\n",
    "        :param adversarial_attack: adversarial attack for adversarial training. Default: None -> the classifier is trained without adversarial perturbation.\n",
    "        :param epochs: The current epch\n",
    "        :param n: from DINO. Default: 4\n",
    "        :param avgpool_patchtokens: from DINO. Default: False\n",
    "        \n",
    "    \"\"\"\n",
    "    classifier.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    for (inp, target, names) in metric_logger.log_every(train_loader, 20, header):\n",
    "        \n",
    "        # move to gpu\n",
    "        inp = inp.cuda(non_blocking=True)\n",
    "        target = target.cuda(non_blocking=True)\n",
    "\n",
    "        # adversarial attack  \n",
    "        if adversarial_attack is not None:\n",
    "            inp = adversarial_attack(inp, target)\n",
    "        \n",
    "        # Normalize\n",
    "        transform = pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        inp = transform(inp)\n",
    "        \n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            if 'get_intermediate_layers' in dir(model):\n",
    "                intermediate_output = model.get_intermediate_layers(inp, n)\n",
    "                output = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
    "                if avgpool:\n",
    "                    output = torch.cat((output.unsqueeze(-1), torch.mean(intermediate_output[-1][:, 1:], dim=1).unsqueeze(-1)), dim=-1)\n",
    "                    output = output.reshape(output.shape[0], -1)\n",
    "            else:\n",
    "                output = model(inp)\n",
    "\n",
    "        # save output      \n",
    "        if tensor_dir is not None and epoch == 0:\n",
    "            save_output_batch(output, names, tensor_dir)\n",
    "        \n",
    "        output = classifier(output)\n",
    "\n",
    "        # compute cross entropy loss\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "\n",
    "        # compute the gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # step\n",
    "        optimizer.step()\n",
    "\n",
    "        # log \n",
    "        torch.cuda.synchronize()\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}, metric_logger\n",
    "\n",
    "\n",
    "def validate_network(model, classifier, validation_loader, tensor_dir=None, adversarial_attack=None, n=4, avgpool=False):\n",
    "    \"\"\" Validates a classifier\n",
    "        \n",
    "        :param model: base model (frozen)\n",
    "        :param classifier: classifier to train\n",
    "        :param validation_loader: dataloader of the validation dataset\n",
    "        :param tensor_dir: if set saves the output of the model in the dir\n",
    "        :param adversarial_attack: adversarial attack for adversarial training. Default: None -> the classifier is trained without adversarial perturbation.\n",
    "        :param n: from DINO. Default: 4\n",
    "        :param avgpool_patchtokens: from DINO. Default: False\n",
    "        \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Test:'\n",
    "    if 'num_labels' in dir(classifier):\n",
    "        num_labels = classifier.num_labels\n",
    "    else:\n",
    "        num_labels = classifier.module.num_labels\n",
    "    for inp, target, names in metric_logger.log_every(validation_loader, 20, header):\n",
    "\n",
    "        # move to gpu\n",
    "        inp = inp.cuda(non_blocking=True)\n",
    "        target = target.cuda(non_blocking=True) \n",
    "\n",
    "        # Normalize\n",
    "        transform = pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        norminp = transform(inp)  \n",
    "        \n",
    "        # benign\n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            if 'get_intermediate_layers' in dir(model):\n",
    "                intermediate_output = model.get_intermediate_layers(norminp, n)\n",
    "                output = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
    "                if avgpool:\n",
    "                    output = torch.cat((output.unsqueeze(-1), torch.mean(intermediate_output[-1][:, 1:], dim=1).unsqueeze(-1)), dim=-1)\n",
    "                    output = output.reshape(output.shape[0], -1)\n",
    "            else:\n",
    "                output = model(norminp)\n",
    "                \n",
    "            # save output\n",
    "            if tensor_dir is not None:\n",
    "                save_output_batch(output, names, tensor_dir)\n",
    "\n",
    "            output = classifier(output)\n",
    "            loss = nn.CrossEntropyLoss()(output, target)\n",
    "        \n",
    "        if num_labels >= 5:\n",
    "            acc1, acc5 = utils.accuracy(output, target, topk=(1, 5))\n",
    "        else:\n",
    "            acc1, = utils.accuracy(output, target, topk=(1,))\n",
    "\n",
    "        batch_size = inp.shape[0]\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n",
    "        if num_labels >= 5:\n",
    "            metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n",
    "        \n",
    "        # adversarial attack\n",
    "        if adversarial_attack is not None:\n",
    "            inp = adversarial_attack(inp, target)\n",
    "\n",
    "            # Normalize\n",
    "            transform = pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "            inp = transform(inp)  \n",
    "\n",
    "            # forward\n",
    "            with torch.no_grad():\n",
    "                if 'get_intermediate_layers' in dir(model):\n",
    "                    intermediate_output = model.get_intermediate_layers(inp, n)\n",
    "                    output = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
    "                    if avgpool:\n",
    "                        output = torch.cat((output.unsqueeze(-1), torch.mean(intermediate_output[-1][:, 1:], dim=1).unsqueeze(-1)), dim=-1)\n",
    "                        output = output.reshape(output.shape[0], -1)\n",
    "                else:\n",
    "                    output = model(inp)\n",
    "\n",
    "                # save output\n",
    "                if tensor_dir is not None:\n",
    "                    save_output_batch(output, names, tensor_dir)\n",
    "\n",
    "                output = classifier(output)\n",
    "                adv_loss = nn.CrossEntropyLoss()(output, target)\n",
    "\n",
    "            if num_labels >= 5:\n",
    "                adv_acc1, adv_acc5 = utils.accuracy(output, target, topk=(1, 5))\n",
    "            else:\n",
    "                adv_acc1, = utils.accuracy(output, target, topk=(1,))\n",
    "\n",
    "            batch_size = inp.shape[0]\n",
    "            metric_logger.update(adv_loss=adv_loss.item())\n",
    "            metric_logger.meters['adv_acc1'].update(adv_acc1.item(), n=batch_size)\n",
    "            if num_labels >= 5:\n",
    "                metric_logger.meters['adv_acc5'].update(adv_acc5.item(), n=batch_size)\n",
    "                \n",
    "    if num_labels >= 5:\n",
    "        print('* Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f} loss {losses.global_avg:.3f}'\n",
    "          .format(top1=metric_logger.acc1, top5=metric_logger.acc5, losses=metric_logger.loss))\n",
    "        if adversarial_attack is not None:\n",
    "            print('* adv_Acc@1 {top1.global_avg:.3f} adv_Acc@5 {top5.global_avg:.3f} adv_loss {losses.global_avg:.3f}'\n",
    "          .format(top1=metric_logger.adv_acc1, top5=metric_logger.adv_acc5, losses=metric_logger.adv_loss))\n",
    "    else:\n",
    "        print('* Acc@1 {top1.global_avg:.3f} loss {losses.global_avg:.3f}'\n",
    "          .format(top1=metric_logger.acc1, losses=metric_logger.loss))\n",
    "        if adversarial_attack is not None:\n",
    "            print('* adv_Acc@1 {top1.global_avg:.3f} adv_loss {losses.global_avg:.3f}'\n",
    "          .format(top1=metric_logger.adv_acc1, losses=metric_logger.adv_loss))\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}, metric_logger\n",
    "\n",
    "\n",
    "\n",
    "def save_output_batch(batch_out, batch_names, output_dir):\n",
    "    for out, name in zip(batch_out, batch_names):\n",
    "        torch.save(out, Path(output_dir,name.split('.')[0]+'.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
