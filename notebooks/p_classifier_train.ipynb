{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This extension reloads external Python files\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "from src.model.dino_model import get_dino\n",
    "from src.model.train import *\n",
    "from src.model.data import *\n",
    "from src.helpers.helpers import create_paths\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = Path('..', 'data_dir')\n",
    "MAX_PATH = DATA_PATH\n",
    "\n",
    "BASE_ADV_PATH = Path(MAX_PATH, 'adversarial_data_tensors')\n",
    "BASE_POSTHOC_PATH = Path(MAX_PATH, 'posthoc_tensors')\n",
    "POSTHOC_MODELS_PATH = Path(MAX_PATH, 'posthoc_models')\n",
    "\n",
    "ORI_PATH = Path(DATA_PATH, 'ori')\n",
    "CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "CLASS_SUBSET = np.load(CLASS_SUBSET_PATH)\n",
    "\n",
    "ADV_DATASETS = ['cw', 'fgsm_06', 'pgd_03']\n",
    "\n",
    "DATASETS = [*ADV_DATASETS, 'ori']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATHS = create_paths(data_name='ori',\n",
    "                 datasets_paths=None,  \n",
    "                 initial_base_path=DATA_PATH, \n",
    "                 posthoc_base_path=BASE_POSTHOC_PATH, \n",
    "                 train_str='train', \n",
    "                 val_str='validation')\n",
    "for adv_ds in ADV_DATASETS:\n",
    "    DATA_PATHS = create_paths(data_name=adv_ds,\n",
    "                 datasets_paths=DATA_PATHS,  \n",
    "                 initial_base_path=BASE_ADV_PATH, \n",
    "                 posthoc_base_path=BASE_POSTHOC_PATH, \n",
    "                 train_str='train', \n",
    "                 val_str='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_SUBSET = None\n",
    "NUM_WORKERS= 0\n",
    "PIN_MEMORY=True\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS= 3\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_df(adv_datasets, dataset_paths):\n",
    "    train_dfs = {}\n",
    "    for ds in adv_datasets:\n",
    "        train_dfs[ds] = pd.read_csv(Path(BASE_POSTHOC_PATH, ds, 'train', 'labels_merged.csv'))\n",
    "\n",
    "    val_dfs = {}\n",
    "    for ds in adv_datasets:\n",
    "        val_dfs[ds] = pd.read_csv(Path(BASE_POSTHOC_PATH, ds, 'validation', 'labels_merged.csv'))\n",
    "\n",
    "    # get adversarial tuples\n",
    "    for name, df in train_dfs.items():\n",
    "        df=df[df['true_labels']==df['ori_pred']]\n",
    "        df=df[df['true_labels']!=df[name+'_pred']]\n",
    "        df =df[['file', 'true_labels', 'ori_pred', name+'_pred']]\n",
    "        train_dfs[name]=df\n",
    "    return train_dfs, val_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfs, val_dfs = prepare_data_df(ADV_DATASETS, DATA_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Binary Classifier\n",
    "class LinearBC(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        self.num_labels = 2\n",
    "        super(LinearBC,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_posthoc_classifier(adv_datasets, dataset_paths, epochs=EPOCHS):\n",
    "    logger_dict = {}\n",
    "    for ds in adv_datasets:\n",
    "        print(\"#\"*50 + f''' training linear classifier for {ds} ''' + \"#\"*50)\n",
    "        # loaders\n",
    "        ori_train = dataset_paths['ori']['posthoc']['train']['images']\n",
    "        adv_train = dataset_paths[ds]['posthoc']['train']['images']\n",
    "        print(f'''original images: {ori_train}''')\n",
    "        print(f'''adversarial images: {adv_train}''')\n",
    "        train_set = PosthocTrainDataset(ori_train, adv_train, train_dfs[ds])\n",
    "        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=True)\n",
    "        \n",
    "        \n",
    "        ori_validation = dataset_paths['ori']['posthoc']['validation']['images']\n",
    "        adv_validation = dataset_paths[ds]['posthoc']['validation']['images']\n",
    "        print(f'''original images: {ori_validation}''')\n",
    "        print(f'''adversarial images: {adv_validation}''')\n",
    "        val_set = PosthocTrainDataset(ori_validation, adv_validation, val_dfs[ds])\n",
    "        val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=False)\n",
    "\n",
    "        print(f'''train samples: {len(train_set)} ''')\n",
    "        print(f'''val samples: {len(val_set)} \\n''')\n",
    "\n",
    "        # Initialise network\n",
    "        classifier = LinearBC(1536)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        classifier.cuda()\n",
    "        optimizer = torch.optim.Adagrad(classifier.parameters(), lr=0.001, lr_decay=1e-08, weight_decay=0)\n",
    "        logger_dict[ds] = train(model=None, \n",
    "                                classifier=classifier, \n",
    "                                train_loader=train_loader, \n",
    "                                validation_loader=val_loader, \n",
    "                                log_dir=Path(POSTHOC_MODELS_PATH,ds),\n",
    "                                tensor_dir=None, \n",
    "                                optimizer=optimizer, \n",
    "                                criterion=criterion, \n",
    "                                adversarial_attack=None, \n",
    "                                epochs=epochs, \n",
    "                                val_freq=1, \n",
    "                                batch_size=16,  \n",
    "                                lr=0.001, \n",
    "                                to_restore = {\"epoch\": 0, \"best_acc\": 0.}, \n",
    "                                n=4, \n",
    "                                avgpool_patchtokens=False)\n",
    "\n",
    "        print(f'''\\n''')\n",
    "    return logger_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loggers = train_posthoc_classifier(['cw'], DATA_PATHS, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
