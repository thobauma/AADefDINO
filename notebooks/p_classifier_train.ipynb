{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "from src.model.dino_model import get_dino\n",
    "from src.model.train import *\n",
    "from src.model.data import *\n",
    "from src.helpers.helpers import create_paths\n",
    "from src.helpers.argparser import parser\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Binary Classifier\n",
    "class LinearBC(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        self.num_labels = 2\n",
    "        super(LinearBC,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_posthoc_classifier(adv_attacks, args):\n",
    "    logger_dict = {}\n",
    "    model, base_linear_classifier = get_dino(args=args)\n",
    "    ORI_TRAIN_PATH = args.filtered_data/'train'\n",
    "    ORI_VALIDATION_PATH = args.filtered_data/'validation'\n",
    "    for name in adv_attacks:\n",
    "        LOG_PATH = Path(args.log_dir,'posthoc', name)\n",
    "        ADV_DATA = Path(args.data_root, 'adv', name)\n",
    "        print(\"#\"*50 + f''' training linear classifier for {name} ''' + \"#\"*50)\n",
    "        # loaders\n",
    "        ori_train = ORI_TRAIN_PATH/'images'\n",
    "        adv_train = Path(ADV_DATA,'train', 'images')\n",
    "        print(f'''original images: {ori_train}''')\n",
    "        print(f'''adversarial images: {adv_train}''')\n",
    "        train_set = PosthocTrainDataset(ori_train, adv_train,ORI_TRAIN_PATH/'labels.csv', Path(ADV_DATA,'train','labels.csv'))\n",
    "        train_loader = DataLoader(train_set, batch_size=args.batch_size, num_workers=args.num_workers, pin_memory=args.pin_memory, shuffle=True)\n",
    "        \n",
    "\n",
    "        ori_validation = ORI_VALIDATION_PATH/'images'\n",
    "        adv_validation = Path(ADV_DATA, 'validation', 'images')\n",
    "        print(f'''original images: {ori_validation}''')\n",
    "        print(f'''adversarial images: {adv_validation}''')\n",
    "        val_set = PosthocTrainDataset(ori_validation, adv_validation, ORI_VALIDATION_PATH/'labels.csv', Path(ADV_DATA,'validation','labels.csv'))\n",
    "        val_loader = DataLoader(val_set, batch_size=args.batch_size, num_workers=args.num_workers, pin_memory=args.pin_memory, shuffle=False)\n",
    "\n",
    "        print(f'''train samples: {len(train_set)} ''')\n",
    "        print(f'''val samples: {len(val_set)} \\n''')\n",
    "\n",
    "        # Initialise network\n",
    "        classifier = LinearBC(base_linear_classifier.linear.in_features)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        classifier.cuda()\n",
    "        optimizer = torch.optim.Adagrad(classifier.parameters(), lr=0.001, lr_decay=1e-08, weight_decay=0)\n",
    "        logger_dict[name] = train(model=model, \n",
    "                                classifier=classifier, \n",
    "                                train_loader=train_loader, \n",
    "                                validation_loader=val_loader, \n",
    "                                log_dir=LOG_PATH,\n",
    "                                tensor_dir=None, \n",
    "                                optimizer=optimizer, \n",
    "                                criterion=criterion, \n",
    "                                adversarial_attack=None, \n",
    "                                epochs=args.epochs, \n",
    "                                val_freq=args.val_freq, \n",
    "                                batch_size=args.batch_size,  \n",
    "                                lr=args.batch_size, \n",
    "                                to_restore = {\"epoch\": 0, \"best_acc\": 0.}, \n",
    "                                n=args.n_last_blocks, \n",
    "                                avgpool_patchtokens=args.avgpool_patchtokens)\n",
    "\n",
    "        print(f'''\\n''')\n",
    "    return logger_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
