{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino, ViTWrapper\n",
    "from src.model.data import *\n",
    "\n",
    "# Custom imports\n",
    "import torchattacks\n",
    "from torchattacks import *\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as pth_transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "username = getpass.getuser()\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "MAX_PATH = Path('/','cluster', 'scratch', 'mmathys', 'dl_data')\n",
    "# Path for intermediate outputs\n",
    "BASE_POSTHOC_PATH = Path(MAX_PATH, 'posthoc-fixed-labels/')\n",
    "#BASE_POSTHOC_PATH = Path(MAX_PATH, 'posthoc-subset/')\n",
    "\n",
    "\n",
    "# Original Dataset\n",
    "ORI_PATH = Path(DATA_PATH, 'ori_data/')\n",
    "CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "\n",
    "TR_PATH = Path(ORI_PATH, 'train/')\n",
    "TR_ORI_LABEL_PATH = Path(TR_PATH,'correct_labels.txt')\n",
    "TR_ORI_IMAGES_PATH = Path(TR_PATH,'images')\n",
    "\n",
    "VAL_PATH = Path(ORI_PATH, 'validation/')\n",
    "VAL_ORI_LABEL_PATH = Path(VAL_PATH,'correct_labels.txt')\n",
    "VAL_ORI_IMAGES_PATH = Path(VAL_PATH,'images')\n",
    "\n",
    "# DAmageNet\n",
    "DN_PATH = Path(DATA_PATH, 'damageNet')\n",
    "DN_LABEL_PATH = Path(DN_PATH, 'val_damagenet.txt')\n",
    "DN_IMAGES_PATH = Path(DN_PATH, 'images')\n",
    "DN_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'damagenet')\n",
    "DN_POSTHOC_LABEL_PATH = Path(DN_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "# PGD\n",
    "TR_PGD_PATH = Path(MAX_PATH, 'adversarial_data/pgd_06/train')\n",
    "TR_PGD_LABEL_PATH = TR_ORI_LABEL_PATH\n",
    "TR_PGD_IMAGES_PATH = Path(TR_PGD_PATH, 'images')\n",
    "TR_PGD_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'pgd/train/')\n",
    "TR_PGD_POSTHOC_LABEL_PATH = Path(TR_PGD_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "VAL_PGD_PATH = Path(MAX_PATH, 'adversarial_data/pgd_06/validation')\n",
    "VAL_PGD_LABEL_PATH = VAL_ORI_LABEL_PATH\n",
    "VAL_PGD_IMAGES_PATH = Path(VAL_PGD_PATH, 'images')\n",
    "VAL_PGD_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'pgd/validation/')\n",
    "VAL_PGD_POSTHOC_LABEL_PATH = Path(VAL_PGD_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "# CW\n",
    "TR_CW_PATH = Path(MAX_PATH, 'adversarial_data/cw/train')\n",
    "TR_CW_LABEL_PATH = TR_ORI_LABEL_PATH\n",
    "TR_CW_IMAGES_PATH = Path(TR_CW_PATH, 'images')\n",
    "TR_CW_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'cw/train/')\n",
    "TR_CW_POSTHOC_LABEL_PATH = Path(TR_CW_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "VAL_CW_PATH = Path(MAX_PATH, 'adversarial_data/cw/validation')\n",
    "VAL_CW_LABEL_PATH = VAL_ORI_LABEL_PATH\n",
    "VAL_CW_IMAGES_PATH = Path(VAL_CW_PATH, 'images')\n",
    "VAL_CW_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'cw/validation/')\n",
    "VAL_CW_POSTHOC_LABEL_PATH = Path(VAL_CW_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "# FGSM\n",
    "TR_FGSM_PATH = Path(MAX_PATH, 'adversarial_data/fgsm_06/train')\n",
    "TR_FGSM_LABEL_PATH = TR_ORI_LABEL_PATH\n",
    "TR_FGSM_IMAGES_PATH = Path(TR_FGSM_PATH, 'images')\n",
    "TR_FGSM_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'fgsm/train/')\n",
    "TR_FGSM_POSTHOC_LABEL_PATH = Path(TR_FGSM_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "VAL_FGSM_PATH = Path(MAX_PATH, 'adversarial_data/fgsm_06/validation')\n",
    "VAL_FGSM_LABEL_PATH = VAL_ORI_LABEL_PATH\n",
    "VAL_FGSM_IMAGES_PATH = Path(VAL_FGSM_PATH, 'images')\n",
    "VAL_FGSM_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'fgsm/validation/')\n",
    "VAL_FGSM_POSTHOC_LABEL_PATH = Path(VAL_FGSM_POSTHOC_PATH, 'labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INDEX_SUBSET = None\n",
    "NUM_WORKERS= 0\n",
    "PIN_MEMORY=True\n",
    "#CLASS_SUBSET = np.load(CLASS_SUBSET_PATH)\n",
    "\n",
    "BATCH_SIZE = 80\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!python ../setup/collect_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights from dino_deitsmall16_pretrain/dino_deitsmall16_linearweights.pth.\n"
     ]
    }
   ],
   "source": [
    "model, linear_classifier = get_dino(model_name='vit_small', patch_size=16, n_last_blocks=4, avgpool_patchtokens=False, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(dim, num_labels)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # linear layer\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                         num_labels=len(CLASS_SUBSET))\n",
    "\n",
    "linear_classifier.load_state_dict(torch.load(\"/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes\" + \"/\" + \"clean.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit([i for i in CLASS_SUBSET])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_paths = {\n",
    "            'cw':{ \n",
    "                'b':{\n",
    "                    'train':{\n",
    "                        'label':TR_ORI_LABEL_PATH,\n",
    "                        'images':TR_CW_IMAGES_PATH\n",
    "                    },\n",
    "                    'val':\n",
    "                    {\n",
    "                        'label':VAL_ORI_LABEL_PATH,\n",
    "                        'images':VAL_CW_IMAGES_PATH\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'fgsm_06':{\n",
    "                'b':{\n",
    "                    'train':{\n",
    "                        'label':TR_ORI_LABEL_PATH,\n",
    "                        'images':TR_FGSM_IMAGES_PATH\n",
    "                    },\n",
    "                    'val':\n",
    "                    {\n",
    "                        'label':VAL_ORI_LABEL_PATH,\n",
    "                        'images':VAL_FGSM_IMAGES_PATH\n",
    "                    }\n",
    "                 }\n",
    "            },\n",
    "            'pgd_06':{\n",
    "                'b':{\n",
    "                    'train':{\n",
    "                        'label':TR_ORI_LABEL_PATH,\n",
    "                        'images':TR_PGD_IMAGES_PATH\n",
    "                    },\n",
    "                    'val':\n",
    "                    {\n",
    "                        'label':VAL_ORI_LABEL_PATH,\n",
    "                        'images':VAL_PGD_IMAGES_PATH\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_wrap = ViTWrapper(model, linear_classifier, device=DEVICE, n_last_blocks=4, avgpool_patchtokens=False)\n",
    "model_wrap = model_wrap.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AdvTrainingImageDataset(\"/cluster/scratch/thobauma/data/ori/filtered/train/images/\", \"/cluster/scratch/thobauma/data/ori/filtered/train/labels.csv\", ORIGINAL_TRANSFORM, index_subset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=2, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "\n",
    "for b in data_loader:\n",
    "    data = b\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.3216, 0.3255, 0.3490,  ..., 0.2627, 0.3020, 0.2510],\n",
       "           [0.3333, 0.3412, 0.3255,  ..., 0.2902, 0.3059, 0.2392],\n",
       "           [0.3098, 0.3098, 0.3176,  ..., 0.2863, 0.2941, 0.2471],\n",
       "           ...,\n",
       "           [0.4706, 0.6314, 0.7608,  ..., 0.4588, 0.3569, 0.4078],\n",
       "           [0.7725, 0.8549, 0.9216,  ..., 0.5961, 0.5373, 0.5333],\n",
       "           [0.6941, 0.6235, 0.6039,  ..., 0.7412, 0.6784, 0.6941]],\n",
       " \n",
       "          [[0.3137, 0.3294, 0.3294,  ..., 0.3490, 0.3647, 0.2824],\n",
       "           [0.3255, 0.3373, 0.3333,  ..., 0.3294, 0.3490, 0.2627],\n",
       "           [0.3176, 0.3176, 0.3059,  ..., 0.3176, 0.3294, 0.2824],\n",
       "           ...,\n",
       "           [0.3882, 0.5294, 0.6706,  ..., 0.3686, 0.2627, 0.2824],\n",
       "           [0.7294, 0.8196, 0.9216,  ..., 0.5216, 0.4510, 0.3961],\n",
       "           [0.7020, 0.6471, 0.6431,  ..., 0.6431, 0.5686, 0.5647]],\n",
       " \n",
       "          [[0.3294, 0.3529, 0.3569,  ..., 0.4078, 0.4824, 0.3765],\n",
       "           [0.3647, 0.3686, 0.3686,  ..., 0.3686, 0.4275, 0.3020],\n",
       "           [0.3333, 0.3373, 0.3608,  ..., 0.4039, 0.4510, 0.3569],\n",
       "           ...,\n",
       "           [0.4353, 0.5725, 0.6863,  ..., 0.3804, 0.2863, 0.3294],\n",
       "           [0.7412, 0.8549, 0.9255,  ..., 0.5686, 0.4863, 0.4627],\n",
       "           [0.7059, 0.6706, 0.6510,  ..., 0.7333, 0.6392, 0.6588]]],\n",
       " \n",
       " \n",
       "         [[[0.4627, 0.4549, 0.4588,  ..., 0.5412, 0.5569, 0.5451],\n",
       "           [0.4588, 0.4549, 0.4549,  ..., 0.5529, 0.5529, 0.5451],\n",
       "           [0.4549, 0.4549, 0.4471,  ..., 0.5451, 0.5490, 0.5490],\n",
       "           ...,\n",
       "           [0.1608, 0.0745, 0.0667,  ..., 0.0627, 0.0784, 0.0471],\n",
       "           [0.0980, 0.1608, 0.0510,  ..., 0.0784, 0.1176, 0.1137],\n",
       "           [0.0353, 0.0667, 0.0471,  ..., 0.1255, 0.1176, 0.1020]],\n",
       " \n",
       "          [[0.4078, 0.4000, 0.4039,  ..., 0.4824, 0.4980, 0.4863],\n",
       "           [0.4039, 0.4000, 0.4000,  ..., 0.4941, 0.4941, 0.4863],\n",
       "           [0.4000, 0.4000, 0.3922,  ..., 0.4863, 0.4902, 0.4902],\n",
       "           ...,\n",
       "           [0.1725, 0.0824, 0.0745,  ..., 0.0706, 0.0824, 0.0549],\n",
       "           [0.1098, 0.1647, 0.0588,  ..., 0.0863, 0.1216, 0.1216],\n",
       "           [0.0431, 0.0745, 0.0549,  ..., 0.1333, 0.1255, 0.1098]],\n",
       " \n",
       "          [[0.3647, 0.3569, 0.3608,  ..., 0.4549, 0.4745, 0.4667],\n",
       "           [0.3608, 0.3569, 0.3569,  ..., 0.4667, 0.4745, 0.4667],\n",
       "           [0.3569, 0.3569, 0.3490,  ..., 0.4588, 0.4627, 0.4667],\n",
       "           ...,\n",
       "           [0.1490, 0.0667, 0.0588,  ..., 0.0706, 0.0863, 0.0549],\n",
       "           [0.0902, 0.1569, 0.0588,  ..., 0.0784, 0.1255, 0.1255],\n",
       "           [0.0314, 0.0627, 0.0510,  ..., 0.1137, 0.1176, 0.1137]]]]),\n",
       " tensor([193, 169]),\n",
       " ['n02096294_3387', 'n02090622_6812']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(data[0].to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([245, 304], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/403 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/scratch/mmathys/dl_data/adversarial_data/cw/train/new_labels.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 403/403 [02:32<00:00,  2.65it/s]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy against cw <src.model.data.AdvTrainingImageDataset object at 0x2b69f352e430>: 95.25 %\n",
      "/cluster/scratch/mmathys/dl_data/adversarial_data/cw/validation/new_labels.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:06<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy against cw <src.model.data.AdvTrainingImageDataset object at 0x2b69f352ef70>: 93.68 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/403 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/scratch/mmathys/dl_data/adversarial_data/fgsm_06/train/new_labels.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 403/403 [02:36<00:00,  2.58it/s]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy against fgsm_06 <src.model.data.AdvTrainingImageDataset object at 0x2b69f352ecd0>: 12.85 %\n",
      "/cluster/scratch/mmathys/dl_data/adversarial_data/fgsm_06/validation/new_labels.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:06<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy against fgsm_06 <src.model.data.AdvTrainingImageDataset object at 0x2b6a2b94b100>: 14.64 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/403 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/scratch/mmathys/dl_data/adversarial_data/pgd_06/train/new_labels.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 403/403 [02:35<00:00,  2.59it/s]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy against pgd_06 <src.model.data.AdvTrainingImageDataset object at 0x2b6a3616eb50>: 0.00 %\n",
      "/cluster/scratch/mmathys/dl_data/adversarial_data/pgd_06/validation/new_labels.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:06<00:00,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy against pgd_06 <src.model.data.AdvTrainingImageDataset object at 0x2b6a2b94b550>: 0.00 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for attack in datasets_paths:\n",
    "        for d in ['train', 'val']:\n",
    "\n",
    "            dataset = AdvTrainingImageDataset(datasets_paths[attack]['b'][d]['images'], datasets_paths[attack]['b'][d]['label'], ONLY_NORMALIZE_TRANSFORM, CLASS_SUBSET, index_subset=None, label_encoder=label_encoder)\n",
    "            data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=False)\n",
    "            total = len(dataset)\n",
    "            \n",
    "            STORE_PATH = str(datasets_paths[attack]['b'][d]['images']).replace('/images', '/new_labels.csv')\n",
    "            print(STORE_PATH)\n",
    "            adv_labels = {}\n",
    "            correct = 0\n",
    "            \n",
    "            for images, labels, img_names in tqdm(data_loader):\n",
    "\n",
    "                labels = labels.to(DEVICE)\n",
    "                images = images.to(DEVICE)\n",
    "                \n",
    "                outputs = model_wrap(images)\n",
    "\n",
    "                _, pre = torch.max(outputs.data, 1)\n",
    "\n",
    "                correct += (pre == labels).sum()\n",
    "\n",
    "                for i in range(images.shape[0]):\n",
    "                    adv_labels[img_names[i]] = pre.cpu().numpy()[i]\n",
    "\n",
    "            print(f'Accuracy against {attack} {dataset}: %.2f %%' % (100 * float(correct) / total))\n",
    "\n",
    "            df = pd.DataFrame.from_dict(adv_labels, orient='index')\n",
    "            df.to_csv(STORE_PATH, sep=\" \", header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
