{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"attack_classifier.ipynb","provenance":[],"collapsed_sections":["9NTVkBDdm-kH"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lVYi_Zwu91it"},"source":["# Install, Paths and Parameters"]},{"cell_type":"code","metadata":{"id":"H3qhJR_04HNn"},"source":["# Requirements. Need to restart after installation (it sucks, I am sorry haha)\n","!pip install torch==1.7.1\n","!pip install torchvision==0.8.2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wMiKhBahg23V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638807931660,"user_tz":-60,"elapsed":57881,"user":{"displayName":"Nasib Naimi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcMhNa1yy2FMf91F3k9aayQvp124BBaqjk8NOKhg=s64","userId":"00717947242244318549"}},"outputId":"5dadd94d-da31-4ac1-f9b8-e0edd86ba1cb"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"RDBcKRbJYciA"},"source":["# all imports\n","# standard libraries\n","from __future__ import print_function, division\n","import os\n","from os import listdir\n","import json\n","# import utils\n","import random\n","import colorsys\n","import requests\n","from io import BytesIO\n","import numpy as np\n","import pandas as pd\n","from typing import List, Callable\n","from tqdm import tqdm\n","\n","# torch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","from torchvision import transforms, utils\n","from torch.utils.tensorboard import SummaryWriter\n","# import vision_transformer as vits\n","\n","# Image stuff\n","import skimage.io\n","from skimage import io, transform\n","from skimage.measure import find_contours\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Polygon\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","# seed\n","SEED = 42\n","random.seed(SEED)\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jz2fe6PfYh_0"},"source":["# general path\n","DRIVE_PATH = '/content/drive/MyDrive/'\n","DL_PROJECT_PATH = os.path.join(DRIVE_PATH, 'DeepLearningProject')\n","ADV_PATH = os.path.join(DL_PROJECT_PATH, 'AdversarialAttacks')\n","\n","# filenames and label path\n","ADV_LABEL_PATH = os.path.join(ADV_PATH,'adversarial_data/DAmageNet/val_damagenet.txt')\n","ORG_LABEL_PATH = os.path.join(ADV_PATH,'original_data/correct_labels.txt')\n","\n","# image paths\n","ORIGINAL_IMAGES_PATH = os.path.join(ADV_PATH,'original_data/images/')\n","ADVERSARIAL_IMAGES_PATH = os.path.join(ADV_PATH,'adversarial_data/DAmageNet/DAmageNet/')\n","\n","# attention paths\n","ORIGINAL_ATTENTION_PATH = os.path.join(ADV_PATH,'org_attn/')\n","ADVERSARIAL_ATTENTION_PATH = os.path.join(ADV_PATH,'adv_attn/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yP4i522l-m7o"},"source":["def get_random_classes(number_of_classes: int = 5, min_rand_class: int = 0, max_rand_class: int = 999):\n","  return np.random.randint(low=min_rand_class, high=max_rand_class, size=(number_of_classes,))\n","\n","CLASS_SUBSET = get_random_classes()\n","\n","BATCH_SIZE = 6 # You can play around with it"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9NTVkBDdm-kH"},"source":["# Visualize attention\n","Taken from: https://github.com/facebookresearch/dino/blob/main/visualize_attention.py"]},{"cell_type":"code","metadata":{"id":"c8plFR_unCgC","colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"status":"error","timestamp":1638618063597,"user_tz":-60,"elapsed":322,"user":{"displayName":"Javier Rando","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06593485543899718786"}},"outputId":"1bb6b826-886c-4f4a-bb98-d0761e0193b9"},"source":["import random\n","import colorsys\n","import requests\n","from io import BytesIO\n","\n","import skimage.io\n","from skimage.measure import find_contours\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Polygon\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torchvision import transforms as pth_transforms\n","import numpy as np\n","from PIL import Image\n","\n","import utils\n","import vision_transformer as vits\n","\n","\n","def apply_mask(image, mask, color, alpha=0.5):\n","    for c in range(3):\n","        image[:, :, c] = image[:, :, c] * (1 - alpha * mask) + alpha * mask * color[c] * 255\n","    return image\n","\n","\n","def random_colors(N, bright=True):\n","    \"\"\"\n","    Generate random colors.\n","    \"\"\"\n","    brightness = 1.0 if bright else 0.7\n","    hsv = [(i / N, 1, brightness) for i in range(N)]\n","    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n","    random.shuffle(colors)\n","    return colors\n","\n","\n","def display_instances(image, mask, fname=\"test\", figsize=(5, 5), blur=False, contour=True, alpha=0.5):\n","    fig = plt.figure(figsize=figsize, frameon=False)\n","    ax = plt.Axes(fig, [0., 0., 1., 1.])\n","    ax.set_axis_off()\n","    fig.add_axes(ax)\n","    ax = plt.gca()\n","\n","    N = 1\n","    mask = mask[None, :, :]\n","    # Generate random colors\n","    colors = random_colors(N)\n","\n","    # Show area outside image boundaries.\n","    height, width = image.shape[:2]\n","    margin = 0\n","    ax.set_ylim(height + margin, -margin)\n","    ax.set_xlim(-margin, width + margin)\n","    ax.axis('off')\n","    masked_image = image.astype(np.uint32).copy()\n","    for i in range(N):\n","        color = colors[i]\n","        _mask = mask[i]\n","        if blur:\n","            _mask = cv2.blur(_mask,(10,10))\n","        # Mask\n","        masked_image = apply_mask(masked_image, _mask, color, alpha)\n","        # Mask Polygon\n","        # Pad to ensure proper polygons for masks that touch image edges.\n","        if contour:\n","            padded_mask = np.zeros((_mask.shape[0] + 2, _mask.shape[1] + 2))\n","            padded_mask[1:-1, 1:-1] = _mask\n","            contours = find_contours(padded_mask, 0.5)\n","            for verts in contours:\n","                # Subtract the padding and flip (y, x) to (x, y)\n","                verts = np.fliplr(verts) - 1\n","                p = Polygon(verts, facecolor=\"none\", edgecolor=color)\n","                ax.add_patch(p)\n","    ax.imshow(masked_image.astype(np.uint8), aspect='auto')\n","    fig.savefig(fname)\n","    print(f\"{fname} saved.\")\n","    return\n","\n","PATCH_SIZE = 16\n","OUTPUT_DIR = \"/content/drive/MyDrive/Deep Learning Project/Adversarial Attacks/original_adv/\"\n","INPUT_IMAGE = \"/content/drive/MyDrive/Deep Learning Project/Adversarial Attacks/original_data/images/ILSVRC2012_val_00000130.JPEG\"\n","model = vits16\n","\n","img = Image.open(INPUT_IMAGE)\n","img = img.convert('RGB')\n","\n","transform = pth_transforms.Compose([\n","    pth_transforms.Resize(256),\n","    pth_transforms.ToTensor(),\n","    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","])\n","\n","img = transform(img)\n","\n","# make the image divisible by the patch size\n","w, h = img.shape[1] - img.shape[1] % PATCH_SIZE, img.shape[2] - img.shape[2] % PATCH_SIZE\n","img = img[:, :w, :h].unsqueeze(0)\n","\n","w_featmap = img.shape[-2] // PATCH_SIZE\n","h_featmap = img.shape[-1] // PATCH_SIZE\n","\n","print(img.shape)\n","attentions = model.get_last_selfattention(img.to(device))\n","\n","nh = attentions.shape[1] # number of heads\n","\n","# we keep only the output patch attention\n","attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n","\n","attentions = attentions.reshape(nh, w_featmap, h_featmap)\n","attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=PATCH_SIZE, mode=\"nearest\")[0].detach().cpu().numpy()\n","\n","# save attentions heatmaps\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","torchvision.utils.save_image(torchvision.utils.make_grid(img, normalize=True, scale_each=True), os.path.join(OUTPUT_DIR, \"img.png\"))\n","for j in range(nh):\n","    fname = os.path.join(OUTPUT_DIR, \"attn-head\" + str(j) + \".png\")\n","    plt.imsave(fname=fname, arr=attentions[j], format='png')\n","    print(f\"{fname} saved.\")"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-743c0f2108a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#import utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mvision_transformer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vision_transformer'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","metadata":{"id":"WdGj5AZNT_34"},"source":["# Adversarial Attack Classifier"]},{"cell_type":"markdown","metadata":{"id":"aGJOqsFylRIU"},"source":["### Custom Dataset Class\n","Sample contains attention and label specifying if the attention comes from an adverserial attack or not"]},{"cell_type":"code","metadata":{"id":"jpSNtu9DouKx"},"source":["class AdverserialAttentionDataset(Dataset):\n","    \"\"\"Adverserial Attention dataset.\"\"\"\n","\n","    def __init__(self, adv_attn_dir, org_attn_dir, transform=None):\n","        \"\"\"\n","        Args:\n","            adv_attn_dir (string): Directory with all the .pt files that contain\n","                attention of the adv. img.\n","            org_attn_dir (string): Directory with all the .pt files that contain\n","                attention of the org. img.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.adv_attn_dir = adv_attn_dir\n","        self.org_attn_dir = org_attn_dir\n","        names_org = [f[:-3]+\"_org\" for f in listdir(org_attn_dir)]\n","        names_adv = [f[:-3]+\"_adv\" for f in listdir(adv_attn_dir)]\n","\n","        self.n = len(names_org)\n","        self.file_names = names_org + names_adv\n","        random.shuffle(self.file_names)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.file_names)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        attn_file = self.file_names[idx]\n","        if \"adv\" in attn_file:\n","          label = float(1)\n","          attn_file = attn_file[:-4]\n","          attn_path = os.path.join(self.adv_attn_dir, attn_file+\".pt\")\n","        else:\n","          label = float(0)\n","          attn_file = attn_file[:-4]\n","          attn_path = os.path.join(self.org_attn_dir, attn_file+\".pt\")\n","\n","        attention = torch.load(attn_path)\n","      \n","        sample = {'attention': attention.squeeze(), 'label': label}\n","\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        return sample"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vNS7CGyLlxt_"},"source":["### Sample visualization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"id":"TwcG1CzqlwGO","executionInfo":{"status":"error","timestamp":1638807981086,"user_tz":-60,"elapsed":39361,"user":{"displayName":"Nasib Naimi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcMhNa1yy2FMf91F3k9aayQvp124BBaqjk8NOKhg=s64","userId":"00717947242244318549"}},"outputId":"9687b1c8-9c6d-4ef0-c002-81053cd022d9"},"source":["BATCH_SIZE = 4\n","\n","test = torch.load(ORIGINAL_ATTENTION_PATH+f\"{0}.pt\")\n","print(test.shape)\n","\n","PATCH_SIZE = 8\n","w_featmap = 28\n","h_featmap = 28\n","attentions = test\n","nh = attentions.shape[1] # number of heads\n","# we keep only the output patch attention\n","plt.ion() # interactive mode\n","\n","fig = plt.figure(figsize=(16, 24), dpi=80)\n","\n","nr = 12\n","for j in range(nr):\n","  attention = attentions[0, :, j, 1:].reshape(nh, -1)\n","  # print(attentions)\n","  attention = attention.reshape(nh, w_featmap, h_featmap)\n","  attention = nn.functional.interpolate(attention.unsqueeze(0), scale_factor=PATCH_SIZE, mode=\"nearest\")[0].detach().cpu().numpy()\n","  for i in range(6):\n","    ind = (i+1)+j*6\n","    ax = plt.subplot(nr, 6, ind)\n","    plt.tight_layout()\n","    ax.set_title('Test Layer:{} Head:{}'.format(j,i))\n","    ax.axis('off')\n","    plt.imshow(attention[i,:,:])\n","\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-94bf91a1c149>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mORIGINAL_ATTENTION_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34mf\"{0}.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/DeepLearningProject/AdversarialAttacks/org_attn/0.pt'"]}]},{"cell_type":"code","metadata":{"id":"rNovjg8Fm2tT"},"source":["test.mean(axis=2).shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y6dfDvJWYx1W"},"source":["## Classifiers\n","- Classifier 1: 2 CNN, 2 pool, and 3 FC Layers\n","- Classifier 2: etc."]},{"cell_type":"code","metadata":{"id":"O6vkw2iwZddn"},"source":["BATCH_SIZE = 6\n","\n","attn_dataset = AdverserialAttentionDataset(adv_attn_dir=ADVERSARIAL_ATTENTION_PATH, org_attn_dir=ORIGINAL_ATTENTION_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zoQpwfpni3jc"},"source":["# Training set and training loader\n","dataset_size = len(attn_dataset)\n","train_size = int(0.8 * dataset_size)\n","test_size = dataset_size - train_size\n","\n","train_dataset, test_dataset = torch.utils.data.random_split(attn_dataset, \n","                                                            [train_size, \n","                                                             test_size],\n","                                                            generator=torch.Generator().manual_seed(42))\n","\n","# Train and test loader\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                                          shuffle=True, num_workers=0)\n","\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset),\n","                                         shuffle=True, num_workers=0)\n","\n","classes = ('non-adversarial', 'adversarial') # Binary classifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KkjILx0qONTy"},"source":["attention = torch.load(ADVERSARIAL_ATTENTION_PATH + \"1.pt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8DwMDhXPOUI-"},"source":["attention.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lbtqts3GYiSL"},"source":["# Classifier 1 Network\n","class ClassifierOne(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(6, 12, 3, stride=1)\n","        self.conv2 = nn.Conv2d(12, 16, 5, stride=1)\n","        self.conv3 = nn.Conv2d(16, 24, 5, stride=1)\n","\n","        self.fc1 = nn.Linear(7*7*24, 120)\n","        self.fc2 = nn.Linear(120, 10)\n","        self.fc3 = nn.Linear(10, 1)\n","\n","        # Utils\n","        self.pool = nn.MaxPool2d(2, 2)\n","\n","    def initialize(self):\n","      nn.init.normal_(self.fc1.weight, mean=0, std=1.0)\n","      nn.init.normal_(self.fc1.bias, mean=0, std=1.0)\n","      nn.init.normal_(self.fc2.weight, mean=0, std=1.0)\n","      nn.init.normal_(self.fc2.bias, mean=0, std=1.0)\n","      nn.init.normal_(self.fc3.weight, mean=0, std=1.0)\n","      nn.init.normal_(self.fc3.bias, mean=0, std=1.0)\n","      nn.init.normal_(self.conv1.weight, mean=0, std=1.0)\n","      nn.init.normal_(self.conv1.bias, mean=0, std=1.0)\n","      nn.init.normal_(self.conv2.weight, mean=0, std=1.0)\n","      nn.init.normal_(self.conv2.bias, mean=0, std=1.0)\n","      nn.init.normal_(self.conv3.weight, mean=0, std=1.0)\n","      nn.init.normal_(self.conv3.bias, mean=0, std=1.0)\n","      # nn.init.xavier_uniform(self.linear.weight.data)\n","      # self.linear.bias.data.zero_()\n","\n","    def forward(self, x):\n","        x = torch.relu(self.conv1(x))\n","        x = self.pool(torch.relu(self.conv2(x)))\n","        x = torch.relu(self.conv3(x))\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = torch.sigmoid(self.fc3(x))\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h6ySwN4ykbvX"},"source":["# Simple Binary Classifier Network\n","class SimpleBC(nn.Module):\n","  def __init__(self,input_shape):\n","    super(SimpleBC,self).__init__()\n","    self.fc1 = nn.Linear(input_shape,32)\n","    self.fc2 = nn.Linear(32,64)\n","    self.fc3 = nn.Linear(64,1)\n","    self.initialize()\n","\n","  def initialize(self):\n","    nn.init.normal_(self.fc1.weight, mean=0, std=1.0)\n","    nn.init.normal_(self.fc1.bias, mean=0, std=1.0)\n","    nn.init.normal_(self.fc2.weight, mean=0, std=1.0)\n","    nn.init.normal_(self.fc2.bias, mean=0, std=1.0)\n","    nn.init.normal_(self.fc3.weight, mean=0, std=1.0)\n","    nn.init.normal_(self.fc3.bias, mean=0, std=1.0)\n","    # nn.init.xavier_uniform(self.linear.weight.data)\n","    # self.linear.bias.data.zero_()\n","\n","  def forward(self,x):\n","    x = torch.relu(self.fc1(x))\n","    x = torch.relu(self.fc2(x))\n","    x = torch.sigmoid(self.fc3(x))\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YoN2mws-nc2o"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"ug6b37O9VtlG"},"source":["# Load TensorBoard notebook extension\n","%load_ext tensorboard\n","\n","# Import SummaryWriter\n","from torch.utils.tensorboard import SummaryWriter\n","\n","# Create a SummaryWriter instance\n","# SummaryWriter writes event files to log_dir\n","log_dir = \"./logs\"\n","writer = SummaryWriter(log_dir)\n","np.set_printoptions(precision=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rIVk8j1VYl62"},"source":["# Hyperparameters\n","EPOCHS = 100\n","\n","# Initialise network\n","net = ClassifierOne()\n","# net = SimpleBC(6*28*28)\n","\n","# Select device\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","net.to(device)\n","\n","# Set model to train\n","net.train()\n","\n","# define loss, optimizer, and scheduler\n","criterion = nn.BCELoss()\n","# optimizer = optim.Adam(net.parameters(), lr=0.001)\n","optimizer = optim.Adagrad(net.parameters(), lr=0.01, lr_decay=1e-08, weight_decay=0)\n","# scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n","\n","losses = []\n","accur = []\n","\n","# Train network\n","pbar = tqdm(range(EPOCHS))\n","for epoch in pbar:  # loop over the dataset multiple times\n","\n","    # Metrics\n","    train_running_loss = 0.0\n","    train_running_loss_mean = 0.0\n","    train_acc = 0.0\n","    train_acc_mean = 0.0\n","    test_running_loss = 0.0\n","    test_acc = 0.0\n","\n","    for i, data in enumerate(train_loader, start=0):\n","      try:\n","        # get the inputs; data is a list of [inputs, labels] and write to device\n","        inputs, labels = data['attention'], data['label']\n","        inputs = inputs[:, :, 0, 1:].reshape(labels.shape[0], 6, 28, 28)\n","        # for simpleBC START\n","        # inputs = inputs.flatten(start_dim=1)\n","        # END\n","        inputs = inputs.to(device)\n","        labels = labels.to(device).float()\n","        \n","        # Forward Pass\n","        outputs = net(inputs).float()\n","        outputs = outputs.reshape(-1)\n","        \n","        # Backpropagation\n","        optimizer.zero_grad() # Reset the gradient\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # loss train\n","        train_running_loss += loss.item()\n","        train_running_loss_mean = train_running_loss / (i+1)\n","\n","        # accuracy train\n","        predicted = net(inputs).reshape(-1).detach().cpu().numpy().round()\n","        acc_labels = labels\n","        acc_labels = acc_labels.detach().cpu().numpy()\n","        inter = np.equal(predicted, acc_labels)\n","        train_acc += inter.mean()\n","        train_acc_mean = train_acc / (i+1)\n","\n","      except Exception as e:\n","        print(\"Error: {}\".format(e))\n","        pass\n","    \n","    with torch.set_grad_enabled(False):\n","      for data in test_loader:\n","        try:\n","          # get the inputs; data is a list of [inputs, labels] and write to device\n","          inputs, labels = data['attention'], data['label']\n","          inputs = inputs[:, :, 0, 1:].reshape(labels.shape[0], 6, 28, 28)\n","          # for simpleBC START\n","          # inputs = inputs.flatten(start_dim=1)\n","          # END\n","          inputs = inputs.to(device)\n","          labels = labels.to(device).float()\n","          \n","          # Forward Pass\n","          outputs = net(inputs).float()\n","          outputs = outputs.reshape(-1)\n","          \n","          # loss test\n","          loss = criterion(outputs, labels)\n","          test_running_loss += loss.item()\n","\n","          # accuracy test\n","          outputs = outputs.detach().cpu().numpy().round()\n","          comparison = np.equal(labels.detach().cpu().numpy(), outputs)\n","          test_acc = comparison.mean()\n","\n","        except Exception as e:\n","          print(\"Error: {}\".format(e))\n","          pass\n","\n","    writer.add_scalar(\"Loss/train\", train_running_loss_mean, epoch)\n","    writer.add_scalar('Loss/test', test_running_loss, epoch) \n","    writer.add_scalar('Accuracy/train', train_acc_mean, epoch)\n","    writer.add_scalar('Accuracy/test', test_acc, epoch)\n","\n","    losses.append(train_running_loss_mean)\n","    accur.append(train_acc_mean)\n","    pbar.set_description(\"Ep: {}\\t Tr. Loss: {:.4f}\\t Tr. Acc: {:.4f}\\t T. Loss: {:.4f}\\t T. Acc: {:.4f}\".format(epoch, \n","                                                                            train_running_loss_mean, \n","                                                                            train_acc_mean, \n","                                                                            test_running_loss, \n","                                                                            test_acc))\n","\n","print('Finished Training')\n","writer.flush()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Krxxg-bnPQCj"},"source":["writer.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ok7s3A-MSNO-"},"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y-Wbo7w4SUlB"},"source":["%tensorboard --logdir ./logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RCWMXW5XPOLM","executionInfo":{"status":"ok","timestamp":1638635593444,"user_tz":-60,"elapsed":711,"user":{"displayName":"Nasib Naimi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcMhNa1yy2FMf91F3k9aayQvp124BBaqjk8NOKhg=s64","userId":"00717947242244318549"}},"outputId":"b5107c9e-5bf7-4cc4-83f8-3577b4b09369"},"source":["from tensorboard import notebook\n","notebook.list() # View open TensorBoard instances"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Known TensorBoard instances:\n","  - port 6006: logdir logs (started 0:01:33 ago; pid 1563)\n"]}]},{"cell_type":"code","metadata":{"id":"Q1pdytJPYqx0"},"source":["PATH = './simple_net.pth'\n","torch.save(net.state_dict(), PATH)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n4AEKriBnlmf"},"source":["### Testing"]},{"cell_type":"code","metadata":{"id":"2mHyLUbqYt32"},"source":["model = SimpleBC(4704)\n","model.load_state_dict(torch.load(PATH))\n","model.eval()\n","\n","# TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LuNxc5p2QWd2"},"source":["# Test stored files"]},{"cell_type":"code","metadata":{"id":"PNaSSFT2hkWd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638618571095,"user_tz":-60,"elapsed":167030,"user":{"displayName":"Javier Rando","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06593485543899718786"}},"outputId":"a12a4859-bd87-4341-e224-58ff47f16cb8"},"source":["for f in os.listdir(ORIGINAL_ATTENTION_PATH):\n","  try:\n","    torch.load(ADVERSARIAL_ATTENTION_PATH+f)\n","  except:\n","    print(f)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["56.pt\n","30.pt\n"]}]},{"cell_type":"code","metadata":{"id":"CZPPGjqiQ42L"},"source":[""],"execution_count":null,"outputs":[]}]}