{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVYi_Zwu91it"
   },
   "source": [
    "# Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3qhJR_04HNn"
   },
   "outputs": [],
   "source": [
    "# Requirements. Need to restart after installation (it sucks, I am sorry haha)\n",
    "!pip install torch==1.7.1\n",
    "!pip install torchvision==0.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMiKhBahg23V"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1639137648034,
     "user": {
      "displayName": "Max Mathys",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3HSmeM7yDULrLjSetp4qxKC5JlZVlKNk0xUMAxQ=s64",
      "userId": "08899314479848539391"
     },
     "user_tz": -60
    },
    "id": "RDBcKRbJYciA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from typing import List, Callable\n",
    "import random\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1639137648035,
     "user": {
      "displayName": "Max Mathys",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3HSmeM7yDULrLjSetp4qxKC5JlZVlKNk0xUMAxQ=s64",
      "userId": "08899314479848539391"
     },
     "user_tz": -60
    },
    "id": "Jz2fe6PfYh_0"
   },
   "outputs": [],
   "source": [
    "# general path\n",
    "DRIVE_PATH = '/content/drive/MyDrive/'\n",
    "DL_PROJECT_PATH = os.path.join(DRIVE_PATH, 'DeepLearningProject')\n",
    "ADV_PATH = os.path.join(DL_PROJECT_PATH, 'AdversarialAttacks')\n",
    "\n",
    "# filenames and label path\n",
    "ADV_LABEL_PATH = os.path.join(ADV_PATH,'adv_data/val_damagenet_cl.txt')\n",
    "ORG_LABEL_PATH = os.path.join(ADV_PATH,'ori_data/correct_labels_cl.txt')\n",
    "\n",
    "# image paths\n",
    "ORIGINAL_IMAGES_PATH = os.path.join(ADV_PATH,'ori_data/ImageNetClasses/')\n",
    "ADVERSARIAL_IMAGES_PATH = os.path.join(ADV_PATH,'adv_data/DAmageNetClasses/')\n",
    "\n",
    "# attention paths\n",
    "ORG_ATTN_PATH = os.path.join(ADV_PATH,'org_attn/')\n",
    "ADV_ATTN_PATH = os.path.join(ADV_PATH,'adv_attn/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1639138713719,
     "user": {
      "displayName": "Max Mathys",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3HSmeM7yDULrLjSetp4qxKC5JlZVlKNk0xUMAxQ=s64",
      "userId": "08899314479848539391"
     },
     "user_tz": -60
    },
    "id": "yP4i522l-m7o"
   },
   "outputs": [],
   "source": [
    "def get_random_classes(number_of_classes: int = 50, min_rand_class: int = 0, max_rand_class: int = 999):\n",
    "  return np.random.randint(low=min_rand_class, high=max_rand_class, size=(number_of_classes,))\n",
    "\n",
    "CLASS_SUBSET = get_random_classes()\n",
    "\n",
    "BATCH_SIZE = 6 # You can play around with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6wDjrMNFIQO"
   },
   "source": [
    "# Import DINO\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "30f3de6d2e7546de8ae0970b681d9321",
      "09409788d2b1447780862ce96f8e5959",
      "8158e31b171a4909b0675e4cbc9730de",
      "b13de2ba75f64a7d9b99daf0d8d1d5fd",
      "7f935cadb4184b9392ea3d10ce5441ae",
      "a59b4495fbaa4efd956d2c61fa21396f",
      "6db8f03b74ff409f9420f7fa6dc1fafa",
      "8e8e241877cc4e6ba76e218048252b48",
      "6d8668a5da684526a77f61c0470c2611",
      "333cfed6d85a4844a3b424cc2af42af7",
      "b95076f42b8b437bb16aa5e48d58eae7"
     ]
    },
    "executionInfo": {
     "elapsed": 7398,
     "status": "ok",
     "timestamp": 1639137655419,
     "user": {
      "displayName": "Max Mathys",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3HSmeM7yDULrLjSetp4qxKC5JlZVlKNk0xUMAxQ=s64",
      "userId": "08899314479848539391"
     },
     "user_tz": -60
    },
    "id": "F14gzRs4vaU9",
    "outputId": "0841c340-4640-4495-e58b-d41a4bc6fd9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/dino/archive/main.zip\" to /root/.cache/torch/hub/main.zip\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_deitsmall8_pretrain/dino_deitsmall8_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dino_deitsmall8_pretrain.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f3de6d2e7546de8ae0970b681d9321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/82.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pretrained weights from PyTorch\n",
    "device = 'cuda'\n",
    "vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits8').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1639137655423,
     "user": {
      "displayName": "Max Mathys",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3HSmeM7yDULrLjSetp4qxKC5JlZVlKNk0xUMAxQ=s64",
      "userId": "08899314479848539391"
     },
     "user_tz": -60
    },
    "id": "9TRurWCDFsr3"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N_LAST_BLOCKS = 4 # Took from official repo\n",
    "PATCH_SIZE=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1639138719116,
     "user": {
      "displayName": "Max Mathys",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3HSmeM7yDULrLjSetp4qxKC5JlZVlKNk0xUMAxQ=s64",
      "userId": "08899314479848539391"
     },
     "user_tz": -60
    },
    "id": "7mNTPKGX1AOY",
    "outputId": "aaffdf29-6176-40cb-a889-112fee8b4fc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and load pretrained weights for linear classifier on ImageNet\n",
    "from torch import nn\n",
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(dim, num_labels) \n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # linear layer\n",
    "        return self.linear(x)\n",
    "\n",
    "linear_classifier = LinearClassifier(vits16.embed_dim * N_LAST_BLOCKS, num_labels=1000)\n",
    "linear_classifier = linear_classifier.cuda()\n",
    "\n",
    "linear_state_dict = torch.hub.load_state_dict_from_url(url=\"https://dl.fbaipublicfiles.com/dino/\" + \"dino_deitsmall8_pretrain/dino_deitsmall8_linearweights.pth\")[\"state_dict\"]\n",
    "\n",
    "# Update state dict to avoid crash. Workaround.\n",
    "linear_state_dict['linear.weight'] = linear_state_dict.pop('module.linear.weight')\n",
    "linear_state_dict['linear.bias'] = linear_state_dict.pop('module.linear.bias')\n",
    "\n",
    "# Load pre-trained weights\n",
    "linear_classifier.load_state_dict(linear_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoZjtdI3HGQU"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1639140072536,
     "user": {
      "displayName": "Max Mathys",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3HSmeM7yDULrLjSetp4qxKC5JlZVlKNk0xUMAxQ=s64",
      "userId": "08899314479848539391"
     },
     "user_tz": -60
    },
    "id": "zuyk3ebWU-Cc"
   },
   "outputs": [],
   "source": [
    "# Custom loader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import traceback\n",
    "from PIL import Image\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "  def __init__(self, img_folder: str, file_name: str, transform: callable, class_subset: List[int] = None):\n",
    "    super().__init__()\n",
    "    self.transform=transform\n",
    "    self.img_folder=img_folder\n",
    "    self.data = self.create_df(file_name)\n",
    "    self.class_subset = class_subset\n",
    "    if self.class_subset is None:\n",
    "      self.data_subset = self.data\n",
    "    else:\n",
    "      self.data_subset = self.data[self.data['label'].isin(self.class_subset)]\n",
    "  \n",
    "  def create_df(self, file_name: str):\n",
    "    df = pd.read_csv(file_name, sep=\" \", header=None)\n",
    "    df.columns=['file', 'label']\n",
    "    return df\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data_subset)\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    img = Image.open(os.path.join(self.img_folder,self.data_subset['file'].iloc[index]))\n",
    "    img = img.convert('RGB')\n",
    "\n",
    "    img=self.transform(img)\n",
    "    target=self.data_subset['label'].iloc[index]\n",
    "\n",
    "    return img,target,self.data_subset['file'].iloc[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1639140076162,
     "user": {
      "displayName": "Max Mathys",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3HSmeM7yDULrLjSetp4qxKC5JlZVlKNk0xUMAxQ=s64",
      "userId": "08899314479848539391"
     },
     "user_tz": -60
    },
    "id": "4AqLkfGWj8f1"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms as pth_transforms\n",
    "\n",
    "# Create loader\n",
    "# Taken from official repo: https://github.com/facebookresearch/dino/blob/main/eval_linear.py\n",
    "transform = pth_transforms.Compose([\n",
    "    pth_transforms.Resize(256, interpolation=3),\n",
    "    pth_transforms.CenterCrop(224),\n",
    "    pth_transforms.ToTensor(),\n",
    "    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "adv_dataset = ImageDataset(img_folder = ADVERSARIAL_IMAGES_PATH, \n",
    "                           file_name = ADV_LABEL_PATH,\n",
    "                           transform=transform,\n",
    "                           class_subset=CLASS_SUBSET)\n",
    "\n",
    "org_dataset = ImageDataset(img_folder = ORIGINAL_IMAGES_PATH,\n",
    "                           file_name = ORG_LABEL_PATH,\n",
    "                           transform=transform,\n",
    "                           class_subset=CLASS_SUBSET)\n",
    "\n",
    "org_loader = torch.utils.data.DataLoader(\n",
    "    org_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "adv_loader = torch.utils.data.DataLoader(\n",
    "    adv_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Qhe36va9J8-"
   },
   "source": [
    "Important note: adversarial labels contain the actual class for the image. Therefore, attack succeeds if prediction is different from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JszFxUgBLB_N"
   },
   "source": [
    "# Adversarial dataset generation (Drive)\n",
    "\n",
    "- original sample is correctly classified by DINO\n",
    "- their adversarial attack actually changes the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1639139852467,
     "user": {
      "displayName": "Max Mathys",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3HSmeM7yDULrLjSetp4qxKC5JlZVlKNk0xUMAxQ=s64",
      "userId": "08899314479848539391"
     },
     "user_tz": -60
    },
    "id": "AAqHb4hdOwXo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Performs a forward pass given a sample `inp` and a classifier.\n",
    "@torch.no_grad()\n",
    "def forward_pass(inp, model, classifier, n):\n",
    "  with torch.no_grad():\n",
    "    intermediate_output = model.get_intermediate_layers(inp, n)\n",
    "    output = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
    "    output = classifier(output)\n",
    "    return output.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1639140082437,
     "user": {
      "displayName": "Max Mathys",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3HSmeM7yDULrLjSetp4qxKC5JlZVlKNk0xUMAxQ=s64",
      "userId": "08899314479848539391"
     },
     "user_tz": -60
    },
    "id": "S5UL3Y_cLFHH"
   },
   "outputs": [],
   "source": [
    "# A Python generator for adversarial samples.\n",
    "# Takes original and adversarial loaders, a model, classifier and yields\n",
    "# a pair of original and adversarial samples based on the definition above.\n",
    "def adv_dataset(org_loader, adv_loader, model, classifier=linear_classifier, n=N_LAST_BLOCKS):\n",
    "  linear_classifier.eval()\n",
    "  model.eval()\n",
    "  for org, adv in zip(org_loader, adv_loader):\n",
    "    # parse the original sample\n",
    "    org_inp, org_tar, org_img_name = org\n",
    "    org_inp = org_inp.to(device)\n",
    "    org_tar = org_tar.to(device)\n",
    "\n",
    "    # parse the adversarial sample\n",
    "    adv_inp, adv_tar, adv_img_name = adv\n",
    "    adv_inp = adv_inp.to(device)\n",
    "    adv_tar = adv_tar.to(device)\n",
    "\n",
    "    # forward pass original and adversarial sample\n",
    "    org_pred = forward_pass(org_inp, model, linear_classifier, n)\n",
    "    adv_pred = forward_pass(adv_inp, model, linear_classifier, n)\n",
    "\n",
    "    #print(f\"Original prediction: {org_pred}\")\n",
    "    #print(f\"Adversarial prediction: {adv_pred}\")\n",
    "    #print(f\"Correct label: {org_tar}\")\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    # label, original image predicted class, adversarial image predicted class\n",
    "    for y, org_y, adv_y, org_x, adv_x, org_name, adv_name in zip(org_tar, org_pred, adv_pred, org_inp, adv_inp, org_img_name, adv_img_name):\n",
    "      # yield a new tuple based if the conditions match. skip otherwise.\n",
    "      org_correct = y == org_y\n",
    "      adv_correct = y == adv_y\n",
    "\n",
    "      org_num = int(org_name.replace('.JPEG', '').split(\"_\")[-1])\n",
    "      adv_num = int(adv_name.replace('.png', '').split(\"_\")[-1])\n",
    "      assert org_num == adv_num, \"Numbers are not matching\"\n",
    "\n",
    "      if org_correct and not adv_correct:\n",
    "        yield org_num, org_x, adv_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ol_2ut3CPx3c"
   },
   "source": [
    "Optionally: save self-attention for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miAWVtfHL7hb"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# output first 5 tuples generated.\n",
    "samples = adv_dataset(org_loader, adv_loader, vits16, linear_classifier)\n",
    "total=1000\n",
    "\n",
    "for i in range(total):\n",
    "  print(i)\n",
    "  num, org, adv = next(samples)\n",
    "  sys.stdout.write(f\"\\rtuple {i+1}/{total}\")\n",
    "  sys.stdout.flush()\n",
    "\n",
    "  # self attention\n",
    "  # add one dimension to input image (get_last_selfattention expects it)\n",
    "  org = org.unsqueeze(0)\n",
    "  adv = adv.unsqueeze(0)\n",
    "  org_attn = vits16.get_last_selfattention(org)\n",
    "  adv_attn = vits16.get_last_selfattention(adv)\n",
    "  \n",
    "  #torch.save(org_attn, ORG_ATTN_PATH + f\"{num}.pt\")\n",
    "  #torch.save(adv_attn, ADV_ATTN_PATH + f\"{num}.pt\")\n",
    "\n",
    "  # folders for org and adv, filename: {org, adv}/<original number>.pt (leave out prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmqoO_ThN81O"
   },
   "source": [
    "# Embedding generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1639138896233,
     "user": {
      "displayName": "Max Mathys",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3HSmeM7yDULrLjSetp4qxKC5JlZVlKNk0xUMAxQ=s64",
      "userId": "08899314479848539391"
     },
     "user_tz": -60
    },
    "id": "K5TtM-7pO_vP",
    "outputId": "dc2b7164-af41-4254-a6ea-cc63b9596c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/DeepLearningProject/AdversarialAttacks/org_emb_mean_patches/\n",
      "/content/drive/MyDrive/DeepLearningProject/AdversarialAttacks/adv_emb_mean_patches/\n"
     ]
    }
   ],
   "source": [
    "ORG_EMB_PATH = os.path.join(ADV_PATH,'org_emb_mean_patches/')\n",
    "ADV_EMB_PATH = os.path.join(ADV_PATH,'adv_emb_mean_patches/')\n",
    "print(ORG_EMB_PATH)\n",
    "print(ADV_EMB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2322,
     "status": "ok",
     "timestamp": 1639140100818,
     "user": {
      "displayName": "Max Mathys",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3HSmeM7yDULrLjSetp4qxKC5JlZVlKNk0xUMAxQ=s64",
      "userId": "08899314479848539391"
     },
     "user_tz": -60
    },
    "id": "ROcb-qNXSnoW",
    "outputId": "ccb13d19-35a3-46d1-c9be-51f60d7828c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')\n",
    "!rm /content/drive/MyDrive/DeepLearningProject/AdversarialAttacks/org_emb_mean_patches/*\n",
    "!rm /content/drive/MyDrive/DeepLearningProject/AdversarialAttacks/adv_emb_mean_patches/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1639140104440,
     "user": {
      "displayName": "Max Mathys",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3HSmeM7yDULrLjSetp4qxKC5JlZVlKNk0xUMAxQ=s64",
      "userId": "08899314479848539391"
     },
     "user_tz": -60
    },
    "id": "BSNTwqvMODG0"
   },
   "outputs": [],
   "source": [
    "def generate_model_output(inp, model, n=N_LAST_BLOCKS, target_tokens=\"CLS\"):\n",
    "    inp = inp[:, :, :].unsqueeze(0)\n",
    "    intermediate_output = model.get_intermediate_layers(inp, n)\n",
    "    if target_tokens == \"CLS\":\n",
    "      return torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
    "    elif target_tokens == \"patches_mean\":\n",
    "      return torch.cat([x[:, 1:].mean(1) for x in intermediate_output], dim=-1)\n",
    "    else:\n",
    "      raise Exception(\"invalid target_tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zzo7guXvO-kg"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# output first 5 tuples generated.\n",
    "samples = adv_dataset(org_loader, adv_loader, vits16, linear_classifier)\n",
    "\n",
    "# generator for cls embeddings\n",
    "def cls_embed_gen(samples):\n",
    "  for num, org, adv in samples:\n",
    "    org_out = generate_model_output(org, vits16, target_tokens=\"CLS\")\n",
    "    adv_out = generate_model_output(adv, vits16, target_tokens=\"CLS\")\n",
    "    yield num, org_out, adv_out\n",
    "\n",
    "# generator for mean of all patch embeddings\n",
    "def mean_patches_gen(samples):\n",
    "  for num, org, adv in samples:\n",
    "    org_out = generate_model_output(org, vits16, target_tokens=\"patches_mean\")\n",
    "    adv_out = generate_model_output(adv, vits16, target_tokens=\"patches_mean\")\n",
    "    yield num, org_out, adv_out\n",
    "\n",
    "# generator for variance of patch embeddings\n",
    "def var_gen(samples):\n",
    "  for num, org, adv in samples:\n",
    "    org_out = generate_model_output(org, vits16, target_tokens=\"patches_mean\")\n",
    "    adv_out = generate_model_output(adv, vits16, target_tokens=\"patches_mean\")\n",
    "    yield num, org_out, adv_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdGj5AZNT_34"
   },
   "source": [
    "# Adversarial Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKFsliWdhVDN"
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpSNtu9DouKx"
   },
   "outputs": [],
   "source": [
    "class AdverserialAttentionDataset(Dataset):\n",
    "    \"\"\"Adverserial Attention dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the .pt files that contain\n",
    "                both attention of the original img and of the adv. img.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        names_org = [f[:-3]+\"_org\" for f in listdir(root_dir)]\n",
    "        names_adv = [f[:-3]+\"_adv\" for f in listdir(root_dir)]\n",
    "        self.n = len(names_org)\n",
    "        self.file_names = names_org + names_adv\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        attn_file = self.file_names[idx]\n",
    "        adv_idx = 0\n",
    "        if attn_file[-3:0] == \"adv\":\n",
    "          adv_idx = 1\n",
    "        attn_file = attn_file[:-4]\n",
    "        \n",
    "        attn_path = os.path.join(self.root_dir, attn_file+\".pt\")\n",
    "        attention = torch.load(attn_path)\n",
    "      \n",
    "        sample = {'attention': attention[adv_idx].squeeze(), 'adverserial': adv_idx}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8joLS0dSl299"
   },
   "outputs": [],
   "source": [
    "# test load attention and visualize\n",
    "import sys\n",
    "\n",
    "ATTN_DIR = \"/content/drive/MyDrive/Deep Learning Project/AdversarialAttacks/attentions/\"\n",
    "\n",
    "test = torch.load(ATTN_DIR+f\"{0}.pt\")\n",
    "print(test[0].shape)\n",
    "\n",
    "attn_dataset = AdverserialAttentionDataset(root_dir=ATTN_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiQIjF6qw21h"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# # Ignore warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# plt.ion()   # interactive mode\n",
    "\n",
    "# fig = plt.figure(figsize=(16, 12), dpi=80)\n",
    "\n",
    "# for i in range(10,len(attn_dataset)):\n",
    "#     i -= 10\n",
    "#     sample = attn_dataset[i]\n",
    "#     # print(sample)\n",
    "\n",
    "#     # print(i, sample['attention'].shape, sample['adverserial'])\n",
    "\n",
    "#     ax = plt.subplot(1, 4, i + 1)\n",
    "#     plt.tight_layout()\n",
    "#     ax.set_title('Sample #{}'.format(i))\n",
    "#     ax.axis('off')\n",
    "#     cpu_tensor = sample['attention'].cpu().detach().squeeze()\n",
    "    \n",
    "#     # plot all the attentions on top of eachother\n",
    "#     for l in range(6):\n",
    "#       plt.imshow(cpu_tensor.numpy()[l,:,:])\n",
    "#     if i == 3:\n",
    "#       plt.show()\n",
    "#       break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoQpwfpni3jc"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "# Training set and training loader\n",
    "dataset_size = len(attn_dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(attn_dataset, \n",
    "                                                            [train_size, \n",
    "                                                             test_size],\n",
    "                                                            generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "# Test set and test loader\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=True, num_workers=0)\n",
    "\n",
    "classes = ('non-adversarial', 'adversarial') # Binary classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6dfDvJWYx1W"
   },
   "source": [
    "## Define CNN Classifier\n",
    "2 CNN Layers, 3 FC Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbtqts3GYiSL"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4d8y8OV2YjZM"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer = optim.adam(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIVk8j1VYl62"
   },
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, start=0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        print(data)\n",
    "        print(data['attention'].shape)\n",
    "        print(data['adverserial'].shape)\n",
    "        inputs, labels = data\n",
    "        print(inputs)\n",
    "        attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "        attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1pdytJPYqx0"
   },
   "outputs": [],
   "source": [
    "PATH = './simple_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mHyLUbqYt32"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNaSSFT2hkWd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uARZ90zvI7V6"
   },
   "source": [
    "# Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgW5Qjfs8krR"
   },
   "outputs": [],
   "source": [
    "# Adapted from official repo\n",
    "@torch.no_grad()\n",
    "def test_network(data_loader, model, linear_classifier, n=N_LAST_BLOCKS):\n",
    "  linear_classifier.eval()\n",
    "  model.eval()\n",
    "\n",
    "  predictions = []\n",
    "\n",
    "  for inp, tar in data_loader:\n",
    "    inp = inp.to(device)\n",
    "    tar = tar.to(device)\n",
    "\n",
    "    print(tar)\n",
    "\n",
    "    # forward\n",
    "    with torch.no_grad():\n",
    "      intermediate_output = model.get_intermediate_layers(inp, n)\n",
    "      output = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
    "      output = linear_classifier(output)\n",
    "      predictions += output.argmax(1).tolist()\n",
    "      print(output.argmax(1))\n",
    "      print(\"\\n\")\n",
    "\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oytyCTDhKQmE"
   },
   "outputs": [],
   "source": [
    "predictions = test_network(org_loader, vits16, linear_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBqnLoOu9j1r"
   },
   "outputs": [],
   "source": [
    "predictions = test_network(adv_loader, vits16, linear_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NTVkBDdm-kH"
   },
   "source": [
    "# Visualize attention\n",
    "Taken from: https://github.com/facebookresearch/dino/blob/main/visualize_attention.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8plFR_unCgC"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import colorsys\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import skimage.io\n",
    "from skimage.measure import find_contours\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms as pth_transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import utils\n",
    "import vision_transformer as vits\n",
    "\n",
    "\n",
    "def apply_mask(image, mask, color, alpha=0.5):\n",
    "    for c in range(3):\n",
    "        image[:, :, c] = image[:, :, c] * (1 - alpha * mask) + alpha * mask * color[c] * 255\n",
    "    return image\n",
    "\n",
    "\n",
    "def random_colors(N, bright=True):\n",
    "    \"\"\"\n",
    "    Generate random colors.\n",
    "    \"\"\"\n",
    "    brightness = 1.0 if bright else 0.7\n",
    "    hsv = [(i / N, 1, brightness) for i in range(N)]\n",
    "    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n",
    "    random.shuffle(colors)\n",
    "    return colors\n",
    "\n",
    "\n",
    "def display_instances(image, mask, fname=\"test\", figsize=(5, 5), blur=False, contour=True, alpha=0.5):\n",
    "    fig = plt.figure(figsize=figsize, frameon=False)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    N = 1\n",
    "    mask = mask[None, :, :]\n",
    "    # Generate random colors\n",
    "    colors = random_colors(N)\n",
    "\n",
    "    # Show area outside image boundaries.\n",
    "    height, width = image.shape[:2]\n",
    "    margin = 0\n",
    "    ax.set_ylim(height + margin, -margin)\n",
    "    ax.set_xlim(-margin, width + margin)\n",
    "    ax.axis('off')\n",
    "    masked_image = image.astype(np.uint32).copy()\n",
    "    for i in range(N):\n",
    "        color = colors[i]\n",
    "        _mask = mask[i]\n",
    "        if blur:\n",
    "            _mask = cv2.blur(_mask,(10,10))\n",
    "        # Mask\n",
    "        masked_image = apply_mask(masked_image, _mask, color, alpha)\n",
    "        # Mask Polygon\n",
    "        # Pad to ensure proper polygons for masks that touch image edges.\n",
    "        if contour:\n",
    "            padded_mask = np.zeros((_mask.shape[0] + 2, _mask.shape[1] + 2))\n",
    "            padded_mask[1:-1, 1:-1] = _mask\n",
    "            contours = find_contours(padded_mask, 0.5)\n",
    "            for verts in contours:\n",
    "                # Subtract the padding and flip (y, x) to (x, y)\n",
    "                verts = np.fliplr(verts) - 1\n",
    "                p = Polygon(verts, facecolor=\"none\", edgecolor=color)\n",
    "                ax.add_patch(p)\n",
    "    ax.imshow(masked_image.astype(np.uint8), aspect='auto')\n",
    "    fig.savefig(fname)\n",
    "    print(f\"{fname} saved.\")\n",
    "    return\n",
    "\n",
    "PATCH_SIZE = 8\n",
    "THRESHOLD = 0.5\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/TEST\"\n",
    "INPUT_IMAGE = \"/content/drive/MyDrive/DeepLearningProject/AdversarialAttacks/ori_data/images/ILSVRC2012_val_00000130.JPEG\"\n",
    "model = vits16\n",
    "\n",
    "img = Image.open(INPUT_IMAGE)\n",
    "img = img.convert('RGB')\n",
    "\n",
    "transform = pth_transforms.Compose([\n",
    "    pth_transforms.Resize(256),\n",
    "    pth_transforms.ToTensor(),\n",
    "    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "img = transform(img)\n",
    "\n",
    "# make the image divisible by the patch size\n",
    "w, h = img.shape[1] - img.shape[1] % PATCH_SIZE, img.shape[2] - img.shape[2] % PATCH_SIZE\n",
    "img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "w_featmap = img.shape[-2] // PATCH_SIZE\n",
    "h_featmap = img.shape[-1] // PATCH_SIZE\n",
    "\n",
    "print(img.shape)\n",
    "attentions = model.get_last_selfattention(img.to(device))\n",
    "\n",
    "nh = attentions.shape[1] # number of heads\n",
    "\n",
    "# we keep only the output patch attention\n",
    "attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "\n",
    "attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=PATCH_SIZE, mode=\"nearest\")[0].detach().cpu().numpy()\n",
    "\n",
    "# save attentions heatmaps\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "torchvision.utils.save_image(torchvision.utils.make_grid(img, normalize=True, scale_each=True), os.path.join(OUTPUT_DIR, \"img.png\"))\n",
    "for j in range(nh):\n",
    "    fname = os.path.join(OUTPUT_DIR, \"attn-head\" + str(j) + \".png\")\n",
    "    plt.imsave(fname=fname, arr=attentions[j], format='png')\n",
    "    plt.imshow(attentions[j])\n",
    "    plt.show()\n",
    "    print(f\"{fname} saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7r3CUAK829N"
   },
   "source": [
    "# Sample forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bt56cn5384k-"
   },
   "outputs": [],
   "source": [
    "PATCH_SIZE = 16\n",
    "INPUT_IMAGE = \"/content/drive/MyDrive/DeepLearningProject/AdversarialAttacks/adversarial_data/images/ILSVRC2012_val_00000130.png\"\n",
    "model = vits16\n",
    "\n",
    "img = Image.open(INPUT_IMAGE)\n",
    "img = img.convert('RGB')\n",
    "\n",
    "transform = pth_transforms.Compose([\n",
    "    pth_transforms.Resize(256),\n",
    "    pth_transforms.ToTensor(),\n",
    "    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "img = transform(img)\n",
    "\n",
    "# make the image divisible by the patch size\n",
    "w, h = img.shape[1] - img.shape[1] % PATCH_SIZE, img.shape[2] - img.shape[2] % PATCH_SIZE\n",
    "img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "  intermediate_output = model.get_intermediate_layers(img.to(device), N_LAST_BLOCKS)\n",
    "  output = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
    "  output = linear_classifier(output)\n",
    "  print(output.argmax(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Pcne3tBXfR8"
   },
   "source": [
    "# Random code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dv-6bEHrWHbW"
   },
   "outputs": [],
   "source": [
    "# Use to create understandable groundtruth for ImageNET\n",
    "with open(\"/content/drive/MyDrive/Deep Learning Project/Adversarial Attacks/original_data/full_custom_groundtruth.txt\", \"w\") as f_write:\n",
    "  with open(\"/content/drive/MyDrive/Deep Learning Project/Adversarial Attacks/original_data/FULL_ILSVRC2012_validation_ground_truth.txt\", \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "      f_write.write(f\"ILSVRC2012_val_{i+1:08}.JPEG \"+line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MagWBBus5g_d"
   },
   "source": [
    "# Local copy\n",
    "\n",
    "Option to download all data instead of using GDrive mount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qG_QFNMK0byu"
   },
   "source": [
    "ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qsHsJuTq0UJO"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gdown --id 1ZoVlgzkFD9cyv6jdenQ7JUVuS9sXMpEG\n",
    "mkdir -p ImageNet\n",
    "tar -xvf ILSVRC2012_img_val.tar -C ImageNet\n",
    "rm ILSVRC2012_img_val.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcDDAy630Yx8"
   },
   "source": [
    "DAmageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 367259,
     "status": "ok",
     "timestamp": 1638719995999,
     "user": {
      "displayName": "Max Mathys",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3HSmeM7yDULrLjSetp4qxKC5JlZVlKNk0xUMAxQ=s64",
      "userId": "08899314479848539391"
     },
     "user_tz": -60
    },
    "id": "8maplp5v0XLA",
    "outputId": "633389ea-8511-42ee-90ec-397e476793c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1K2C38bWRCeuNRtWW6qbAF8xFjxB-969U\n",
      "To: /content/DAmageNet.zip\n",
      "100%|##########| 5.46G/5.46G [01:57<00:00, 46.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gdown --id 1K2C38bWRCeuNRtWW6qbAF8xFjxB-969U\n",
    "unzip \"/content/DAmageNet.zip\"\n",
    "rm DAmageNet.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApXJZUYo33-E"
   },
   "outputs": [],
   "source": [
    "ORIGINAL_IMAGES_PATH = '/content/ImageNet'\n",
    "ADVERSARIAL_IMAGES_PATH = '/content/DAmageNet/DAmageNet'\n",
    "!mkdir -p '/content/org_attn/'\n",
    "!mkdir -p '/content/adv_attn/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYcBNtMh65Ie"
   },
   "outputs": [],
   "source": [
    "# alternatively set attention path local as well\n",
    "ORG_ATTN_PATH = '/content/org_attn/'\n",
    "ADV_ATTN_PATH = '/content/adv_attn/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYBYUyfl7B6t"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Q6wDjrMNFIQO",
    "BoZjtdI3HGQU",
    "uARZ90zvI7V6",
    "9NTVkBDdm-kH",
    "O7r3CUAK829N",
    "4Pcne3tBXfR8",
    "MagWBBus5g_d",
    "WdGj5AZNT_34"
   ],
   "name": "Starting setup.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09409788d2b1447780862ce96f8e5959": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30f3de6d2e7546de8ae0970b681d9321": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8158e31b171a4909b0675e4cbc9730de",
       "IPY_MODEL_b13de2ba75f64a7d9b99daf0d8d1d5fd",
       "IPY_MODEL_7f935cadb4184b9392ea3d10ce5441ae"
      ],
      "layout": "IPY_MODEL_09409788d2b1447780862ce96f8e5959"
     }
    },
    "333cfed6d85a4844a3b424cc2af42af7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d8668a5da684526a77f61c0470c2611": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6db8f03b74ff409f9420f7fa6dc1fafa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f935cadb4184b9392ea3d10ce5441ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b95076f42b8b437bb16aa5e48d58eae7",
      "placeholder": "​",
      "style": "IPY_MODEL_333cfed6d85a4844a3b424cc2af42af7",
      "value": " 82.7M/82.7M [00:02&lt;00:00, 38.6MB/s]"
     }
    },
    "8158e31b171a4909b0675e4cbc9730de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6db8f03b74ff409f9420f7fa6dc1fafa",
      "placeholder": "​",
      "style": "IPY_MODEL_a59b4495fbaa4efd956d2c61fa21396f",
      "value": "100%"
     }
    },
    "8e8e241877cc4e6ba76e218048252b48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a59b4495fbaa4efd956d2c61fa21396f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b13de2ba75f64a7d9b99daf0d8d1d5fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d8668a5da684526a77f61c0470c2611",
      "max": 86728949,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8e8e241877cc4e6ba76e218048252b48",
      "value": 86728949
     }
    },
    "b95076f42b8b437bb16aa5e48d58eae7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
