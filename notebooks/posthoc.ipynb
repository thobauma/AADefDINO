{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posthoc Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This extension reloads external Python files\n",
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino\n",
    "from src.model.data import create_loader, adv_dataset\n",
    "from src.model.eval import validate_network\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "username = getpass.getuser()\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "\n",
    "DN_PATH = Path(DATA_PATH, 'damageNet')\n",
    "DN_LABEL_PATH = Path(DN_PATH, 'val_damagenet.txt')\n",
    "DN_IMAGES_PATH = Path(DN_PATH, 'images')\n",
    "\n",
    "ORI_PATH = Path(DATA_PATH, 'ori_data/validation/')\n",
    "ORI_LABEL_PATH = Path(ORI_PATH,'correct_labels.txt')\n",
    "ORI_IMAGES_PATH = Path(ORI_PATH,'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If CLASS_SUBSET is specified, INDEX_SUBSET will be ignored. Set CLASS_SUBSET=None if you want to use indexes.\n",
    "INDEX_SUBSET = get_random_indexes()\n",
    "CLASS_SUBSET = get_random_classes()\n",
    "INDEX_SUBSET = None\n",
    "CLASS_SUBSET = None\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!python $HOME/deeplearning/setup/collect_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import DINO\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "We load the reference pretrained linear weights.\n"
     ]
    }
   ],
   "source": [
    "model, linear_classifier = get_dino()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_loader = create_loader(ORI_IMAGES_PATH, ORI_LABEL_PATH, INDEX_SUBSET, CLASS_SUBSET, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_loader = create_loader(DN_IMAGES_PATH, DN_LABEL_PATH, INDEX_SUBSET, CLASS_SUBSET, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial sample generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs a forward pass given a sample `inp` and a classifier.\n",
    "def generate_model_output(inp, n=4):\n",
    "    inp = inp.to(\"cuda\")\n",
    "    # add one dimension to input image (get_intermediate_layers expects it)\n",
    "    inp = inp.unsqueeze(dim=0)\n",
    "    intermediate_output = model.get_intermediate_layers(inp, n)\n",
    "    return torch.cat([x[:, 0] for x in intermediate_output], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvTupleIterator:\n",
    "    def __init__(self, ori_loader, dn_loader, model, linear_classifier):\n",
    "        self.samples = adv_dataset(ori_loader, dn_loader, model, linear_classifier)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        num, org, adv = next(self.samples)\n",
    "        org_out = generate_model_output(org)\n",
    "        adv_out = generate_model_output(adv)\n",
    "        return num, org_out, adv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing iterator\n",
    "\n",
    "total=10\n",
    "samples = AdvTupleIterator(ori_loader, dn_loader, model, linear_classifier)\n",
    "\n",
    "for i in range(total):\n",
    "  num, org, adv = next(samples)\n",
    "  sys.stdout.write(f\"\\rtuple {i+1}/{total} ({num})\")\n",
    "  sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posthoc Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Binary Classifier Network\n",
    "class SimpleBC(nn.Module):\n",
    "    def __init__(self,input_shape):\n",
    "        super(SimpleBC,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,64)\n",
    "        self.fc2 = nn.Linear(64,32)\n",
    "        self.fc3 = nn.Linear(32,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f1db17652046>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10\n",
    "\n",
    "# Initialise network\n",
    "net = SimpleBC(1536)\n",
    "\n",
    "# Select device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "net.to(device)\n",
    "\n",
    "# Set model to train\n",
    "net.train()\n",
    "\n",
    "# define loss, optimizer, and scheduler\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.01, lr_decay=1e-08, weight_decay=0)\n",
    "# scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "\n",
    "losses = []\n",
    "accur = []\n",
    "\n",
    "# Train network\n",
    "pbar = tqdm(range(EPOCHS))\n",
    "for epoch in pbar:  # loop over the dataset multiple times\n",
    "\n",
    "    # Metrics\n",
    "    train_running_loss = 0.0\n",
    "    train_running_loss_mean = 0.0\n",
    "    train_acc = 0.0\n",
    "    train_acc_mean = 0.0\n",
    "    test_running_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader, start=0):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = net(inputs).float()\n",
    "        outputs = outputs.reshape(-1)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # Reset the gradient\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss train\n",
    "        train_running_loss += loss.item()\n",
    "        train_running_loss_mean = train_running_loss / (i+1)\n",
    "\n",
    "        # accuracy train\n",
    "        predicted = net(inputs).reshape(-1).detach().cpu().numpy().round()\n",
    "        acc_labels = labels\n",
    "        acc_labels = acc_labels.detach().cpu().numpy()\n",
    "        inter = np.equal(predicted, acc_labels)\n",
    "        train_acc += inter.mean()\n",
    "        train_acc_mean = train_acc / (i+1)\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for inputs, labels in test_loader:\n",
    "            try:\n",
    "                # get the inputs; data is a list of [inputs, labels] and write to device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).float()\n",
    "\n",
    "                # Forward Pass\n",
    "                outputs = net(inputs).float()\n",
    "                outputs = outputs.reshape(-1)\n",
    "\n",
    "                # loss test\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_running_loss += loss.item()\n",
    "\n",
    "                # accuracy test\n",
    "                outputs = outputs.detach().cpu().numpy().round()\n",
    "                comparison = np.equal(labels.detach().cpu().numpy(), outputs)\n",
    "                test_acc = comparison.mean()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Error: {}\".format(e))\n",
    "                pass\n",
    "\n",
    "    losses.append(train_running_loss_mean)\n",
    "    accur.append(train_acc_mean)\n",
    "    pbar.set_description(\"Ep: {}\\t Tr. Loss: {:.4f}\\t Tr. Acc: {:.4f}\\t T. Loss: {:.4f}\\t T. Acc: {:.4f}\".format(epoch, \n",
    "                                                                            train_running_loss_mean, \n",
    "                                                                            train_acc_mean, \n",
    "                                                                            test_running_loss, \n",
    "                                                                            test_acc))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
