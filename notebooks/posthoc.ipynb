{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posthoc Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This extension reloads external Python files\n",
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "from torch.utils.data import random_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino\n",
    "from src.model.data import create_loader, adv_dataset, ORIGINAL_TRANSFORM, NO_NORM_TRANSFORM, TO_TENSOR_TRANSFORM\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "username = getpass.getuser()\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "MAX_PATH = Path('/','cluster', 'scratch', 'mmathys', 'dl_data')\n",
    "# Path for intermediate outputs\n",
    "BASE_POSTHOC_PATH = Path(MAX_PATH, 'posthoc/')\n",
    "\n",
    "# Original Dataset\n",
    "ORI_PATH = Path(DATA_PATH, 'ori_data/')\n",
    "CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "\n",
    "TR_PATH = Path(ORI_PATH, 'train/')\n",
    "TR_ORI_LABEL_PATH = Path(TR_PATH,'correct_labels.txt')\n",
    "TR_ORI_IMAGES_PATH = Path(TR_PATH,'images')\n",
    "\n",
    "VAL_PATH = Path(ORI_PATH, 'validation/')\n",
    "VAL_ORI_LABEL_PATH = Path(VAL_PATH,'correct_labels.txt')\n",
    "VAL_ORI_IMAGES_PATH = Path(VAL_PATH,'images')\n",
    "\n",
    "# DAmageNet\n",
    "#DN_PATH = Path(DATA_PATH, 'damageNet')\n",
    "#DN_LABEL_PATH = Path(DN_PATH, 'val_damagenet.txt')\n",
    "#DN_IMAGES_PATH = Path(DN_PATH, 'images')\n",
    "#DN_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'damagenet')\n",
    "#DN_POSTHOC_LABEL_PATH = Path(DN_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "# PGD\n",
    "VAL_PGD_PATH = Path(DATA_PATH, 'adversarial_data/pgd_03/validation')\n",
    "VAL_PGD_LABEL_PATH = VAL_ORI_LABEL_PATH\n",
    "VAL_PGD_IMAGES_PATH = Path(VAL_PGD_PATH, 'images')\n",
    "VAL_PGD_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'pgd/validation/')\n",
    "VAL_PGD_POSTHOC_LABEL_PATH = Path(VAL_PGD_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "# CW\n",
    "VAL_CW_PATH = Path(MAX_PATH, 'adversarial_data/cw/validation')\n",
    "VAL_CW_LABEL_PATH = VAL_ORI_LABEL_PATH\n",
    "VAL_CW_IMAGES_PATH = Path(VAL_CW_PATH, 'images')\n",
    "VAL_CW_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'cw/validation/')\n",
    "VAL_CW_POSTHOC_LABEL_PATH = Path(VAL_CW_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "# FGSM\n",
    "TR_FGSM_PATH = Path(MAX_PATH, 'adversarial_data/fgsm_06/train')\n",
    "TR_FGSM_LABEL_PATH = TR_ORI_LABEL_PATH\n",
    "TR_FGSM_IMAGES_PATH = Path(TR_FGSM_PATH, 'images')\n",
    "TR_FGSM_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'fgsm/train/')\n",
    "TR_FGSM_POSTHOC_LABEL_PATH = Path(TR_FGSM_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "VAL_FGSM_PATH = Path(MAX_PATH, 'adversarial_data/fgsm_06/validation')\n",
    "VAL_FGSM_LABEL_PATH = VAL_ORI_LABEL_PATH\n",
    "VAL_FGSM_IMAGES_PATH = Path(VAL_FGSM_PATH, 'images')\n",
    "VAL_FGSM_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'fgsm/validation/')\n",
    "VAL_FGSM_POSTHOC_LABEL_PATH = Path(VAL_FGSM_POSTHOC_PATH, 'labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If CLASS_SUBSET is specified, INDEX_SUBSET will be ignored. Set CLASS_SUBSET=None if you want to use indexes.\n",
    "INDEX_SUBSET = get_random_indexes()\n",
    "CLASS_SUBSET = get_random_classes()\n",
    "INDEX_SUBSET = None\n",
    "\n",
    "CLASS_SUBSET = np.load(CLASS_SUBSET_PATH) # for train\n",
    "#CLASS_SUBSET = None # for validation\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CLASS_SUBSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!python $HOME/deeplearning/setup/collect_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import DINO\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights.\n"
     ]
    }
   ],
   "source": [
    "model, linear_classifier = get_dino()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ori_loader = create_loader(TR_ORI_IMAGES_PATH, TR_ORI_LABEL_PATH, INDEX_SUBSET, CLASS_SUBSET, BATCH_SIZE, transform=NO_NORM_TRANSFORM)\n",
    "\n",
    "\n",
    "val_ori_loader = create_loader(VAL_ORI_IMAGES_PATH, VAL_ORI_LABEL_PATH, INDEX_SUBSET, None, BATCH_SIZE, transform=NO_NORM_TRANSFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dn_loader = create_loader(DN_IMAGES_PATH, DN_LABEL_PATH, INDEX_SUBSET, CLASS_SUBSET, BATCH_SIZE, transform=NO_NORM_TRANSFORM)\n",
    "val_pgd_loader = create_loader(VAL_PGD_IMAGES_PATH, VAL_PGD_LABEL_PATH, INDEX_SUBSET, None, BATCH_SIZE, transform=TO_TENSOR_TRANSFORM)\n",
    "val_cw_loader = create_loader(VAL_CW_IMAGES_PATH, VAL_CW_LABEL_PATH, INDEX_SUBSET, None, BATCH_SIZE, transform=TO_TENSOR_TRANSFORM)\n",
    "tr_fgsm_loader = create_loader(TR_FGSM_IMAGES_PATH, TR_FGSM_LABEL_PATH, INDEX_SUBSET, CLASS_SUBSET, BATCH_SIZE, transform=TO_TENSOR_TRANSFORM)\n",
    "val_fgsm_loader = create_loader(VAL_FGSM_IMAGES_PATH, VAL_FGSM_LABEL_PATH, INDEX_SUBSET, None, BATCH_SIZE, transform=TO_TENSOR_TRANSFORM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Sample Tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs a forward pass given a sample `inp` and a classifier.\n",
    "def generate_model_output(inp, n=4):\n",
    "    inp = inp.to(\"cuda\")\n",
    "    # add one dimension to input image (get_intermediate_layers expects it)\n",
    "    inp = inp.unsqueeze(dim=0)\n",
    "    intermediate_output = model.get_intermediate_layers(inp, n)\n",
    "    return torch.cat([x[:, 0] for x in intermediate_output], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvTupleIterator:\n",
    "    def __init__(self, ori_loader, adv_loader, model, linear_classifier, max=0, visualize=False):\n",
    "        self.samples = adv_dataset(ori_loader, adv_loader, model, linear_classifier)\n",
    "        self.max = max\n",
    "        self.num = 0\n",
    "        self.visualize = visualize\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.num = 0\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        if(self.max > 0 and self.num >= self.max):\n",
    "            raise StopIteration\n",
    "        self.num += 1\n",
    "        # payload can be original or adversarial.\n",
    "        sample, payload, label = next(self.samples)\n",
    "        if self.visualize:\n",
    "            img = payload.permute(1, 2, 0)\n",
    "            plt.imshow(img.cpu(), interpolation='nearest')\n",
    "            plt.show()\n",
    "        payload_out = generate_model_output(payload)\n",
    "        if self.visualize:\n",
    "            print(f\"shape of image: {img.shape}\")\n",
    "            print(f\"shape of model output: {payload_out.shape}\")\n",
    "        return sample, payload_out, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing iterator\n",
    "tmp_ori_loader = tr_ori_loader\n",
    "#tmp_ori_loader = val_ori_loader\n",
    "#tmp_adv_loader = val_pgd_loader\n",
    "#tmp_adv_loader = val_cw_loader\n",
    "#tmp_adv_loader = tr_pgd_loader\n",
    "tmp_adv_loader = tr_fgsm_loader\n",
    "\n",
    "total=2\n",
    "samples = AdvTupleIterator(tmp_ori_loader, tmp_adv_loader, model, linear_classifier, visualize=True)\n",
    "\n",
    "for i in range(total):\n",
    "  num, payload, label = next(samples)\n",
    "  sys.stdout.write(f\"\\rpayload {i+1}/{total} (label {label})\")\n",
    "  sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to disk (once per dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk!\n",
    "limit = 10\n",
    "datasets = [\n",
    "    #{\"posthoc_path\": DN_POSTHOC_PATH,  \"label_path\": DN_POSTHOC_LABEL_PATH},\n",
    "    {\n",
    "        \"ori_loader\": val_ori_loader,\n",
    "        \"adv_loader\": val_pgd_loader,\n",
    "        \"posthoc_path\": VAL_PGD_POSTHOC_PATH,\n",
    "        \"label_path\": VAL_PGD_POSTHOC_LABEL_PATH\n",
    "    },\n",
    "    {\n",
    "        \"ori_loader\": val_ori_loader,\n",
    "        \"adv_loader\": val_cw_loader,\n",
    "        \"posthoc_path\": VAL_CW_POSTHOC_PATH,\n",
    "        \"label_path\": VAL_CW_POSTHOC_LABEL_PATH\n",
    "    },\n",
    "    {\n",
    "        \"ori_loader\": val_ori_loader,\n",
    "        \"adv_loader\": val_fgsm_loader,\n",
    "        \"posthoc_path\": VAL_FGSM_POSTHOC_PATH,\n",
    "        \"label_path\": VAL_FGSM_POSTHOC_LABEL_PATH\n",
    "    },\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "    ori_loader = dataset[\"ori_loader\"]\n",
    "    adv_loader = dataset[\"adv_loader\"]\n",
    "    posthoc_path = dataset[\"posthoc_path\"]\n",
    "    label_path = dataset[\"label_path\"]\n",
    "\n",
    "    print(f\"posthoc path: {posthoc_path}\")\n",
    "    print(f\"label path: {label_path}\")\n",
    "\n",
    "    iterator = AdvTupleIterator(ori_loader, adv_loader, model, linear_classifier, max=0)\n",
    "    names = []\n",
    "    paths = []\n",
    "    labels = []\n",
    "\n",
    "    for i, (name, payload, label) in enumerate(iterator):\n",
    "        if limit > 0 and i >= limit: break\n",
    "        sys.stdout.write(f\"\\r {i+1}/{limit} {name} (label {label})\")\n",
    "        sys.stdout.flush()\n",
    "        # original: 0, adversarial: 1\n",
    "        if label == 0:\n",
    "            path = f\"org/{name}\"\n",
    "        else:\n",
    "            path = f\"adv/{name}\"\n",
    "        paths.append(path)\n",
    "        names.append(name)\n",
    "        labels.append(label)\n",
    "        path = Path(posthoc_path, path)\n",
    "        torch.save(payload, path)\n",
    "\n",
    "    df = pd.DataFrame(data={\"path\": paths, \"name\": names, \"label\": labels})\n",
    "    df.to_csv(label_path, index=False)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset for reading from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_folder, index_df):\n",
    "        super().__init__()\n",
    "        self.img_folder = img_folder\n",
    "        self.index_df = index_df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename = self.index_df['path'].iloc[index]\n",
    "        label = self.index_df['label'].iloc[index]\n",
    "        payload = torch.load(Path(self.img_folder, filename))\n",
    "        return filename, payload, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posthoc Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Binary Classifier Network\n",
    "class SimpleBC(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(SimpleBC,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,64)\n",
    "        self.fc2 = nn.Linear(64,32)\n",
    "        self.fc3 = nn.Linear(32,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Binary Classifier\n",
    "class LinearBC(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(LinearBC,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGD:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TR_PGD_POSTHOC_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-c3369007dfc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m#train(DN_POSTHOC_PATH)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PGD:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTR_PGD_POSTHOC_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAL_PGD_POSTHOC_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CW:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAL_CW_POSTHOC_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAL_CW_POSTHOC_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TR_PGD_POSTHOC_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10\n",
    "\n",
    "# Initialise network\n",
    "net = LinearBC(1536)\n",
    "\n",
    "# Select device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "net.to(device)\n",
    "\n",
    "# Set model to train\n",
    "net.train()\n",
    "\n",
    "# define loss, optimizer, and scheduler\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.01, lr_decay=1e-08, weight_decay=0)\n",
    "# scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "\n",
    "losses = []\n",
    "accur = []\n",
    "\n",
    "train_files = []\n",
    "test_files = []\n",
    "\n",
    "def train(train_posthoc_path, val_posthoc_path):\n",
    "    idx_train_df = pd.read_csv(Path(train_posthoc_path, 'labels.csv'))\n",
    "    idx_val_df = pd.read_csv(Path(val_posthoc_path, 'labels.csv'))\n",
    "\n",
    "    #train_dataset, test_dataset = random_split(AdvDataset(posthoc_path, index_df), [train_len, test_len], generator=torch.Generator().manual_seed(42))\n",
    "    train_dataset = AdvDataset(train_posthoc_path, idx_train_df)\n",
    "    val_dataset = AdvDataset(val_posthoc_path, idx_val_df)\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=8, shuffle=False)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    # Train network\n",
    "    pbar = tqdm(range(EPOCHS))\n",
    "    for epoch in pbar:  # loop over the dataset multiple times\n",
    "\n",
    "        # Metrics\n",
    "        train_running_loss = 0.0\n",
    "        train_running_loss_mean = 0.0\n",
    "        train_acc = 0.0\n",
    "        train_acc_mean = 0.0\n",
    "        test_running_loss = 0.0\n",
    "        test_acc = 0.0\n",
    "\n",
    "        for i, (filename, inputs, labels) in enumerate(train_loader, start=0):        \n",
    "            train_files.append(filename)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            # Forward Pass\n",
    "            outputs = net(inputs).float()\n",
    "            outputs = outputs.reshape(-1)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad() # Reset the gradient\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss train\n",
    "            train_running_loss += loss.item()\n",
    "            train_running_loss_mean = train_running_loss / (i+1)\n",
    "\n",
    "            # accuracy train\n",
    "            predicted = net(inputs).reshape(-1).detach().cpu().numpy().round()\n",
    "            acc_labels = labels\n",
    "            acc_labels = acc_labels.detach().cpu().numpy()\n",
    "            inter = np.equal(predicted, acc_labels)\n",
    "            train_acc += inter.sum()\n",
    "            #train_acc_mean = train_acc / (i+1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for filename, inputs, labels in val_loader:\n",
    "                test_files.append(filename)\n",
    "                try:\n",
    "                    # get the inputs; data is a list of [inputs, labels] and write to device\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device).float()\n",
    "\n",
    "                    # Forward Pass\n",
    "                    outputs = net(inputs).float()\n",
    "                    outputs = outputs.reshape(-1)\n",
    "\n",
    "                    # loss test\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    test_running_loss += loss.item()\n",
    "\n",
    "                    # accuracy test\n",
    "                    outputs = outputs.detach().cpu().numpy().round()\n",
    "                    comparison = np.equal(labels.detach().cpu().numpy(), outputs)\n",
    "                    test_acc += comparison.sum()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Error: {}\".format(e))\n",
    "                    pass\n",
    "\n",
    "        losses.append(train_running_loss_mean)\n",
    "        accur.append(train_acc_mean)\n",
    "        pbar.set_description(\"Ep: {}\\t Tr. Loss: {:.4f}\\t Tr. Acc: {:.4f}\\t T. Loss: {:.4f}\\t T. Acc: {:.4f}\".format(epoch, \n",
    "                                                                                train_running_loss_mean, \n",
    "                                                                                train_acc / len(train_loader.dataset), \n",
    "                                                                                test_running_loss, \n",
    "                                                                                test_acc / len(val_loader.dataset)))\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "#print(\"DAmageNet:\")\n",
    "#train(DN_POSTHOC_PATH)\n",
    "print(\"PGD:\")\n",
    "train(TR_PGD_POSTHOC_PATH, VAL_PGD_POSTHOC_PATH)\n",
    "print(\"CW:\")\n",
    "train(VAL_CW_POSTHOC_PATH, VAL_CW_POSTHOC_PATH)\n",
    "print(\"FGSM:\")\n",
    "train(VAL_FGSM_POSTHOC_PATH, VAL_FGSM_POSTHOC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
