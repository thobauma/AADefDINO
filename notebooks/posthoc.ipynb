{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posthoc Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This extension reloads external Python files\n",
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "from torch.utils.data import random_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino\n",
    "from src.model.data import create_loader, adv_dataset, ORIGINAL_TRANSFORM, ONLY_NORMALIZE_TRANSFORM\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "username = getpass.getuser()\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "MAX_PATH = Path('/','cluster', 'scratch', 'mmathys', 'dl_data')\n",
    "# Path for intermediate outputs\n",
    "BASE_POSTHOC_PATH = Path(MAX_PATH, 'posthoc/')\n",
    "\n",
    "# Original Dataset\n",
    "ORI_PATH = Path(DATA_PATH, 'ori_data/')\n",
    "CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "\n",
    "TR_PATH = Path(ORI_PATH, 'train/')\n",
    "TR_ORI_LABEL_PATH = Path(TR_PATH,'correct_labels.txt')\n",
    "TR_ORI_IMAGES_PATH = Path(TR_PATH,'images')\n",
    "\n",
    "VAL_PATH = Path(ORI_PATH, 'validation/')\n",
    "VAL_ORI_LABEL_PATH = Path(VAL_PATH,'correct_labels.txt')\n",
    "VAL_ORI_IMAGES_PATH = Path(VAL_PATH,'images')\n",
    "\n",
    "# DAmageNet\n",
    "#DN_PATH = Path(DATA_PATH, 'damageNet')\n",
    "#DN_LABEL_PATH = Path(DN_PATH, 'val_damagenet.txt')\n",
    "#DN_IMAGES_PATH = Path(DN_PATH, 'images')\n",
    "#DN_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'damagenet')\n",
    "#DN_POSTHOC_LABEL_PATH = Path(DN_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "# PGD\n",
    "TR_PGD_PATH = Path(MAX_PATH, 'adversarial_data/pgd_06/train')\n",
    "TR_PGD_LABEL_PATH = TR_ORI_LABEL_PATH\n",
    "TR_PGD_IMAGES_PATH = Path(TR_PGD_PATH, 'images')\n",
    "TR_PGD_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'pgd/train/')\n",
    "TR_PGD_POSTHOC_LABEL_PATH = Path(TR_PGD_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "VAL_PGD_PATH = Path(MAX_PATH, 'adversarial_data/pgd_03/validation')\n",
    "VAL_PGD_LABEL_PATH = VAL_ORI_LABEL_PATH\n",
    "VAL_PGD_IMAGES_PATH = Path(VAL_PGD_PATH, 'images')\n",
    "VAL_PGD_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'pgd/validation/')\n",
    "VAL_PGD_POSTHOC_LABEL_PATH = Path(VAL_PGD_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "# CW\n",
    "TR_CW_PATH = Path(MAX_PATH, 'adversarial_data/cw/train')\n",
    "TR_CW_LABEL_PATH = TR_ORI_LABEL_PATH\n",
    "TR_CW_IMAGES_PATH = Path(TR_CW_PATH, 'images')\n",
    "TR_CW_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'cw/train/')\n",
    "TR_CW_POSTHOC_LABEL_PATH = Path(TR_CW_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "VAL_CW_PATH = Path(MAX_PATH, 'adversarial_data/cw/validation')\n",
    "VAL_CW_LABEL_PATH = VAL_ORI_LABEL_PATH\n",
    "VAL_CW_IMAGES_PATH = Path(VAL_CW_PATH, 'images')\n",
    "VAL_CW_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'cw/validation/')\n",
    "VAL_CW_POSTHOC_LABEL_PATH = Path(VAL_CW_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "# FGSM\n",
    "TR_FGSM_PATH = Path(MAX_PATH, 'adversarial_data/fgsm_06/train')\n",
    "TR_FGSM_LABEL_PATH = TR_ORI_LABEL_PATH\n",
    "TR_FGSM_IMAGES_PATH = Path(TR_FGSM_PATH, 'images')\n",
    "TR_FGSM_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'fgsm/train/')\n",
    "TR_FGSM_POSTHOC_LABEL_PATH = Path(TR_FGSM_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "VAL_FGSM_PATH = Path(MAX_PATH, 'adversarial_data/fgsm_06/validation')\n",
    "VAL_FGSM_LABEL_PATH = VAL_ORI_LABEL_PATH\n",
    "VAL_FGSM_IMAGES_PATH = Path(VAL_FGSM_PATH, 'images')\n",
    "VAL_FGSM_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'fgsm/validation/')\n",
    "VAL_FGSM_POSTHOC_LABEL_PATH = Path(VAL_FGSM_POSTHOC_PATH, 'labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If CLASS_SUBSET is specified, INDEX_SUBSET will be ignored. Set CLASS_SUBSET=None if you want to use indexes.\n",
    "INDEX_SUBSET = get_random_indexes()\n",
    "CLASS_SUBSET = get_random_classes()\n",
    "INDEX_SUBSET = None\n",
    "\n",
    "CLASS_SUBSET = np.load(CLASS_SUBSET_PATH) # for train\n",
    "#CLASS_SUBSET = None # for validation\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CLASS_SUBSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!python $HOME/deeplearning/setup/collect_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import DINO\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights.\n"
     ]
    }
   ],
   "source": [
    "model, linear_classifier = get_dino()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ori_loader = create_loader(TR_ORI_IMAGES_PATH, TR_ORI_LABEL_PATH, INDEX_SUBSET, CLASS_SUBSET, BATCH_SIZE, transform=ORIGINAL_TRANSFORM)\n",
    "val_ori_loader = create_loader(VAL_ORI_IMAGES_PATH, VAL_ORI_LABEL_PATH, INDEX_SUBSET, None, BATCH_SIZE, transform=ORIGINAL_TRANSFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dn_loader = create_loader(DN_IMAGES_PATH, DN_LABEL_PATH, INDEX_SUBSET, CLASS_SUBSET, BATCH_SIZE, transform=NO_NORM_TRANSFORM)\n",
    "tr_pgd_loader = create_loader(TR_PGD_IMAGES_PATH, TR_PGD_LABEL_PATH, INDEX_SUBSET, CLASS_SUBSET, BATCH_SIZE, transform=ONLY_NORMALIZE_TRANSFORM)\n",
    "val_pgd_loader = create_loader(VAL_PGD_IMAGES_PATH, VAL_PGD_LABEL_PATH, INDEX_SUBSET, None, BATCH_SIZE, transform=ONLY_NORMALIZE_TRANSFORM)\n",
    "tr_cw_loader = create_loader(TR_CW_IMAGES_PATH, TR_CW_LABEL_PATH, INDEX_SUBSET, CLASS_SUBSET, BATCH_SIZE, transform=ONLY_NORMALIZE_TRANSFORM)\n",
    "val_cw_loader = create_loader(VAL_CW_IMAGES_PATH, VAL_CW_LABEL_PATH, INDEX_SUBSET, None, BATCH_SIZE, transform=ONLY_NORMALIZE_TRANSFORM)\n",
    "tr_fgsm_loader = create_loader(TR_FGSM_IMAGES_PATH, TR_FGSM_LABEL_PATH, INDEX_SUBSET, CLASS_SUBSET, BATCH_SIZE, transform=ONLY_NORMALIZE_TRANSFORM)\n",
    "val_fgsm_loader = create_loader(VAL_FGSM_IMAGES_PATH, VAL_FGSM_LABEL_PATH, INDEX_SUBSET, None, BATCH_SIZE, transform=ONLY_NORMALIZE_TRANSFORM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Sample Tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs a forward pass given a sample `inp` and a classifier.\n",
    "def generate_model_output(inp, n=4):\n",
    "    inp = inp.to(\"cuda\")\n",
    "    # add one dimension to input image (get_intermediate_layers expects it)\n",
    "    inp = inp.unsqueeze(dim=0)\n",
    "    intermediate_output = model.get_intermediate_layers(inp, n)\n",
    "    return torch.cat([x[:, 0] for x in intermediate_output], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvTupleIterator:\n",
    "    def __init__(self, ori_loader, adv_loader, model, linear_classifier, max=0, visualize=False, verbose=False):\n",
    "        self.samples = adv_dataset(ori_loader, adv_loader, model, linear_classifier, verbose=verbose)\n",
    "        self.max = max\n",
    "        self.num = 0\n",
    "        self.visualize = visualize\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.num = 0\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        if(self.max > 0 and self.num >= self.max):\n",
    "            raise StopIteration\n",
    "        self.num += 1\n",
    "        # payload can be original or adversarial.\n",
    "        sample, payload, label = next(self.samples)\n",
    "        if self.visualize:\n",
    "            img = payload.permute(1, 2, 0)\n",
    "            plt.imshow(img.cpu(), interpolation='nearest')\n",
    "            plt.show()\n",
    "        payload_out = generate_model_output(payload)\n",
    "        if self.visualize:\n",
    "            print(f\"shape of image: {img.shape}\")\n",
    "            print(f\"shape of model output: {payload_out.shape}\")\n",
    "        return sample, payload_out, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adv_dataset is verbose.\n",
      "tuple: n02093859_4002 y: 107 org: 233 adv: 233 \n",
      "tuple: n02093859_3652 y: 107 org: 183 adv: 183 \n",
      "tuple: n01980166_5317 y: 615 org: 120 adv: 120 \n",
      "tuple: n03376595_9240 y: 309 org: 559 adv: 559 \n",
      "tuple: n03617480_5886 y: 770 org: 614 adv: 614 \n",
      "tuple: n03617480_163 y: 770 org: 614 adv: 614 \n",
      "tuple: n03376595_9054 y: 309 org: 559 adv: 559 \n",
      "tuple: n02094114_8542 y: 88 org: 185 adv: 185 \n",
      "tuple: n03259280_6109 y: 662 org: 544 adv: 544 \n",
      "tuple: n01872401_4981 y: 215 org: 102 adv: 102 \n",
      "tuple: n02092002_3929 y: 21 org: 213 adv: 177 \n",
      "tuple: n02489166_12861 y: 100 org: 376 adv: 376 \n",
      "tuple: n02892767_24507 y: 872 org: 459 adv: 459 \n",
      "tuple: n02489166_10601 y: 100 org: 376 adv: 376 \n",
      "tuple: n02489166_9988 y: 100 org: 376 adv: 376 \n",
      "tuple: n02483708_559 y: 122 org: 369 adv: 369 \n",
      "tuple: n02892767_16755 y: 872 org: 459 adv: 459 \n",
      "tuple: n02980441_9554 y: 701 org: 483 adv: 483 \n",
      "tuple: n06874185_22023 y: 861 org: 920 adv: 920 \n",
      "tuple: n01687978_2093 y: 467 org: 42 adv: 42 \n",
      "tuple: n01872401_6581 y: 215 org: 102 adv: 102 \n",
      "tuple: n02093859_4397 y: 107 org: 183 adv: 183 \n",
      "tuple: n04442312_9997 y: 664 org: 859 adv: 859 \n",
      "tuple: n01665541_33125 y: 459 org: 34 adv: 34 \n",
      "tuple: n06874185_10554 y: 861 org: 920 adv: 920 \n",
      "tuple: n12144580_1622 y: 331 org: 987 adv: 998 \n",
      "tuple: n02093859_5269 y: 107 org: 189 adv: 189 \n",
      "tuple: n03376595_15353 y: 309 org: 559 adv: 559 \n",
      "tuple: n06874185_21762 y: 861 org: 925 adv: 925 \n",
      "tuple: n01755581_8545 y: 492 org: 54 adv: 54 \n",
      "tuple: n03617480_2066 y: 770 org: 614 adv: 614 \n",
      "tuple: n01872401_5902 y: 215 org: 102 adv: 102 \n",
      "tuple: n03617480_14392 y: 770 org: 614 adv: 614 \n",
      "tuple: n06874185_4995 y: 861 org: 971 adv: 417 \n",
      "tuple: n02134084_17622 y: 103 org: 296 adv: 296 \n",
      "tuple: n02033041_1865 y: 436 org: 142 adv: 142 \n",
      "tuple: n03259280_37 y: 662 org: 813 adv: 925 \n",
      "tuple: n02483708_1891 y: 122 org: 369 adv: 369 \n",
      "tuple: n01872401_4029 y: 215 org: 102 adv: 102 \n",
      "tuple: n01665541_9650 y: 459 org: 34 adv: 34 \n",
      "tuple: n03770679_15707 y: 271 org: 656 adv: 656 \n",
      "tuple: n02980441_6112 y: 701 org: 483 adv: 483 \n",
      "tuple: n02092002_5071 y: 21 org: 177 adv: 177 \n",
      "tuple: n03617480_759 y: 770 org: 614 adv: 614 \n",
      "tuple: n03259280_11759 y: 662 org: 544 adv: 544 \n",
      "tuple: n03495258_1221 y: 344 org: 594 adv: 594 \n",
      "tuple: n02097658_8266 y: 131 org: 201 adv: 201 \n",
      "tuple: n01980166_4178 y: 615 org: 120 adv: 120 \n",
      "tuple: n02092002_1632 y: 21 org: 170 adv: 170 \n",
      "tuple: n02134084_17 y: 103 org: 296 adv: 296 \n",
      "tuple: n02980441_8198 y: 701 org: 483 adv: 975 \n",
      "tuple: n01665541_43377 y: 459 org: 34 adv: 34 \n",
      "tuple: n01755581_18029 y: 492 org: 67 adv: 67 \n",
      "tuple: n06874185_27160 y: 861 org: 920 adv: 555 \n",
      "tuple: n01665541_1528 y: 459 org: 34 adv: 34 \n",
      "tuple: n02483708_1470 y: 122 org: 369 adv: 369 \n",
      "tuple: n02095889_11949 y: 72 org: 190 adv: 190 \n",
      "tuple: n02483708_7353 y: 122 org: 381 adv: 256 \n",
      "tuple: n01872401_1479 y: 215 org: 102 adv: 102 \n",
      "tuple: n04442312_15355 y: 664 org: 648 adv: 648 \n",
      "tuple: n02483708_2521 y: 122 org: 369 adv: 369 \n",
      "tuple: n03617480_8911 y: 770 org: 824 adv: 824 \n",
      "tuple: n03617480_32175 y: 770 org: 614 adv: 614 \n",
      "tuple: n03259280_3129 y: 662 org: 544 adv: 544 \n",
      "tuple: n01872401_14421 y: 215 org: 102 adv: 102 \n",
      "tuple: n01665541_29088 y: 459 org: 33 adv: 33 \n",
      "tuple: n03617480_14051 y: 770 org: 614 adv: 614 \n",
      "tuple: n03495258_25664 y: 344 org: 594 adv: 594 \n",
      "tuple: n02093859_3784 y: 107 org: 183 adv: 183 \n",
      "tuple: n02095889_534 y: 72 org: 190 adv: 190 \n",
      "tuple: n03376595_2351 y: 309 org: 431 adv: 431 \n",
      "tuple: n02097658_366 y: 131 org: 200 adv: 200 \n",
      "tuple: n12144580_1470 y: 331 org: 987 adv: 987 \n",
      "tuple: n02093859_565 y: 107 org: 183 adv: 183 \n",
      "tuple: n02095889_271 y: 72 org: 202 adv: 202 \n",
      "tuple: n03495258_5865 y: 344 org: 594 adv: 594 \n",
      "tuple: n01755581_4794 y: 492 org: 67 adv: 67 \n",
      "tuple: n02095889_504 y: 72 org: 190 adv: 190 \n",
      "tuple: n02092002_15917 y: 21 org: 170 adv: 177 \n",
      "tuple: n02489166_8778 y: 100 org: 376 adv: 376 \n",
      "tuple: n02489166_12301 y: 100 org: 376 adv: 376 \n",
      "tuple: n02892767_13971 y: 872 org: 578 adv: 578 \n",
      "tuple: n01665541_13892 y: 459 org: 34 adv: 34 \n",
      "tuple: n02097658_1063 y: 131 org: 201 adv: 201 \n",
      "tuple: n01687978_5687 y: 467 org: 42 adv: 48 \n",
      "tuple: n02033041_3328 y: 436 org: 142 adv: 142 \n",
      "tuple: n04442312_29825 y: 664 org: 859 adv: 859 \n",
      "tuple: n01980166_3194 y: 615 org: 120 adv: 120 \n",
      "tuple: n03495258_3090 y: 344 org: 594 adv: 594 \n",
      "tuple: n01665541_12364 y: 459 org: 34 adv: 34 \n",
      "tuple: n03617480_7139 y: 770 org: 614 adv: 614 \n",
      "tuple: n02489166_9022 y: 100 org: 376 adv: 376 \n",
      "tuple: n02033041_2001 y: 436 org: 142 adv: 142 \n",
      "tuple: n06874185_25677 y: 861 org: 920 adv: 920 \n",
      "tuple: n01755581_11817 y: 492 org: 67 adv: 68 \n",
      "tuple: n03495258_15099 y: 344 org: 594 adv: 594 \n",
      "tuple: n03770679_8618 y: 271 org: 656 adv: 656 \n",
      "tuple: n02134084_10566 y: 103 org: 296 adv: 296 \n",
      "tuple: n02489166_14332 y: 100 org: 376 adv: 376 \n",
      "tuple: n03495258_19189 y: 344 org: 594 adv: 594 \n",
      "tuple: n03376595_7351 y: 309 org: 559 adv: 559 \n",
      "tuple: n02094114_7900 y: 88 org: 160 adv: 220 \n",
      "tuple: n03259280_11666 y: 662 org: 544 adv: 544 \n",
      "tuple: n04442312_2717 y: 664 org: 859 adv: 921 \n",
      "tuple: n03495258_12960 y: 344 org: 594 adv: 594 \n",
      "tuple: n02134084_15424 y: 103 org: 296 adv: 296 \n",
      "tuple: n02489166_7268 y: 100 org: 376 adv: 376 \n",
      "tuple: n02892767_7454 y: 872 org: 459 adv: 459 \n",
      "tuple: n12144580_6873 y: 331 org: 666 adv: 666 \n",
      "tuple: n04442312_33528 y: 664 org: 859 adv: 861 \n",
      "tuple: n03995372_6633 y: 373 org: 784 adv: 784 \n",
      "tuple: n02095889_6879 y: 72 org: 229 adv: 229 \n",
      "tuple: n02033041_10553 y: 436 org: 142 adv: 142 \n",
      "tuple: n02093859_3782 y: 107 org: 183 adv: 183 \n",
      "tuple: n02980441_9391 y: 701 org: 483 adv: 483 \n",
      "tuple: n06874185_31760 y: 861 org: 920 adv: 920 \n",
      "tuple: n03995372_19035 y: 373 org: 549 adv: 549 \n",
      "tuple: n03770679_29923 y: 271 org: 609 adv: 609 \n",
      "tuple: n02094114_6226 y: 88 org: 185 adv: 185 \n",
      "tuple: n04442312_10251 y: 664 org: 651 adv: 651 \n",
      "tuple: n03495258_4962 y: 344 org: 754 adv: 618 \n",
      "tuple: n01980166_7238 y: 615 org: 120 adv: 120 \n",
      "tuple: n03259280_3041 y: 662 org: 544 adv: 544 \n",
      "tuple: n02097658_2429 y: 131 org: 193 adv: 193 \n",
      "tuple: n04442312_8295 y: 664 org: 859 adv: 859 \n",
      "tuple: n03259280_6499 y: 662 org: 652 adv: 652 \n",
      "tuple: n03770679_17397 y: 271 org: 656 adv: 656 \n",
      "tuple: n01980166_9180 y: 615 org: 120 adv: 120 \n",
      "tuple: n02094114_2897 y: 88 org: 185 adv: 189 \n",
      "tuple: n03376595_13978 y: 309 org: 559 adv: 559 \n",
      "tuple: n02134084_15764 y: 103 org: 296 adv: 296 \n",
      "tuple: n02094114_2955 y: 88 org: 185 adv: 185 \n",
      "tuple: n02093859_2699 y: 107 org: 183 adv: 183 \n",
      "tuple: n01980166_4013 y: 615 org: 120 adv: 120 \n",
      "tuple: n02134084_42216 y: 103 org: 296 adv: 296 \n",
      "tuple: n02489166_4002 y: 100 org: 371 adv: 371 \n",
      "tuple: n03495258_3483 y: 344 org: 594 adv: 594 \n",
      "tuple: n02483708_1464 y: 122 org: 365 adv: 365 \n",
      "tuple: n03770679_27668 y: 271 org: 436 adv: 436 \n",
      "tuple: n12144580_2315 y: 331 org: 813 adv: 680 \n",
      "tuple: n02980441_402 y: 701 org: 483 adv: 483 \n",
      "tuple: n01665541_3219 y: 459 org: 34 adv: 34 \n",
      "tuple: n01872401_5260 y: 215 org: 102 adv: 102 \n",
      "tuple: n02093859_1195 y: 107 org: 183 adv: 183 \n",
      "tuple: n01755581_546 y: 492 org: 67 adv: 66 \n",
      "tuple: n02093859_4487 y: 107 org: 197 adv: 197 \n",
      "tuple: n02097658_4148 y: 131 org: 201 adv: 201 \n",
      "tuple: n04442312_8324 y: 664 org: 859 adv: 859 \n",
      "tuple: n03259280_2319 y: 662 org: 544 adv: 544 \n",
      "tuple: n01687978_12605 y: 467 org: 42 adv: 42 \n",
      "tuple: n02094114_7105 y: 88 org: 185 adv: 185 \n",
      "tuple: n02980441_6653 y: 701 org: 483 adv: 483 \n",
      "tuple: n01980166_15 y: 615 org: 120 adv: 120 \n",
      "tuple: n02033041_4800 y: 436 org: 141 adv: 141 \n",
      "tuple: n01872401_581 y: 215 org: 102 adv: 102 \n",
      "tuple: n03495258_23687 y: 344 org: 594 adv: 594 \n",
      "tuple: n02094114_7010 y: 88 org: 669 adv: 254 \n",
      "tuple: n02095889_310 y: 72 org: 190 adv: 190 \n",
      "tuple: n04442312_27408 y: 664 org: 831 adv: 831 \n",
      "tuple: n02092002_28 y: 21 org: 169 adv: 169 \n",
      "tuple: n01665541_11070 y: 459 org: 34 adv: 34 \n",
      "tuple: n03770679_29496 y: 271 org: 656 adv: 656 \n",
      "tuple: n04442312_11217 y: 664 org: 859 adv: 859 \n",
      "tuple: n02095889_2840 y: 72 org: 190 adv: 190 \n",
      "tuple: n03995372_13267 y: 373 org: 801 adv: 801 \n",
      "tuple: n01872401_5726 y: 215 org: 102 adv: 102 \n",
      "tuple: n04442312_2398 y: 664 org: 859 adv: 859 \n",
      "tuple: n01872401_11420 y: 215 org: 102 adv: 102 \n",
      "tuple: n03995372_8549 y: 373 org: 592 adv: 592 \n",
      "tuple: n01687978_7307 y: 467 org: 42 adv: 42 \n",
      "tuple: n02094114_3762 y: 88 org: 185 adv: 185 \n",
      "tuple: n02483708_5883 y: 122 org: 368 adv: 369 \n",
      "tuple: n03376595_7042 y: 309 org: 489 adv: 489 \n",
      "tuple: n02892767_3149 y: 872 org: 459 adv: 459 \n",
      "tuple: n06874185_32561 y: 861 org: 733 adv: 733 \n",
      "tuple: n02892767_19471 y: 872 org: 459 adv: 459 \n",
      "tuple: n03376595_5876 y: 309 org: 559 adv: 559 \n",
      "tuple: n04442312_2100 y: 664 org: 859 adv: 859 \n",
      "tuple: n03259280_1556 y: 662 org: 469 adv: 469 \n",
      "tuple: n02093859_3161 y: 107 org: 199 adv: 199 \n",
      "tuple: n03617480_15023 y: 770 org: 399 adv: 399 \n",
      "tuple: n02093859_4134 y: 107 org: 183 adv: 183 \n",
      "tuple: n02092002_5437 y: 21 org: 177 adv: 177 \n",
      "tuple: n03376595_11014 y: 309 org: 559 adv: 559 \n",
      "tuple: n01665541_5613 y: 459 org: 34 adv: 34 \n",
      "tuple: n02483708_9482 y: 122 org: 368 adv: 368 \n",
      "tuple: n03495258_492 y: 344 org: 594 adv: 594 \n",
      "tuple: n12144580_11397 y: 331 org: 987 adv: 987 \n",
      "tuple: n03770679_5848 y: 271 org: 656 adv: 656 \n",
      "tuple: n02095889_2329 y: 72 org: 190 adv: 190 \n",
      "tuple: n01665541_4745 y: 459 org: 34 adv: 34 \n",
      "tuple: n02033041_1138 y: 436 org: 141 adv: 141 \n",
      "tuple: n01687978_11086 y: 467 org: 42 adv: 42 \n",
      "tuple: n02483708_7419 y: 122 org: 381 adv: 369 \n",
      "tuple: n03617480_7857 y: 770 org: 614 adv: 697 \n",
      "tuple: n03495258_26604 y: 344 org: 594 adv: 594 \n",
      "tuple: n03995372_1482 y: 373 org: 534 adv: 571 \n",
      "tuple: n02093859_895 y: 107 org: 183 adv: 183 \n",
      "tuple: n02033041_13209 y: 436 org: 142 adv: 142 \n",
      "tuple: n03617480_4775 y: 770 org: 697 adv: 697 \n",
      "tuple: n06874185_15182 y: 861 org: 920 adv: 920 \n",
      "tuple: n01755581_12960 y: 492 org: 67 adv: 67 \n",
      "tuple: n01872401_10492 y: 215 org: 102 adv: 102 \n",
      "tuple: n01665541_3963 y: 459 org: 34 adv: 34 \n",
      "tuple: n01872401_32786 y: 215 org: 102 adv: 102 \n",
      "tuple: n02483708_6883 y: 122 org: 381 adv: 367 \n",
      "tuple: n01980166_2176 y: 615 org: 120 adv: 120 \n",
      "tuple: n03617480_51 y: 770 org: 614 adv: 614 \n",
      "tuple: n01980166_102 y: 615 org: 120 adv: 120 \n",
      "tuple: n03617480_2193 y: 770 org: 614 adv: 614 \n",
      "tuple: n03995372_7059 y: 373 org: 740 adv: 740 \n",
      "tuple: n01980166_129 y: 615 org: 120 adv: 120 \n",
      "tuple: n01665541_15794 y: 459 org: 34 adv: 34 \n",
      "tuple: n02980441_3995 y: 701 org: 483 adv: 483 \n",
      "tuple: n02094114_4518 y: 88 org: 185 adv: 185 \n",
      "tuple: n03617480_3587 y: 770 org: 982 adv: 399 \n",
      "tuple: n02092002_12079 y: 21 org: 175 adv: 244 \n",
      "tuple: n03259280_11479 y: 662 org: 544 adv: 659 \n",
      "tuple: n02093859_3703 y: 107 org: 183 adv: 183 \n",
      "tuple: n02094114_8131 y: 88 org: 242 adv: 211 \n",
      "tuple: n02134084_13700 y: 103 org: 296 adv: 296 \n",
      "tuple: n01687978_1076 y: 467 org: 42 adv: 42 \n",
      "tuple: n02489166_5466 y: 100 org: 376 adv: 376 \n",
      "tuple: n02093859_2921 y: 107 org: 183 adv: 183 \n",
      "tuple: n03376595_6986 y: 309 org: 559 adv: 559 \n",
      "tuple: n03617480_21151 y: 770 org: 549 adv: 549 \n",
      "tuple: n02892767_26410 y: 872 org: 669 adv: 669 \n",
      "tuple: n01872401_4117 y: 215 org: 102 adv: 102 \n",
      "tuple: n03376595_3835 y: 309 org: 559 adv: 559 \n",
      "tuple: n03617480_2938 y: 770 org: 614 adv: 614 \n",
      "tuple: n04442312_6677 y: 664 org: 859 adv: 859 \n",
      "tuple: n03995372_17936 y: 373 org: 622 adv: 686 \n",
      "tuple: n01980166_3633 y: 615 org: 120 adv: 120 \n",
      "tuple: n02097658_670 y: 131 org: 201 adv: 201 \n",
      "tuple: n02134084_424 y: 103 org: 296 adv: 296 \n",
      "tuple: n02980441_10002 y: 701 org: 832 adv: 832 \n",
      "tuple: n03770679_2115 y: 271 org: 734 adv: 654 \n",
      "tuple: n01687978_13278 y: 467 org: 42 adv: 42 \n",
      "tuple: n02489166_4559 y: 100 org: 376 adv: 376 \n",
      "tuple: n03995372_8567 y: 373 org: 571 adv: 571 \n",
      "tuple: n02092002_1945 y: 21 org: 177 adv: 177 \n",
      "tuple: n02489166_4745 y: 100 org: 376 adv: 376 \n",
      "tuple: n02097658_659 y: 131 org: 201 adv: 201 \n",
      "tuple: n01980166_10405 y: 615 org: 120 adv: 120 \n",
      "tuple: n01755581_6741 y: 492 org: 54 adv: 54 \n",
      "tuple: n01687978_5578 y: 467 org: 42 adv: 42 \n",
      "tuple: n02489166_11909 y: 100 org: 376 adv: 376 \n",
      "tuple: n02033041_5578 y: 436 org: 142 adv: 142 \n",
      "tuple: n02892767_14378 y: 872 org: 459 adv: 459 \n",
      "tuple: n01687978_9803 y: 467 org: 42 adv: 42 \n",
      "tuple: n04442312_5124 y: 664 org: 859 adv: 521 \n",
      "tuple: n06874185_14176 y: 861 org: 920 adv: 920 \n",
      "tuple: n02892767_10425 y: 872 org: 445 adv: 445 \n",
      "tuple: n12144580_35335 y: 331 org: 998 adv: 998 \n",
      "tuple: n01872401_4766 y: 215 org: 102 adv: 102 \n",
      "tuple: n02483708_1150 y: 122 org: 369 adv: 369 \n",
      "tuple: n03770679_25830 y: 271 org: 654 adv: 654 \n",
      "tuple: n01755581_22984 y: 492 org: 67 adv: 67 \n",
      "tuple: n03617480_6670 y: 770 org: 614 adv: 614 \n",
      "tuple: n02097658_3035 y: 131 org: 187 adv: 187 \n",
      "tuple: n01665541_9961 y: 459 org: 34 adv: 34 \n",
      "tuple: n01687978_2015 y: 467 org: 42 adv: 42 \n",
      "tuple: n01687978_754 y: 467 org: 42 adv: 42 \n",
      "tuple: n06874185_3685 y: 861 org: 920 adv: 920 \n",
      "tuple: n02980441_11772 y: 701 org: 483 adv: 483 \n",
      "tuple: n04442312_12096 y: 664 org: 859 adv: 859 \n",
      "tuple: n03495258_12821 y: 344 org: 594 adv: 594 \n",
      "tuple: n12144580_28708 y: 331 org: 987 adv: 987 \n",
      "tuple: n04442312_22062 y: 664 org: 648 adv: 648 \n",
      "tuple: n02097658_2534 y: 131 org: 187 adv: 187 \n",
      "tuple: n12144580_11764 y: 331 org: 998 adv: 998 \n",
      "tuple: n02134084_22801 y: 103 org: 296 adv: 296 \n",
      "tuple: n03770679_9852 y: 271 org: 656 adv: 656 \n",
      "tuple: n01687978_11133 y: 467 org: 42 adv: 42 \n",
      "tuple: n02095889_7326 y: 72 org: 207 adv: 207 \n",
      "tuple: n04442312_11965 y: 664 org: 570 adv: 570 \n",
      "tuple: n03617480_9096 y: 770 org: 614 adv: 614 \n",
      "tuple: n02033041_5839 y: 436 org: 142 adv: 142 \n",
      "tuple: n02892767_1768 y: 872 org: 459 adv: 459 \n",
      "tuple: n03259280_4088 y: 662 org: 544 adv: 544 \n",
      "tuple: n04442312_15350 y: 664 org: 859 adv: 729 \n",
      "tuple: n03495258_28274 y: 344 org: 594 adv: 594 \n",
      "tuple: n01755581_11781 y: 492 org: 67 adv: 67 \n",
      "tuple: n02892767_36522 y: 872 org: 459 adv: 459 \n",
      "tuple: n02033041_5928 y: 436 org: 142 adv: 142 \n",
      "tuple: n01872401_6925 y: 215 org: 102 adv: 102 \n",
      "tuple: n01687978_5013 y: 467 org: 42 adv: 42 \n",
      "tuple: n01872401_17959 y: 215 org: 102 adv: 102 \n",
      "tuple: n03376595_7824 y: 309 org: 559 adv: 559 \n",
      "tuple: n01980166_2805 y: 615 org: 120 adv: 120 \n",
      "tuple: n12144580_9335 y: 331 org: 987 adv: 987 \n",
      "tuple: n12144580_6333 y: 331 org: 987 adv: 998 \n",
      "tuple: n02980441_33266 y: 701 org: 483 adv: 483 \n",
      "tuple: n02092002_7491 y: 21 org: 242 adv: 242 \n",
      "tuple: n03376595_6115 y: 309 org: 559 adv: 559 \n",
      "tuple: n03995372_15213 y: 373 org: 740 adv: 740 \n",
      "tuple: n03617480_2507 y: 770 org: 614 adv: 614 \n",
      "tuple: n01755581_7634 y: 492 org: 67 adv: 67 \n",
      "tuple: n02134084_742 y: 103 org: 296 adv: 296 \n",
      "tuple: n04442312_7963 y: 664 org: 648 adv: 648 \n",
      "tuple: n01980166_833 y: 615 org: 120 adv: 120 \n",
      "tuple: n03495258_4021 y: 344 org: 594 adv: 594 \n",
      "tuple: n01980166_8198 y: 615 org: 120 adv: 120 \n",
      "tuple: n02483708_4567 y: 122 org: 369 adv: 369 \n",
      "tuple: n03995372_3864 y: 373 org: 845 adv: 845 \n",
      "tuple: n02134084_3647 y: 103 org: 296 adv: 296 \n",
      "tuple: n03495258_19605 y: 344 org: 594 adv: 594 \n",
      "tuple: n12144580_13008 y: 331 org: 998 adv: 998 \n",
      "tuple: n02092002_2648 y: 21 org: 177 adv: 177 \n",
      "tuple: n02095889_9982 y: 72 org: 190 adv: 190 \n",
      "tuple: n02134084_2177 y: 103 org: 296 adv: 296 \n",
      "tuple: n02094114_3928 y: 88 org: 185 adv: 185 \n",
      "tuple: n03376595_4242 y: 309 org: 559 adv: 559 \n",
      "tuple: n02980441_12858 y: 701 org: 483 adv: 483 \n",
      "tuple: n02095889_14737 y: 72 org: 190 adv: 190 \n",
      "tuple: n02097658_1507 y: 131 org: 201 adv: 201 \n",
      "tuple: n02489166_5131 y: 100 org: 376 adv: 376 \n",
      "tuple: n01980166_4365 y: 615 org: 120 adv: 120 \n",
      "tuple: n02097658_8824 y: 131 org: 187 adv: 187 \n",
      "tuple: n02095889_14326 y: 72 org: 190 adv: 190 \n",
      "tuple: n01687978_4770 y: 467 org: 42 adv: 42 \n",
      "tuple: n02093859_3645 y: 107 org: 785 adv: 257 \n",
      "tuple: n03376595_4891 y: 309 org: 765 adv: 765 \n",
      "tuple: n01980166_400 y: 615 org: 120 adv: 120 \n",
      "tuple: n02093859_373 y: 107 org: 183 adv: 183 \n",
      "tuple: n02097658_2461 y: 131 org: 187 adv: 187 \n",
      "tuple: n03259280_4234 y: 662 org: 111 adv: 111 \n",
      "tuple: n02033041_8373 y: 436 org: 142 adv: 142 \n",
      "tuple: n02892767_18204 y: 872 org: 459 adv: 459 \n",
      "tuple: n03617480_7635 y: 770 org: 614 adv: 614 \n",
      "tuple: n03495258_21778 y: 344 org: 594 adv: 594 \n",
      "tuple: n06874185_22473 y: 861 org: 920 adv: 405 \n",
      "tuple: n02892767_9971 y: 872 org: 459 adv: 459 \n",
      "tuple: n02980441_38076 y: 701 org: 483 adv: 483 \n",
      "tuple: n02033041_937 y: 436 org: 142 adv: 142 \n",
      "tuple: n02092002_28644 y: 21 org: 154 adv: 154 \n",
      "tuple: n02483708_5724 y: 122 org: 369 adv: 369 \n",
      "tuple: n01687978_3148 y: 467 org: 42 adv: 42 \n",
      "tuple: n02092002_6503 y: 21 org: 177 adv: 177 \n",
      "tuple: n02489166_11746 y: 100 org: 376 adv: 376 \n",
      "tuple: n02095889_11213 y: 72 org: 190 adv: 190 \n",
      "tuple: n02095889_7756 y: 72 org: 190 adv: 190 \n",
      "tuple: n02980441_927 y: 701 org: 483 adv: 483 \n",
      "tuple: n03617480_510 y: 770 org: 614 adv: 614 \n",
      "tuple: n12144580_15343 y: 331 org: 767 adv: 720 \n",
      "tuple: n02892767_22957 y: 872 org: 459 adv: 459 \n",
      "tuple: n02033041_14924 y: 436 org: 142 adv: 142 \n",
      "tuple: n02483708_3320 y: 122 org: 381 adv: 369 \n",
      "tuple: n01872401_2891 y: 215 org: 102 adv: 102 \n",
      "tuple: n01872401_7284 y: 215 org: 102 adv: 102 \n",
      "tuple: n02980441_5955 y: 701 org: 483 adv: 483 \n",
      "tuple: n02094114_5174 y: 88 org: 185 adv: 185 \n",
      "tuple: n03376595_10035 y: 309 org: 868 adv: 868 \n",
      "tuple: n03995372_544 y: 373 org: 543 adv: 543 \n",
      "tuple: n02483708_904 y: 122 org: 369 adv: 369 \n",
      "tuple: n01980166_893 y: 615 org: 120 adv: 120 \n",
      "tuple: n02094114_7494 y: 88 org: 189 adv: 189 \n",
      "tuple: n02489166_8455 y: 100 org: 376 adv: 376 \n",
      "tuple: n03495258_23090 y: 344 org: 594 adv: 594 \n",
      "tuple: n03376595_3566 y: 309 org: 977 adv: 977 \n",
      "tuple: n02097658_11479 y: 131 org: 201 adv: 201 \n",
      "tuple: n02095889_124 y: 72 org: 190 adv: 190 \n",
      "tuple: n04442312_16683 y: 664 org: 859 adv: 859 \n",
      "tuple: n02892767_8211 y: 872 org: 459 adv: 459 \n",
      "tuple: n02095889_940 y: 72 org: 190 adv: 190 \n",
      "tuple: n02093859_3785 y: 107 org: 223 adv: 223 \n",
      "tuple: n03617480_27073 y: 770 org: 982 adv: 399 \n",
      "tuple: n12144580_14430 y: 331 org: 998 adv: 998 \n",
      "tuple: n02483708_4999 y: 122 org: 369 adv: 369 \n",
      "tuple: n03995372_3697 y: 373 org: 445 adv: 445 \n",
      "tuple: n02980441_7669 y: 701 org: 483 adv: 483 \n",
      "tuple: n02095889_9359 y: 72 org: 190 adv: 190 \n",
      "tuple: n01665541_43884 y: 459 org: 34 adv: 34 \n",
      "tuple: n03995372_7851 y: 373 org: 506 adv: 506 \n",
      "tuple: n03617480_4038 y: 770 org: 614 adv: 614 \n",
      "tuple: n01755581_19831 y: 492 org: 67 adv: 68 \n",
      "tuple: n02093859_2226 y: 107 org: 183 adv: 183 \n",
      "tuple: n02094114_7331 y: 88 org: 171 adv: 171 \n",
      "tuple: n12144580_3839 y: 331 org: 987 adv: 987 \n",
      "tuple: n03376595_3357 y: 309 org: 765 adv: 765 \n",
      "tuple: n06874185_105129 y: 861 org: 920 adv: 920 \n",
      "tuple: n02489166_8809 y: 100 org: 376 adv: 376 \n",
      "tuple: n02483708_7377 y: 122 org: 369 adv: 369 \n",
      "tuple: n01980166_3181 y: 615 org: 120 adv: 876 \n",
      "tuple: n12144580_3831 y: 331 org: 998 adv: 998 \n",
      "tuple: n02483708_2437 y: 122 org: 369 adv: 369 \n",
      "tuple: n02980441_59977 y: 701 org: 483 adv: 483 \n",
      "tuple: n03617480_16477 y: 770 org: 614 adv: 614 \n",
      "tuple: n01872401_3475 y: 215 org: 102 adv: 102 \n",
      "tuple: n02892767_847 y: 872 org: 459 adv: 459 \n",
      "tuple: n01687978_13645 y: 467 org: 42 adv: 42 \n",
      "tuple: n01665541_11776 y: 459 org: 45 adv: 47 \n",
      "tuple: n02095889_9543 y: 72 org: 190 adv: 190 \n",
      "tuple: n01755581_11101 y: 492 org: 67 adv: 67 \n",
      "tuple: n03495258_6322 y: 344 org: 594 adv: 594 \n",
      "tuple: n06874185_8021 y: 861 org: 920 adv: 920 \n",
      "tuple: n01687978_339 y: 467 org: 42 adv: 42 \n",
      "tuple: n02489166_4724 y: 100 org: 376 adv: 376 \n",
      "tuple: n01980166_361 y: 615 org: 120 adv: 120 \n",
      "tuple: n02134084_7670 y: 103 org: 296 adv: 296 \n",
      "tuple: n01872401_3097 y: 215 org: 102 adv: 102 \n",
      "tuple: n02033041_4155 y: 436 org: 142 adv: 142 \n",
      "tuple: n03495258_19451 y: 344 org: 594 adv: 594 \n",
      "tuple: n03376595_10382 y: 309 org: 559 adv: 559 \n",
      "tuple: n01755581_12579 y: 492 org: 54 adv: 54 \n",
      "tuple: n02892767_14362 y: 872 org: 459 adv: 459 \n",
      "tuple: n02892767_28799 y: 872 org: 549 adv: 551 \n",
      "tuple: n03376595_2734 y: 309 org: 559 adv: 559 \n",
      "tuple: n03259280_10142 y: 662 org: 926 adv: 926 \n",
      "tuple: n03495258_17529 y: 344 org: 594 adv: 594 \n",
      "tuple: n12144580_8770 y: 331 org: 998 adv: 998 \n",
      "tuple: n01687978_2114 y: 467 org: 42 adv: 42 \n",
      "tuple: n03995372_6074 y: 373 org: 740 adv: 740 \n",
      "tuple: n02892767_17852 y: 872 org: 459 adv: 459 \n",
      "tuple: n03495258_10179 y: 344 org: 594 adv: 594 \n",
      "tuple: n01665541_1532 y: 459 org: 34 adv: 34 \n",
      "tuple: n03259280_10202 y: 662 org: 469 adv: 469 \n",
      "tuple: n03376595_11304 y: 309 org: 559 adv: 559 \n",
      "tuple: n02489166_5770 y: 100 org: 376 adv: 376 \n",
      "tuple: n01665541_5855 y: 459 org: 34 adv: 34 \n",
      "tuple: n02094114_2165 y: 88 org: 185 adv: 185 \n",
      "tuple: n03770679_13012 y: 271 org: 565 adv: 565 \n",
      "tuple: n12144580_841 y: 331 org: 987 adv: 998 \n",
      "tuple: n02134084_23974 y: 103 org: 296 adv: 296 \n",
      "tuple: n04442312_22632 y: 664 org: 648 adv: 648 \n",
      "tuple: n01872401_726 y: 215 org: 102 adv: 102 \n",
      "tuple: n02892767_20231 y: 872 org: 459 adv: 459 \n",
      "tuple: n02033041_10047 y: 436 org: 142 adv: 142 \n",
      "tuple: n02134084_15373 y: 103 org: 296 adv: 296 \n",
      "tuple: n02097658_6 y: 131 org: 187 adv: 187 \n",
      "tuple: n02092002_10759 y: 21 org: 177 adv: 177 \n",
      "tuple: n02489166_10605 y: 100 org: 376 adv: 376 \n",
      "tuple: n01872401_7981 y: 215 org: 102 adv: 102 \n",
      "tuple: n04442312_1442 y: 664 org: 859 adv: 859 \n",
      "tuple: n02033041_14714 y: 436 org: 142 adv: 142 \n",
      "tuple: n02095889_9206 y: 72 org: 190 adv: 190 \n",
      "tuple: n02489166_12549 y: 100 org: 376 adv: 376 \n",
      "tuple: n02033041_7044 y: 436 org: 142 adv: 142 \n",
      "tuple: n02095889_40 y: 72 org: 190 adv: 190 \n",
      "tuple: n03259280_3633 y: 662 org: 910 adv: 910 \n",
      "tuple: n01872401_6737 y: 215 org: 102 adv: 102 \n",
      "tuple: n01872401_316 y: 215 org: 102 adv: 102 \n",
      "tuple: n02483708_3421 y: 122 org: 369 adv: 369 \n",
      "tuple: n01687978_9247 y: 467 org: 42 adv: 42 \n",
      "tuple: n02095889_5371 y: 72 org: 153 adv: 190 \n",
      "tuple: n02092002_8108 y: 21 org: 160 adv: 177 \n",
      "tuple: n04442312_12803 y: 664 org: 913 adv: 569 \n",
      "tuple: n02095889_65 y: 72 org: 190 adv: 190 \n",
      "tuple: n03495258_27314 y: 344 org: 594 adv: 594 \n",
      "tuple: n02980441_30420 y: 701 org: 976 adv: 976 \n",
      "tuple: n03617480_10202 y: 770 org: 614 adv: 614 \n",
      "tuple: n01872401_3796 y: 215 org: 102 adv: 102 \n",
      "tuple: n03259280_3725 y: 662 org: 544 adv: 544 \n",
      "tuple: n02489166_9745 y: 100 org: 376 adv: 376 \n",
      "tuple: n02483708_3803 y: 122 org: 369 adv: 369 \n",
      "tuple: n03376595_8125 y: 309 org: 559 adv: 559 \n",
      "tuple: n01872401_1107 y: 215 org: 102 adv: 102 \n",
      "tuple: n02093859_494 y: 107 org: 183 adv: 183 \n",
      "tuple: n02489166_11697 y: 100 org: 376 adv: 376 \n",
      "tuple: n12144580_989 y: 331 org: 987 adv: 987 \n",
      "tuple: n02097658_2695 y: 131 org: 187 adv: 186 \n",
      "tuple: n02094114_5522 y: 88 org: 185 adv: 185 \n",
      "tuple: n03376595_8923 y: 309 org: 559 adv: 559 \n",
      "tuple: n02980441_1514 y: 701 org: 483 adv: 483 \n",
      "tuple: n03376595_5926 y: 309 org: 559 adv: 559 \n",
      "tuple: n01665541_6826 y: 459 org: 34 adv: 34 \n",
      "tuple: n12144580_8973 y: 331 org: 987 adv: 987 \n",
      "tuple: n02980441_3617 y: 701 org: 483 adv: 483 \n",
      "tuple: n02033041_6686 y: 436 org: 142 adv: 142 \n",
      "tuple: n02892767_21352 y: 872 org: 445 adv: 459 \n",
      "tuple: n01665541_42357 y: 459 org: 34 adv: 34 \n",
      "tuple: n01872401_11375 y: 215 org: 102 adv: 462 \n",
      "tuple: n02134084_9332 y: 103 org: 296 adv: 296 \n",
      "tuple: n03617480_4360 y: 770 org: 614 adv: 614 \n",
      "tuple: n12144580_4491 y: 331 org: 998 adv: 998 \n",
      "tuple: n02033041_9924 y: 436 org: 142 adv: 142 \n",
      "tuple: n02483708_6330 y: 122 org: 369 adv: 369 \n",
      "tuple: n01755581_2387 y: 492 org: 67 adv: 67 \n",
      "tuple: n02483708_1892 y: 122 org: 369 adv: 369 \n",
      "tuple: n01872401_5231 y: 215 org: 102 adv: 102 \n",
      "tuple: n03995372_2925 y: 373 org: 740 adv: 740 \n",
      "tuple: n02092002_15748 y: 21 org: 205 adv: 205 \n",
      "tuple: n01872401_737 y: 215 org: 102 adv: 102 \n",
      "tuple: n01687978_3638 y: 467 org: 42 adv: 42 \n",
      "tuple: n02980441_12984 y: 701 org: 483 adv: 483 \n",
      "tuple: n01755581_15191 y: 492 org: 67 adv: 67 \n",
      "tuple: n02980441_40722 y: 701 org: 483 adv: 483 \n",
      "tuple: n02134084_8039 y: 103 org: 296 adv: 296 \n",
      "tuple: n02092002_13735 y: 21 org: 177 adv: 177 \n",
      "tuple: n01980166_1360 y: 615 org: 120 adv: 120 \n",
      "tuple: n02980441_4847 y: 701 org: 483 adv: 483 \n",
      "tuple: n12144580_16604 y: 331 org: 987 adv: 987 \n",
      "tuple: n03617480_6248 y: 770 org: 614 adv: 614 \n",
      "tuple: n03495258_2810 y: 344 org: 594 adv: 594 \n",
      "tuple: n01665541_5054 y: 459 org: 34 adv: 34 \n",
      "tuple: n01665541_10793 y: 459 org: 34 adv: 34 \n",
      "tuple: n02097658_5407 y: 131 org: 185 adv: 334 \n",
      "tuple: n03259280_9295 y: 662 org: 521 adv: 712 \n",
      "tuple: n02483708_2991 y: 122 org: 369 adv: 371 \n",
      "tuple: n03770679_12176 y: 271 org: 656 adv: 656 \n",
      "tuple: n01687978_4746 y: 467 org: 42 adv: 42 \n",
      "tuple: n01980166_1618 y: 615 org: 120 adv: 120 \n",
      "tuple: n12144580_3386 y: 331 org: 987 adv: 987 \n",
      "tuple: n03259280_1124 y: 662 org: 544 adv: 544 \n",
      "tuple: n12144580_9438 y: 331 org: 151 adv: 151 \n",
      "tuple: n02095889_4051 y: 72 org: 190 adv: 190 \n",
      "tuple: n03376595_5209 y: 309 org: 559 adv: 559 \n",
      "tuple: n02097658_6978 y: 131 org: 193 adv: 193 \n",
      "tuple: n02134084_15968 y: 103 org: 296 adv: 296 \n",
      "tuple: n02483708_7170 y: 122 org: 367 adv: 367 \n",
      "tuple: n04442312_11191 y: 664 org: 859 adv: 435 \n",
      "tuple: n01872401_40436 y: 215 org: 102 adv: 102 \n",
      "tuple: n01755581_6093 y: 492 org: 67 adv: 67 \n",
      "tuple: n03995372_1400 y: 373 org: 740 adv: 477 \n",
      "tuple: n02097658_2033 y: 131 org: 200 adv: 200 \n",
      "tuple: n03259280_1281 y: 662 org: 544 adv: 544 \n",
      "tuple: n02134084_9466 y: 103 org: 296 adv: 296 \n",
      "tuple: n03770679_24846 y: 271 org: 656 adv: 656 \n",
      "tuple: n02092002_2546 y: 21 org: 211 adv: 211 \n",
      "tuple: n06874185_26874 y: 861 org: 688 adv: 688 \n",
      "tuple: n02092002_314 y: 21 org: 177 adv: 177 \n",
      "tuple: n06874185_17785 y: 861 org: 920 adv: 920 \n",
      "tuple: n12144580_6817 y: 331 org: 998 adv: 998 \n",
      "tuple: n01980166_6855 y: 615 org: 120 adv: 120 \n",
      "tuple: n02483708_5538 y: 122 org: 295 adv: 106 \n",
      "tuple: n02892767_21818 y: 872 org: 445 adv: 445 \n",
      "tuple: n02092002_6432 y: 21 org: 177 adv: 177 \n",
      "tuple: n02489166_7633 y: 100 org: 376 adv: 376 \n",
      "tuple: n02033041_5390 y: 436 org: 142 adv: 142 \n",
      "tuple: n02489166_4914 y: 100 org: 376 adv: 376 \n",
      "tuple: n03259280_9872 y: 662 org: 961 adv: 964 \n",
      "tuple: n02097658_2535 y: 131 org: 187 adv: 187 \n",
      "tuple: n01687978_6544 y: 467 org: 42 adv: 42 \n",
      "tuple: n01755581_6264 y: 492 org: 67 adv: 67 \n",
      "tuple: n02092002_10281 y: 21 org: 170 adv: 170 \n",
      "tuple: n02095889_9498 y: 72 org: 190 adv: 190 \n",
      "tuple: n02033041_9173 y: 436 org: 142 adv: 142 \n",
      "tuple: n02892767_20834 y: 872 org: 459 adv: 459 \n",
      "tuple: n06874185_37845 y: 861 org: 920 adv: 920 \n",
      "tuple: n02033041_4455 y: 436 org: 142 adv: 142 \n",
      "tuple: n12144580_1182 y: 331 org: 987 adv: 987 \n",
      "tuple: n01665541_14461 y: 459 org: 34 adv: 34 \n",
      "tuple: n02092002_12561 y: 21 org: 177 adv: 177 \n",
      "tuple: n03495258_13165 y: 344 org: 594 adv: 594 \n",
      "tuple: n03770679_21358 y: 271 org: 436 adv: 436 \n",
      "tuple: n03495258_4931 y: 344 org: 594 adv: 594 \n",
      "tuple: n06874185_10589 y: 861 org: 920 adv: 920 \n",
      "tuple: n02489166_4254 y: 100 org: 376 adv: 376 \n",
      "tuple: n03259280_3925 y: 662 org: 544 adv: 544 \n",
      "tuple: n02980441_2540 y: 701 org: 483 adv: 483 \n",
      "tuple: n02094114_8227 y: 88 org: 185 adv: 185 \n",
      "tuple: n02095889_1230 y: 72 org: 190 adv: 190 \n",
      "tuple: n03259280_7423 y: 662 org: 544 adv: 544 \n",
      "tuple: n02483708_1208 y: 122 org: 369 adv: 369 \n",
      "tuple: n03259280_11669 y: 662 org: 544 adv: 544 \n",
      "tuple: n02094114_1116 y: 88 org: 185 adv: 185 \n",
      "tuple: n02483708_4618 y: 122 org: 295 adv: 295 \n",
      "tuple: n02892767_19677 y: 872 org: 459 adv: 459 \n",
      "tuple: n02095889_7054 y: 72 org: 190 adv: 190 \n",
      "tuple: n01980166_283 y: 615 org: 74 adv: 73 \n",
      "tuple: n02483708_85 y: 122 org: 369 adv: 369 \n",
      "tuple: n03995372_9890 y: 373 org: 740 adv: 740 \n",
      "tuple: n02980441_10122 y: 701 org: 483 adv: 483 \n",
      "tuple: n03770679_7530 y: 271 org: 656 adv: 654 \n",
      "tuple: n01755581_6123 y: 492 org: 62 adv: 62 \n",
      "tuple: n03376595_6514 y: 309 org: 559 adv: 559 \n",
      "tuple: n03617480_7623 y: 770 org: 614 adv: 614 \n",
      "tuple: n04442312_19481 y: 664 org: 859 adv: 859 \n",
      "tuple: n12144580_3807 y: 331 org: 938 adv: 995 \n",
      "tuple: n02033041_1301 y: 436 org: 142 adv: 142 \n",
      "tuple: n01665541_11065 y: 459 org: 34 adv: 34 \n",
      "tuple: n01755581_22554 y: 492 org: 67 adv: 67 \n",
      "tuple: n02489166_7856 y: 100 org: 376 adv: 376 \n",
      "tuple: n02033041_7951 y: 436 org: 142 adv: 142 \n",
      "tuple: n01755581_12003 y: 492 org: 67 adv: 67 \n",
      "tuple: n06874185_44205 y: 861 org: 920 adv: 920 \n",
      "tuple: n03376595_11346 y: 309 org: 559 adv: 559 \n",
      "tuple: n02094114_6601 y: 88 org: 185 adv: 185 \n",
      "tuple: n02033041_2616 y: 436 org: 142 adv: 142 \n",
      "tuple: n02980441_9442 y: 701 org: 483 adv: 483 \n",
      "tuple: n02093859_1772 y: 107 org: 197 adv: 197 \n",
      "tuple: n12144580_14113 y: 331 org: 998 adv: 998 \n",
      "tuple: n03495258_22402 y: 344 org: 594 adv: 594 \n",
      "tuple: n02097658_2957 y: 131 org: 185 adv: 185 \n",
      "tuple: n01980166_8075 y: 615 org: 120 adv: 120 \n",
      "tuple: n01687978_6345 y: 467 org: 42 adv: 42 \n",
      "tuple: n01755581_4777 y: 492 org: 67 adv: 67 \n",
      "tuple: n01872401_5492 y: 215 org: 102 adv: 102 \n",
      "tuple: n01687978_5662 y: 467 org: 42 adv: 38 \n",
      "tuple: n02094114_8347 y: 88 org: 199 adv: 186 \n",
      "tuple: n06874185_31270 y: 861 org: 920 adv: 920 \n",
      "tuple: n02483708_3137 y: 122 org: 367 adv: 367 \n",
      "tuple: n01665541_9242 y: 459 org: 33 adv: 33 \n",
      "tuple: n02094114_6861 y: 88 org: 203 adv: 203 \n",
      "tuple: n02092002_4717 y: 21 org: 210 adv: 210 \n",
      "tuple: n04442312_3079 y: 664 org: 859 adv: 710 \n",
      "tuple: n02483708_4289 y: 122 org: 369 adv: 369 \n",
      "tuple: n02134084_4247 y: 103 org: 296 adv: 296 \n",
      "tuple: n12144580_1473 y: 331 org: 998 adv: 998 \n",
      "tuple: n02033041_5687 y: 436 org: 142 adv: 142 \n",
      "tuple: n02094114_8597 y: 88 org: 154 adv: 154 \n",
      "tuple: n02093859_4180 y: 107 org: 183 adv: 183 \n",
      "tuple: n03770679_10048 y: 271 org: 656 adv: 656 \n",
      "tuple: n02134084_7706 y: 103 org: 296 adv: 296 \n",
      "tuple: n03376595_13596 y: 309 org: 559 adv: 559 \n",
      "tuple: n06874185_19861 y: 861 org: 920 adv: 920 \n",
      "tuple: n02094114_7364 y: 88 org: 196 adv: 196 \n",
      "tuple: n03495258_1934 y: 344 org: 594 adv: 594 \n",
      "tuple: n02094114_6625 y: 88 org: 185 adv: 185 \n",
      "tuple: n03376595_490 y: 309 org: 415 adv: 762 \n",
      "tuple: n02033041_5233 y: 436 org: 142 adv: 142 \n",
      "tuple: n02892767_2123 y: 872 org: 459 adv: 459 \n",
      "tuple: n01872401_1173 y: 215 org: 102 adv: 102 \n",
      "tuple: n01755581_16809 y: 492 org: 67 adv: 67 \n",
      "tuple: n03617480_13590 y: 770 org: 614 adv: 614 \n",
      "tuple: n03495258_13013 y: 344 org: 457 adv: 457 \n",
      "tuple: n02094114_3378 y: 88 org: 185 adv: 185 \n",
      "tuple: n01872401_16852 y: 215 org: 102 adv: 102 \n",
      "tuple: n12144580_9096 y: 331 org: 998 adv: 998 \n",
      "tuple: n03617480_4798 y: 770 org: 614 adv: 614 \n",
      "tuple: n03617480_7660 y: 770 org: 614 adv: 614 \n",
      "tuple: n01665541_11967 y: 459 org: 33 adv: 395 \n",
      "tuple: n02094114_7744 y: 88 org: 185 adv: 192 \n",
      "tuple: n03770679_24632 y: 271 org: 717 adv: 717 \n",
      "tuple: n01872401_1814 y: 215 org: 102 adv: 102 \n",
      "tuple: n03770679_1366 y: 271 org: 656 adv: 656 \n",
      "tuple: n02134084_14448 y: 103 org: 296 adv: 296 \n",
      "tuple: n02489166_11153 y: 100 org: 376 adv: 376 \n",
      "tuple: n03995372_1179 y: 373 org: 740 adv: 740 \n",
      "tuple: n01980166_2488 y: 615 org: 120 adv: 120 \n",
      "tuple: n03770679_11032 y: 271 org: 511 adv: 575 \n",
      "tuple: n12144580_16826 y: 331 org: 998 adv: 998 \n",
      "tuple: n02483708_5552 y: 122 org: 381 adv: 369 \n",
      "tuple: n03617480_6268 y: 770 org: 614 adv: 614 \n",
      "tuple: n01872401_18923 y: 215 org: 102 adv: 102 \n",
      "tuple: n02980441_5507 y: 701 org: 483 adv: 483 \n",
      "tuple: n02892767_58235 y: 872 org: 459 adv: 459 \n",
      "tuple: n02489166_3413 y: 100 org: 376 adv: 376 \n",
      "tuple: n03259280_5978 y: 662 org: 551 adv: 551 \n",
      "tuple: n03617480_5150 y: 770 org: 614 adv: 614 \n",
      "tuple: n04442312_17459 y: 664 org: 859 adv: 859 \n",
      "tuple: n02097658_2388 y: 131 org: 200 adv: 200 \n",
      "tuple: n02097658_1094 y: 131 org: 201 adv: 201 \n",
      "tuple: n02134084_4875 y: 103 org: 296 adv: 296 \n",
      "tuple: n12144580_18223 y: 331 org: 998 adv: 998 \n",
      "tuple: n03995372_547 y: 373 org: 882 adv: 882 \n",
      "tuple: n06874185_2634 y: 861 org: 920 adv: 920 \n",
      "tuple: n03995372_4082 y: 373 org: 543 adv: 543 \n",
      "tuple: n02092002_15215 y: 21 org: 177 adv: 177 \n",
      "tuple: n01872401_3324 y: 215 org: 102 adv: 102 \n",
      "tuple: n01872401_13280 y: 215 org: 102 adv: 102 \n",
      "tuple: n02483708_8776 y: 122 org: 369 adv: 369 \n",
      "tuple: n03770679_14286 y: 271 org: 656 adv: 656 \n",
      "tuple: n03617480_16624 y: 770 org: 614 adv: 614 \n",
      "tuple: n03495258_12975 y: 344 org: 594 adv: 594 \n",
      "tuple: n06874185_41789 y: 861 org: 920 adv: 920 \n",
      "tuple: n02483708_3215 y: 122 org: 381 adv: 369 \n",
      "tuple: n01872401_5479 y: 215 org: 102 adv: 102 \n",
      "tuple: n02097658_2766 y: 131 org: 187 adv: 226 \n",
      "tuple: n03617480_2146 y: 770 org: 614 adv: 614 \n",
      "tuple: n03495258_6987 y: 344 org: 594 adv: 594 \n",
      "tuple: n02097658_500 y: 131 org: 187 adv: 187 \n",
      "tuple: n02980441_7969 y: 701 org: 483 adv: 483 \n",
      "tuple: n01665541_4921 y: 459 org: 34 adv: 33 \n",
      "tuple: n06874185_26841 y: 861 org: 920 adv: 920 \n",
      "tuple: n02489166_11168 y: 100 org: 376 adv: 376 \n",
      "tuple: n02483708_4526 y: 122 org: 368 adv: 368 \n",
      "tuple: n02093859_1628 y: 107 org: 183 adv: 183 \n",
      "tuple: n01687978_3166 y: 467 org: 42 adv: 42 \n",
      "tuple: n03770679_15163 y: 271 org: 656 adv: 656 \n",
      "tuple: n02483708_5833 y: 122 org: 369 adv: 369 \n",
      "tuple: n01665541_1601 y: 459 org: 34 adv: 34 \n",
      "tuple: n01872401_41932 y: 215 org: 102 adv: 102 \n",
      "tuple: n01872401_7012 y: 215 org: 102 adv: 102 \n",
      "tuple: n02980441_3050 y: 701 org: 483 adv: 483 \n",
      "tuple: n01872401_1897 y: 215 org: 102 adv: 102 \n",
      "tuple: n03376595_14464 y: 309 org: 559 adv: 559 \n",
      "tuple: n02134084_7904 y: 103 org: 296 adv: 296 \n",
      "tuple: n02892767_1705 y: 872 org: 775 adv: 775 \n",
      "tuple: n03495258_13075 y: 344 org: 594 adv: 594 \n",
      "tuple: n02093859_1073 y: 107 org: 183 adv: 183 \n",
      "tuple: n02097658_1449 y: 131 org: 187 adv: 187 \n",
      "tuple: n06874185_18023 y: 861 org: 920 adv: 920 \n",
      "tuple: n06874185_34342 y: 861 org: 920 adv: 920 \n",
      "tuple: n03617480_2159 y: 770 org: 614 adv: 614 \n",
      "tuple: n02095889_6570 y: 72 org: 190 adv: 190 \n",
      "tuple: n01872401_278 y: 215 org: 102 adv: 102 \n",
      "tuple: n01755581_10398 y: 492 org: 67 adv: 67 \n",
      "tuple: n01665541_14409 y: 459 org: 34 adv: 34 \n",
      "tuple: n03376595_16747 y: 309 org: 559 adv: 559 \n",
      "tuple: n06874185_112956 y: 861 org: 920 adv: 920 \n",
      "tuple: n03617480_3221 y: 770 org: 982 adv: 601 \n",
      "tuple: n03770679_1729 y: 271 org: 717 adv: 354 \n",
      "tuple: n06874185_3927 y: 861 org: 920 adv: 920 \n",
      "tuple: n02093859_5349 y: 107 org: 181 adv: 181 \n",
      "tuple: n02483708_1068 y: 122 org: 369 adv: 369 \n",
      "tuple: n03617480_20249 y: 770 org: 879 adv: 879 \n",
      "tuple: n02094114_6324 y: 88 org: 185 adv: 185 \n",
      "tuple: n02892767_26089 y: 872 org: 459 adv: 459 \n",
      "tuple: n03770679_13170 y: 271 org: 656 adv: 751 \n",
      "tuple: n01665541_12101 y: 459 org: 34 adv: 34 \n",
      "tuple: n01687978_8976 y: 467 org: 42 adv: 42 \n",
      "tuple: n04442312_1578 y: 664 org: 859 adv: 859 \n",
      "tuple: n03376595_2158 y: 309 org: 559 adv: 559 \n",
      "tuple: n01665541_12 y: 459 org: 34 adv: 34 \n",
      "tuple: n03376595_8339 y: 309 org: 559 adv: 559 \n",
      "tuple: n02094114_462 y: 88 org: 192 adv: 192 \n",
      "tuple: n02483708_906 y: 122 org: 369 adv: 369 \n",
      "tuple: n06874185_13089 y: 861 org: 920 adv: 920 \n",
      "tuple: n03259280_2689 y: 662 org: 615 adv: 870 \n",
      "tuple: n02095889_7445 y: 72 org: 190 adv: 190 \n",
      "tuple: n02483708_9844 y: 122 org: 369 adv: 369 \n",
      "tuple: n02134084_3782 y: 103 org: 296 adv: 296 \n",
      "tuple: n02489166_16238 y: 100 org: 376 adv: 376 \n",
      "tuple: n03495258_6146 y: 344 org: 594 adv: 594 \n",
      "tuple: n02134084_12355 y: 103 org: 296 adv: 296 \n",
      "tuple: n01665541_17184 y: 459 org: 34 adv: 34 \n",
      "tuple: n03617480_12529 y: 770 org: 614 adv: 614 \n",
      "tuple: n02094114_8557 y: 88 org: 185 adv: 185 \n",
      "tuple: n01665541_20882 y: 459 org: 34 adv: 34 \n",
      "tuple: n03495258_18380 y: 344 org: 594 adv: 594 \n",
      "tuple: n04442312_14782 y: 664 org: 859 adv: 859 \n",
      "tuple: n02489166_5165 y: 100 org: 374 adv: 376 \n",
      "tuple: n03617480_5346 y: 770 org: 614 adv: 614 \n",
      "tuple: n02092002_15463 y: 21 org: 177 adv: 177 \n",
      "tuple: n01872401_6936 y: 215 org: 102 adv: 102 \n",
      "tuple: n02094114_3147 y: 88 org: 434 adv: 434 \n",
      "tuple: n03770679_10668 y: 271 org: 785 adv: 785 \n",
      "tuple: n12144580_1279 y: 331 org: 987 adv: 998 \n",
      "tuple: n03770679_14576 y: 271 org: 468 adv: 675 \n",
      "tuple: n04442312_30854 y: 664 org: 651 adv: 651 \n",
      "tuple: n01872401_2988 y: 215 org: 102 adv: 102 \n",
      "tuple: n02892767_1916 y: 872 org: 459 adv: 459 \n",
      "tuple: n03259280_1968 y: 662 org: 544 adv: 544 \n",
      "tuple: n02980441_12473 y: 701 org: 483 adv: 483 \n",
      "tuple: n01872401_2278 y: 215 org: 102 adv: 102 \n",
      "tuple: n02097658_4600 y: 131 org: 201 adv: 187 \n",
      "tuple: n02094114_2526 y: 88 org: 185 adv: 185 \n",
      "tuple: n02093859_4026 y: 107 org: 183 adv: 183 \n",
      "tuple: n01755581_2093 y: 492 org: 67 adv: 67 \n",
      "tuple: n01687978_9114 y: 467 org: 42 adv: 42 \n",
      "tuple: n03259280_1472 y: 662 org: 544 adv: 544 \n",
      "tuple: n04442312_5155 y: 664 org: 859 adv: 859 \n",
      "tuple: n03376595_10266 y: 309 org: 559 adv: 559 \n",
      "tuple: n06874185_19189 y: 861 org: 920 adv: 920 \n",
      "tuple: n03376595_7592 y: 309 org: 559 adv: 559 \n",
      "tuple: n03617480_7669 y: 770 org: 614 adv: 614 \n",
      "tuple: n01872401_6500 y: 215 org: 102 adv: 102 \n",
      "tuple: n12144580_4733 y: 331 org: 987 adv: 987 \n",
      "tuple: n03770679_2014 y: 271 org: 407 adv: 407 \n",
      "tuple: n02095889_1072 y: 72 org: 190 adv: 190 \n",
      "tuple: n01687978_8482 y: 467 org: 42 adv: 42 \n",
      "tuple: n02093859_617 y: 107 org: 183 adv: 183 \n",
      "tuple: n02094114_6081 y: 88 org: 185 adv: 185 \n",
      "tuple: n03495258_1439 y: 344 org: 594 adv: 594 \n",
      "tuple: n01980166_1692 y: 615 org: 120 adv: 120 \n",
      "tuple: n02892767_12360 y: 872 org: 638 adv: 638 \n",
      "tuple: n02092002_14021 y: 21 org: 177 adv: 177 \n",
      "tuple: n02094114_6177 y: 88 org: 185 adv: 185 \n",
      "tuple: n03495258_15256 y: 344 org: 594 adv: 594 \n",
      "tuple: n02094114_6446 y: 88 org: 185 adv: 185 \n",
      "tuple: n02092002_11437 y: 21 org: 170 adv: 170 \n",
      "tuple: n02094114_3260 y: 88 org: 200 adv: 199 \n",
      "tuple: n01665541_8962 y: 459 org: 33 adv: 33 \n",
      "tuple: n01687978_6334 y: 467 org: 42 adv: 42 \n",
      "tuple: n03995372_17588 y: 373 org: 410 adv: 410 \n",
      "tuple: n02094114_1309 y: 88 org: 185 adv: 185 \n",
      "tuple: n06874185_45994 y: 861 org: 920 adv: 920 \n",
      "tuple: n12144580_33372 y: 331 org: 987 adv: 998 \n",
      "tuple: n02980441_5126 y: 701 org: 483 adv: 483 \n",
      "tuple: n03770679_4154 y: 271 org: 656 adv: 656 \n",
      "tuple: n12144580_1218 y: 331 org: 987 adv: 998 \n",
      "tuple: n03617480_3555 y: 770 org: 614 adv: 614 \n",
      "tuple: n03376595_5974 y: 309 org: 977 adv: 977 \n",
      "tuple: n02892767_14605 y: 872 org: 459 adv: 459 \n",
      "tuple: n06874185_7671 y: 861 org: 920 adv: 920 \n",
      "tuple: n02092002_10507 y: 21 org: 177 adv: 177 \n",
      "tuple: n01665541_1692 y: 459 org: 34 adv: 34 \n",
      "tuple: n01980166_3672 y: 615 org: 120 adv: 120 \n",
      "tuple: n02033041_100 y: 436 org: 142 adv: 142 \n",
      "tuple: n02094114_8407 y: 88 org: 185 adv: 185 \n",
      "tuple: n03770679_15090 y: 271 org: 656 adv: 656 \n",
      "tuple: n02483708_3610 y: 122 org: 369 adv: 369 \n",
      "tuple: n02092002_7016 y: 21 org: 177 adv: 177 \n",
      "tuple: n01980166_310 y: 615 org: 120 adv: 120 \n",
      "tuple: n01687978_7973 y: 467 org: 39 adv: 38 \n",
      "tuple: n12144580_12828 y: 331 org: 987 adv: 987 \n",
      "tuple: n01980166_4010 y: 615 org: 120 adv: 120 \n",
      "tuple: n01755581_13145 y: 492 org: 67 adv: 67 \n",
      "tuple: n03259280_7140 y: 662 org: 463 adv: 463 \n",
      "tuple: n06874185_20913 y: 861 org: 920 adv: 920 \n",
      "tuple: n02980441_11917 y: 701 org: 483 adv: 483 \n",
      "tuple: n02092002_15511 y: 21 org: 177 adv: 177 \n",
      "tuple: n01980166_463 y: 615 org: 120 adv: 120 \n",
      "tuple: n02892767_4089 y: 872 org: 459 adv: 459 \n",
      "tuple: n03770679_30727 y: 271 org: 734 adv: 734 \n",
      "tuple: n02033041_4180 y: 436 org: 141 adv: 141 \n",
      "tuple: n02094114_5515 y: 88 org: 185 adv: 185 \n",
      "tuple: n03495258_21433 y: 344 org: 669 adv: 669 \n",
      "tuple: n03259280_3469 y: 662 org: 896 adv: 896 \n",
      "tuple: n01872401_1625 y: 215 org: 102 adv: 102 \n",
      "tuple: n01687978_6862 y: 467 org: 42 adv: 42 \n",
      "tuple: n02892767_12094 y: 872 org: 655 adv: 655 \n",
      "tuple: n03617480_18854 y: 770 org: 614 adv: 614 \n",
      "tuple: n04442312_10611 y: 664 org: 659 adv: 647 \n",
      "tuple: n02134084_5435 y: 103 org: 296 adv: 296 \n",
      "tuple: n03376595_53 y: 309 org: 559 adv: 559 \n",
      "tuple: n02483708_3039 y: 122 org: 369 adv: 369 \n",
      "tuple: n01687978_7214 y: 467 org: 42 adv: 42 \n",
      "tuple: n01665541_7037 y: 459 org: 34 adv: 34 \n",
      "tuple: n03770679_5961 y: 271 org: 656 adv: 656 \n",
      "tuple: n02094114_8584 y: 88 org: 185 adv: 185 \n",
      "tuple: n01872401_3290 y: 215 org: 102 adv: 102 \n",
      "tuple: n02092002_15426 y: 21 org: 177 adv: 177 \n",
      "tuple: n02892767_10424 y: 872 org: 639 adv: 445 \n",
      "tuple: n12144580_5187 y: 331 org: 987 adv: 987 \n",
      "tuple: n01872401_7868 y: 215 org: 102 adv: 102 \n",
      "tuple: n02094114_8234 y: 88 org: 185 adv: 185 \n",
      "tuple: n01872401_3513 y: 215 org: 102 adv: 102 \n",
      "tuple: n01687978_3973 y: 467 org: 41 adv: 41 \n",
      "tuple: n03770679_10624 y: 271 org: 656 adv: 656 \n",
      "tuple: n02092002_4122 y: 21 org: 177 adv: 177 \n",
      "tuple: n03495258_11462 y: 344 org: 594 adv: 594 \n",
      "tuple: n01755581_19630 y: 492 org: 67 adv: 63 \n",
      "tuple: n03770679_11271 y: 271 org: 656 adv: 656 \n",
      "tuple: n01872401_3176 y: 215 org: 102 adv: 102 \n",
      "tuple: n01755581_12869 y: 492 org: 67 adv: 68 \n",
      "tuple: n03259280_9990 y: 662 org: 969 adv: 969 \n",
      "tuple: n02892767_5482 y: 872 org: 459 adv: 459 \n",
      "tuple: n02033041_1193 y: 436 org: 142 adv: 142 \n",
      "tuple: n02892767_17819 y: 872 org: 543 adv: 543 \n",
      "tuple: n02094114_6982 y: 88 org: 185 adv: 185 \n",
      "tuple: n04442312_16735 y: 664 org: 804 adv: 804 \n",
      "tuple: n02980441_2530 y: 701 org: 483 adv: 483 \n",
      "tuple: n03770679_8894 y: 271 org: 803 adv: 803 \n",
      "tuple: n01755581_9340 y: 492 org: 66 adv: 66 \n",
      "tuple: n02095889_1727 y: 72 org: 190 adv: 190 \n",
      "tuple: n06874185_24824 y: 861 org: 920 adv: 920 \n",
      "tuple: n02092002_7737 y: 21 org: 177 adv: 177 \n",
      "tuple: n02092002_6723 y: 21 org: 177 adv: 177 \n",
      "tuple: n01665541_8087 y: 459 org: 34 adv: 34 \n",
      "tuple: n03495258_5661 y: 344 org: 594 adv: 594 \n",
      "tuple: n01980166_1870 y: 615 org: 120 adv: 120 \n",
      "tuple: n02033041_4032 y: 436 org: 141 adv: 141 \n",
      "tuple: n02489166_9800 y: 100 org: 376 adv: 376 \n",
      "tuple: n02094114_4332 y: 88 org: 185 adv: 185 \n",
      "tuple: n03376595_10451 y: 309 org: 559 adv: 559 \n",
      "tuple: n02097658_7333 y: 131 org: 187 adv: 187 \n",
      "tuple: n03617480_5733 y: 770 org: 614 adv: 614 \n",
      "tuple: n01665541_32171 y: 459 org: 147 adv: 147 \n",
      "tuple: n02094114_8371 y: 88 org: 154 adv: 154 \n",
      "tuple: n01872401_1646 y: 215 org: 102 adv: 102 \n",
      "tuple: n02095889_1853 y: 72 org: 199 adv: 199 \n",
      "tuple: n02892767_22585 y: 872 org: 459 adv: 459 \n",
      "tuple: n02095889_1098 y: 72 org: 190 adv: 190 \n",
      "tuple: n12144580_3164 y: 331 org: 998 adv: 998 \n",
      "tuple: n03259280_4394 y: 662 org: 463 adv: 463 \n",
      "tuple: n01872401_1221 y: 215 org: 102 adv: 102 \n",
      "tuple: n02033041_3851 y: 436 org: 142 adv: 142 \n",
      "tuple: n02134084_8662 y: 103 org: 296 adv: 296 \n",
      "tuple: n02134084_6286 y: 103 org: 296 adv: 296 \n",
      "tuple: n03617480_10567 y: 770 org: 614 adv: 614 \n",
      "tuple: n02093859_889 y: 107 org: 183 adv: 183 \n",
      "tuple: n02097658_908 y: 131 org: 201 adv: 201 \n",
      "tuple: n01755581_2409 y: 492 org: 68 adv: 68 \n",
      "tuple: n03770679_5709 y: 271 org: 656 adv: 656 \n",
      "tuple: n01872401_2391 y: 215 org: 102 adv: 102 \n",
      "tuple: n02489166_4677 y: 100 org: 376 adv: 376 \n",
      "tuple: n02033041_4458 y: 436 org: 142 adv: 142 \n",
      "tuple: n03259280_5945 y: 662 org: 544 adv: 544 \n",
      "tuple: n03995372_8764 y: 373 org: 589 adv: 589 \n",
      "tuple: n02033041_3638 y: 436 org: 142 adv: 142 \n",
      "tuple: n04442312_14104 y: 664 org: 651 adv: 651 \n",
      "tuple: n02095889_6307 y: 72 org: 190 adv: 190 \n",
      "tuple: n03376595_3120 y: 309 org: 559 adv: 559 \n",
      "tuple: n01665541_41148 y: 459 org: 34 adv: 34 \n",
      "tuple: n03995372_9723 y: 373 org: 740 adv: 740 \n",
      "tuple: n02134084_10648 y: 103 org: 296 adv: 296 \n",
      "tuple: n02092002_9656 y: 21 org: 177 adv: 177 \n",
      "tuple: n02092002_7066 y: 21 org: 177 adv: 177 \n",
      "tuple: n02095889_6232 y: 72 org: 190 adv: 190 \n",
      "tuple: n02483708_2106 y: 122 org: 369 adv: 369 \n",
      "tuple: n02489166_9676 y: 100 org: 376 adv: 376 \n",
      "tuple: n02094114_2769 y: 88 org: 185 adv: 185 \n",
      "tuple: n02094114_3172 y: 88 org: 185 adv: 185 \n",
      "tuple: n04442312_7039 y: 664 org: 859 adv: 859 \n",
      "tuple: n02094114_5061 y: 88 org: 185 adv: 185 \n",
      "tuple: n04442312_7552 y: 664 org: 532 adv: 532 \n",
      "tuple: n02094114_4853 y: 88 org: 185 adv: 193 \n",
      "tuple: n03376595_196 y: 309 org: 462 adv: 462 \n",
      "tuple: n01980166_4236 y: 615 org: 120 adv: 120 \n",
      "tuple: n01980166_529 y: 615 org: 120 adv: 120 \n",
      "tuple: n02489166_1750 y: 100 org: 376 adv: 376 \n",
      "tuple: n02097658_2941 y: 131 org: 187 adv: 187 \n",
      "tuple: n06874185_26898 y: 861 org: 920 adv: 920 \n",
      "tuple: n03376595_6025 y: 309 org: 559 adv: 559 \n",
      "tuple: n02033041_7196 y: 436 org: 142 adv: 142 \n",
      "tuple: n01687978_5634 y: 467 org: 42 adv: 42 \n",
      "tuple: n12144580_11631 y: 331 org: 987 adv: 998 \n",
      "tuple: n03617480_18516 y: 770 org: 421 adv: 421 \n",
      "tuple: n12144580_4134 y: 331 org: 987 adv: 987 \n",
      "tuple: n01980166_1054 y: 615 org: 120 adv: 120 \n",
      "tuple: n04442312_198 y: 664 org: 523 adv: 451 \n",
      "tuple: n03495258_21874 y: 344 org: 594 adv: 594 \n",
      "tuple: n01755581_7278 y: 492 org: 67 adv: 67 \n",
      "tuple: n04442312_16620 y: 664 org: 859 adv: 859 \n",
      "tuple: n03770679_20687 y: 271 org: 656 adv: 656 \n",
      "tuple: n02097658_5413 y: 131 org: 187 adv: 187 \n",
      "tuple: n01872401_2597 y: 215 org: 102 adv: 102 \n",
      "tuple: n02483708_10018 y: 122 org: 369 adv: 369 \n",
      "tuple: n03376595_18884 y: 309 org: 559 adv: 559 \n",
      "tuple: n03259280_2514 y: 662 org: 926 adv: 926 \n",
      "tuple: n02092002_25802 y: 21 org: 177 adv: 177 \n",
      "tuple: n02092002_5347 y: 21 org: 170 adv: 170 \n",
      "tuple: n01755581_8430 y: 492 org: 67 adv: 67 \n",
      "tuple: n02092002_2275 y: 21 org: 226 adv: 226 \n",
      "tuple: n01665541_706 y: 459 org: 34 adv: 34 \n",
      "tuple: n02094114_4813 y: 88 org: 185 adv: 185 \n",
      "tuple: n03259280_6760 y: 662 org: 938 adv: 925 \n",
      "tuple: n03617480_20366 y: 770 org: 614 adv: 614 \n",
      "tuple: n01755581_9397 y: 492 org: 67 adv: 67 \n",
      "tuple: n01687978_152 y: 467 org: 42 adv: 42 \n",
      "tuple: n12144580_20426 y: 331 org: 998 adv: 998 \n",
      "tuple: n02980441_11920 y: 701 org: 483 adv: 483 \n",
      "tuple: n02093859_179 y: 107 org: 183 adv: 183 \n",
      "tuple: n02097658_2054 y: 131 org: 187 adv: 187 \n",
      "tuple: n01980166_1168 y: 615 org: 120 adv: 120 \n",
      "tuple: n01980166_2534 y: 615 org: 120 adv: 120 \n",
      "tuple: n02092002_15443 y: 21 org: 177 adv: 177 \n",
      "tuple: n02095889_5759 y: 72 org: 190 adv: 190 \n",
      "tuple: n02095889_3354 y: 72 org: 190 adv: 190 \n",
      "tuple: n02033041_2171 y: 436 org: 142 adv: 142 \n",
      "tuple: n03495258_227 y: 344 org: 594 adv: 594 \n",
      "tuple: n01980166_3469 y: 615 org: 120 adv: 120 \n",
      "tuple: n02094114_1416 y: 88 org: 204 adv: 204 \n",
      "tuple: n03617480_5541 y: 770 org: 614 adv: 614 \n",
      "tuple: n02980441_37702 y: 701 org: 483 adv: 483 \n",
      "tuple: n02980441_1345 y: 701 org: 483 adv: 483 \n",
      "tuple: n04442312_9359 y: 664 org: 530 adv: 530 \n",
      "tuple: n01665541_26183 y: 459 org: 33 adv: 34 \n",
      "tuple: n02093859_2650 y: 107 org: 183 adv: 183 \n",
      "tuple: n06874185_33520 y: 861 org: 920 adv: 920 \n",
      "tuple: n03770679_13314 y: 271 org: 656 adv: 656 \n",
      "tuple: n03495258_12765 y: 344 org: 594 adv: 594 \n",
      "tuple: n02892767_4611 y: 872 org: 868 adv: 868 \n",
      "tuple: n01687978_7037 y: 467 org: 42 adv: 42 \n",
      "tuple: n12144580_12189 y: 331 org: 987 adv: 998 \n",
      "tuple: n02483708_2631 y: 122 org: 369 adv: 369 \n",
      "tuple: n12144580_7432 y: 331 org: 17 adv: 17 \n",
      "tuple: n04442312_3081 y: 664 org: 859 adv: 859 \n",
      "tuple: n02134084_12605 y: 103 org: 296 adv: 296 \n",
      "tuple: n06874185_27131 y: 861 org: 920 adv: 920 \n",
      "tuple: n02033041_2219 y: 436 org: 142 adv: 142 \n",
      "tuple: n03259280_6828 y: 662 org: 969 adv: 969 \n",
      "tuple: n03617480_18189 y: 770 org: 614 adv: 614 \n",
      "tuple: n06874185_42652 y: 861 org: 539 adv: 498 \n",
      "tuple: n02092002_7286 y: 21 org: 170 adv: 202 \n",
      "tuple: n03376595_6071 y: 309 org: 559 adv: 559 \n",
      "tuple: n02489166_21 y: 100 org: 376 adv: 376 \n",
      "tuple: n01980166_1676 y: 615 org: 120 adv: 120 \n",
      "tuple: n03770679_25459 y: 271 org: 436 adv: 436 \n",
      "tuple: n03995372_11248 y: 373 org: 585 adv: 585 \n",
      "tuple: n03495258_26059 y: 344 org: 594 adv: 594 \n",
      "tuple: n12144580_3415 y: 331 org: 419 adv: 419 \n",
      "tuple: n02097658_1098 y: 131 org: 226 adv: 226 \n",
      "tuple: n02483708_2701 y: 122 org: 366 adv: 381 \n",
      "tuple: n02092002_23134 y: 21 org: 170 adv: 170 \n",
      "tuple: n12144580_6381 y: 331 org: 987 adv: 987 \n",
      "tuple: n03376595_3358 y: 309 org: 559 adv: 559 \n",
      "tuple: n02134084_4431 y: 103 org: 296 adv: 296 \n",
      "tuple: n04442312_19733 y: 664 org: 697 adv: 739 \n",
      "tuple: n04442312_34013 y: 664 org: 549 adv: 549 \n",
      "tuple: n02489166_2918 y: 100 org: 373 adv: 376 \n",
      "tuple: n12144580_4936 y: 331 org: 987 adv: 998 \n",
      "tuple: n12144580_4125 y: 331 org: 998 adv: 998 \n",
      "tuple: n01665541_8407 y: 459 org: 34 adv: 34 \n",
      "tuple: n06874185_30075 y: 861 org: 920 adv: 920 \n",
      "tuple: n02095889_2820 y: 72 org: 190 adv: 190 \n",
      "tuple: n01872401_19 y: 215 org: 102 adv: 102 \n",
      "tuple: n03995372_8001 y: 373 org: 492 adv: 492 \n",
      "tuple: n03995372_3378 y: 373 org: 740 adv: 740 \n",
      "tuple: n03495258_21364 y: 344 org: 594 adv: 594 \n",
      "tuple: n06874185_19631 y: 861 org: 920 adv: 920 \n",
      "tuple: n06874185_27019 y: 861 org: 33 adv: 33 \n",
      "tuple: n02033041_2737 y: 436 org: 142 adv: 142 \n",
      "tuple: n02095889_9239 y: 72 org: 190 adv: 190 \n",
      "tuple: n03376595_932 y: 309 org: 559 adv: 559 \n",
      "tuple: n01665541_33191 y: 459 org: 34 adv: 34 \n",
      "tuple: n01980166_4320 y: 615 org: 120 adv: 120 \n",
      "tuple: n01687978_465 y: 467 org: 42 adv: 42 \n",
      "tuple: n06874185_17292 y: 861 org: 571 adv: 571 \n",
      "tuple: n03259280_667 y: 662 org: 534 adv: 534 \n",
      "tuple: n03617480_2973 y: 770 org: 614 adv: 614 \n",
      "tuple: n02134084_14426 y: 103 org: 296 adv: 296 \n",
      "tuple: n02489166_8876 y: 100 org: 376 adv: 376 \n",
      "tuple: n01665541_16615 y: 459 org: 34 adv: 34 \n",
      "tuple: n02097658_2491 y: 131 org: 187 adv: 226 \n",
      "tuple: n03495258_21138 y: 344 org: 594 adv: 594 \n",
      "tuple: n03495258_21518 y: 344 org: 594 adv: 594 \n",
      "tuple: n03376595_6683 y: 309 org: 532 adv: 532 \n",
      "tuple: n02134084_1871 y: 103 org: 296 adv: 296 \n",
      "tuple: n03495258_21614 y: 344 org: 594 adv: 594 \n",
      "tuple: n02033041_1036 y: 436 org: 142 adv: 142 \n",
      "tuple: n02092002_3460 y: 21 org: 170 adv: 170 \n",
      "tuple: n02892767_9153 y: 872 org: 459 adv: 459 \n",
      "tuple: n04442312_4955 y: 664 org: 859 adv: 859 \n",
      "tuple: n03495258_19777 y: 344 org: 813 adv: 813 \n",
      "tuple: n02092002_3513 y: 21 org: 177 adv: 177 \n",
      "tuple: n03259280_596 y: 662 org: 961 adv: 544 \n",
      "tuple: n01872401_11578 y: 215 org: 102 adv: 102 \n",
      "tuple: n02033041_3895 y: 436 org: 142 adv: 142 \n",
      "tuple: n12144580_20886 y: 331 org: 965 adv: 965 \n",
      "tuple: n03617480_6394 y: 770 org: 614 adv: 614 \n",
      "tuple: n02483708_7231 y: 122 org: 369 adv: 369 \n",
      "tuple: n03495258_26018 y: 344 org: 594 adv: 594 \n",
      "tuple: n01755581_23277 y: 492 org: 60 adv: 68 \n",
      "tuple: n02094114_7870 y: 88 org: 185 adv: 233 \n",
      "tuple: n04442312_2563 y: 664 org: 556 adv: 556 \n",
      "tuple: n02092002_30347 y: 21 org: 177 adv: 177 \n",
      "tuple: n03995372_6860 y: 373 org: 845 adv: 543 \n",
      "tuple: n02097658_1060 y: 131 org: 187 adv: 187 \n",
      "tuple: n01872401_4328 y: 215 org: 102 adv: 102 \n",
      "tuple: n03770679_31539 y: 271 org: 656 adv: 627 \n",
      "tuple: n01665541_4868 y: 459 org: 34 adv: 34 \n",
      "tuple: n01755581_9632 y: 492 org: 67 adv: 67 \n",
      "tuple: n02980441_3373 y: 701 org: 483 adv: 483 \n",
      "tuple: n02097658_2776 y: 131 org: 200 adv: 200 \n",
      "tuple: n06874185_27013 y: 861 org: 920 adv: 920 \n",
      "tuple: n04442312_11551 y: 664 org: 691 adv: 691 \n",
      "tuple: n03495258_6291 y: 344 org: 594 adv: 594 \n",
      "tuple: n02094114_3840 y: 88 org: 186 adv: 186 \n",
      "tuple: n03376595_2056 y: 309 org: 559 adv: 861 \n",
      "tuple: n01872401_13035 y: 215 org: 102 adv: 102 \n",
      "tuple: n01665541_4985 y: 459 org: 34 adv: 34 \n",
      "tuple: n02097658_4550 y: 131 org: 201 adv: 201 \n",
      "tuple: n06874185_14623 y: 861 org: 920 adv: 920 \n",
      "tuple: n02095889_2927 y: 72 org: 190 adv: 190 \n",
      "tuple: n01665541_4028 y: 459 org: 34 adv: 34 \n",
      "tuple: n03495258_22832 y: 344 org: 594 adv: 594 \n",
      "tuple: n02092002_15206 y: 21 org: 177 adv: 177 \n",
      "tuple: n02094114_7076 y: 88 org: 185 adv: 185 \n",
      "tuple: n03495258_13374 y: 344 org: 594 adv: 594 \n",
      "tuple: n06874185_28815 y: 861 org: 920 adv: 920 \n",
      "tuple: n02033041_2092 y: 436 org: 142 adv: 142 \n",
      "tuple: n03259280_790 y: 662 org: 544 adv: 544 \n",
      "tuple: n03376595_3195 y: 309 org: 559 adv: 559 \n",
      "tuple: n06874185_38797 y: 861 org: 920 adv: 920 \n",
      "tuple: n02892767_19716 y: 872 org: 459 adv: 459 \n",
      "tuple: n02892767_15749 y: 872 org: 459 adv: 459 \n",
      "tuple: n01980166_602 y: 615 org: 120 adv: 120 \n",
      "tuple: n02033041_2627 y: 436 org: 142 adv: 142 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-58c7cdcaa9ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\rpayload {i+1}/{total} (label {label})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-050f06b55a46>\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# payload can be original or adversarial.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AADefDINO/src/model/data.py\u001b[0m in \u001b[0;36madv_dataset\u001b[0;34m(org_loader, adv_loader, model, linear_classifier, n, device, verbose)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# forward pass original and adversarial sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0morg_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0madv_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AADefDINO/src/model/forward_pass.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(inp, model, linear_classifier, n)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_intermediate_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mintermediate_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AADefDINO/dino/vision_transformer.py\u001b[0m in \u001b[0;36mget_intermediate_layers\u001b[0;34m(self, x, n)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AADefDINO/dino/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, return_attention)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_attention\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m                 self._forward_hooks.values()):\n\u001b[1;32m    731\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# testing iterator\n",
    "tmp_ori_loader = tr_ori_loader\n",
    "#tmp_ori_loader = val_ori_loader\n",
    "\n",
    "#tmp_adv_loader = val_pgd_loader\n",
    "#tmp_adv_loader = val_cw_loader\n",
    "#tmp_adv_loader = tr_pgd_loader\n",
    "#tmp_adv_loader = tr_fgsm_loader\n",
    "tmp_adv_loader = tr_cw_loader\n",
    "\n",
    "total=2\n",
    "samples = AdvTupleIterator(tmp_ori_loader, tmp_adv_loader, model, linear_classifier, visualize=True, verbose=True)\n",
    "\n",
    "for i in range(total):\n",
    "  num, payload, label = next(samples)\n",
    "  sys.stdout.write(f\"\\rpayload {i+1}/{total} (label {label})\")\n",
    "  sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to disk (once per dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posthoc path: /cluster/scratch/mmathys/dl_data/posthoc/pgd/validation\n",
      "label path: /cluster/scratch/mmathys/dl_data/posthoc/pgd/validation/labels.csv\n",
      " 50/50 ILSVRC2012_val_00000034 (label 1)\n",
      "posthoc path: /cluster/scratch/mmathys/dl_data/posthoc/cw/validation\n",
      "label path: /cluster/scratch/mmathys/dl_data/posthoc/cw/validation/labels.csv\n",
      " 50/50 ILSVRC2012_val_00000236 (label 1)\n",
      "posthoc path: /cluster/scratch/mmathys/dl_data/posthoc/fgsm/validation\n",
      "label path: /cluster/scratch/mmathys/dl_data/posthoc/fgsm/validation/labels.csv\n",
      " 50/50 ILSVRC2012_val_00000034 (label 1)\n"
     ]
    }
   ],
   "source": [
    "# save to disk!\n",
    "limit = 50\n",
    "datasets = [\n",
    "    #{\"posthoc_path\": DN_POSTHOC_PATH,  \"label_path\": DN_POSTHOC_LABEL_PATH},\n",
    "    {\n",
    "        \"ori_loader\": val_ori_loader,\n",
    "        \"adv_loader\": val_pgd_loader,\n",
    "        \"posthoc_path\": VAL_PGD_POSTHOC_PATH,\n",
    "        \"label_path\": VAL_PGD_POSTHOC_LABEL_PATH\n",
    "    },\n",
    "    {\n",
    "        \"ori_loader\": val_ori_loader,\n",
    "        \"adv_loader\": val_cw_loader,\n",
    "        \"posthoc_path\": VAL_CW_POSTHOC_PATH,\n",
    "        \"label_path\": VAL_CW_POSTHOC_LABEL_PATH\n",
    "    },\n",
    "    {\n",
    "        \"ori_loader\": val_ori_loader,\n",
    "        \"adv_loader\": val_fgsm_loader,\n",
    "        \"posthoc_path\": VAL_FGSM_POSTHOC_PATH,\n",
    "        \"label_path\": VAL_FGSM_POSTHOC_LABEL_PATH\n",
    "    },\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "    ori_loader = dataset[\"ori_loader\"]\n",
    "    adv_loader = dataset[\"adv_loader\"]\n",
    "    posthoc_path = dataset[\"posthoc_path\"]\n",
    "    label_path = dataset[\"label_path\"]\n",
    "\n",
    "    print(f\"posthoc path: {posthoc_path}\")\n",
    "    print(f\"label path: {label_path}\")\n",
    "\n",
    "    iterator = AdvTupleIterator(ori_loader, adv_loader, model, linear_classifier, max=0)\n",
    "    names = []\n",
    "    paths = []\n",
    "    labels = []\n",
    "\n",
    "    for i, (name, payload, label) in enumerate(iterator):\n",
    "        if limit > 0 and i >= limit: break\n",
    "        sys.stdout.write(f\"\\r {i+1}/{limit} {name} (label {label})\")\n",
    "        sys.stdout.flush()\n",
    "        # original: 0, adversarial: 1\n",
    "        if label == 0:\n",
    "            path = f\"org/{name}\"\n",
    "        else:\n",
    "            path = f\"adv/{name}\"\n",
    "        paths.append(path)\n",
    "        names.append(name)\n",
    "        labels.append(label)\n",
    "        path = Path(posthoc_path, path)\n",
    "        torch.save(payload, path)\n",
    "\n",
    "    df = pd.DataFrame(data={\"path\": paths, \"name\": names, \"label\": labels})\n",
    "    df.to_csv(label_path, index=False)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset for reading from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_folder, index_df):\n",
    "        super().__init__()\n",
    "        self.img_folder = img_folder\n",
    "        self.index_df = index_df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename = self.index_df['path'].iloc[index]\n",
    "        label = self.index_df['label'].iloc[index]\n",
    "        payload = torch.load(Path(self.img_folder, filename))\n",
    "        return filename, payload, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posthoc Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Binary Classifier Network\n",
    "class SimpleBC(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(SimpleBC,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,64)\n",
    "        self.fc2 = nn.Linear(64,32)\n",
    "        self.fc3 = nn.Linear(32,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Binary Classifier\n",
    "class LinearBC(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(LinearBC,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGD:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep: 9\t Tr. Loss: 0.0004\t Tr. Acc: 1.0000\t T. Loss: 2.9291\t T. Acc: 0.7333: 100%|██████████| 10/10 [00:00<00:00, 13.23it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n",
      "CW:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep: 9\t Tr. Loss: 0.1678\t Tr. Acc: 1.0000\t T. Loss: 0.4739\t T. Acc: 0.8667: 100%|██████████| 10/10 [00:00<00:00, 14.60it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n",
      "FGSM:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep: 9\t Tr. Loss: 0.0004\t Tr. Acc: 1.0000\t T. Loss: 0.9848\t T. Acc: 0.7333: 100%|██████████| 10/10 [00:00<00:00, 13.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10\n",
    "\n",
    "# Initialise network\n",
    "net = LinearBC(1536)\n",
    "\n",
    "# Select device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "net.to(device)\n",
    "\n",
    "# Set model to train\n",
    "net.train()\n",
    "\n",
    "# define loss, optimizer, and scheduler\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.01, lr_decay=1e-08, weight_decay=0)\n",
    "# scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "\n",
    "losses = []\n",
    "accur = []\n",
    "\n",
    "train_files = []\n",
    "test_files = []\n",
    "\n",
    "def train(train_posthoc_path, val_posthoc_path):\n",
    "    #idx_train_df = pd.read_csv(Path(train_posthoc_path, 'labels.csv'))\n",
    "    \n",
    "    idx_val_df = pd.read_csv(Path(val_posthoc_path, 'labels.csv'))\n",
    "    train_len = math.floor(len(idx_val_df) * 0.7)\n",
    "    test_len = len(idx_val_df) - train_len\n",
    "    train_dataset, val_dataset = random_split(AdvDataset(val_posthoc_path, idx_val_df), [train_len, test_len], generator=torch.Generator().manual_seed(42))\n",
    "    #train_dataset = AdvDataset(train_posthoc_path, idx_train_df)\n",
    "    #val_dataset = AdvDataset(val_posthoc_path, idx_val_df)\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=8, shuffle=False)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    # Train network\n",
    "    pbar = tqdm(range(EPOCHS))\n",
    "    for epoch in pbar:  # loop over the dataset multiple times\n",
    "\n",
    "        # Metrics\n",
    "        train_running_loss = 0.0\n",
    "        train_running_loss_mean = 0.0\n",
    "        train_acc = 0.0\n",
    "        train_acc_mean = 0.0\n",
    "        test_running_loss = 0.0\n",
    "        test_acc = 0.0\n",
    "\n",
    "        for i, (filename, inputs, labels) in enumerate(train_loader, start=0):        \n",
    "            train_files.append(filename)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            # Forward Pass\n",
    "            outputs = net(inputs).float()\n",
    "            outputs = outputs.reshape(-1)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad() # Reset the gradient\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss train\n",
    "            train_running_loss += loss.item()\n",
    "            train_running_loss_mean = train_running_loss / (i+1)\n",
    "\n",
    "            # accuracy train\n",
    "            predicted = net(inputs).reshape(-1).detach().cpu().numpy().round()\n",
    "            acc_labels = labels\n",
    "            acc_labels = acc_labels.detach().cpu().numpy()\n",
    "            inter = np.equal(predicted, acc_labels)\n",
    "            train_acc += inter.sum()\n",
    "            #train_acc_mean = train_acc / (i+1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for filename, inputs, labels in val_loader:\n",
    "                test_files.append(filename)\n",
    "                try:\n",
    "                    # get the inputs; data is a list of [inputs, labels] and write to device\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device).float()\n",
    "\n",
    "                    # Forward Pass\n",
    "                    outputs = net(inputs).float()\n",
    "                    outputs = outputs.reshape(-1)\n",
    "\n",
    "                    # loss test\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    test_running_loss += loss.item()\n",
    "\n",
    "                    # accuracy test\n",
    "                    outputs = outputs.detach().cpu().numpy().round()\n",
    "                    comparison = np.equal(labels.detach().cpu().numpy(), outputs)\n",
    "                    test_acc += comparison.sum()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Error: {}\".format(e))\n",
    "                    pass\n",
    "\n",
    "        losses.append(train_running_loss_mean)\n",
    "        accur.append(train_acc_mean)\n",
    "        pbar.set_description(\"Ep: {}\\t Tr. Loss: {:.4f}\\t Tr. Acc: {:.4f}\\t T. Loss: {:.4f}\\t T. Acc: {:.4f}\".format(epoch, \n",
    "                                                                                train_running_loss_mean, \n",
    "                                                                                train_acc / len(train_loader.dataset), \n",
    "                                                                                test_running_loss, \n",
    "                                                                                test_acc / len(val_loader.dataset)))\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "#print(\"DAmageNet:\")\n",
    "#train(DN_POSTHOC_PATH)\n",
    "print(\"PGD:\")\n",
    "train(None, VAL_PGD_POSTHOC_PATH)\n",
    "print(\"CW:\")\n",
    "train(None, VAL_CW_POSTHOC_PATH)\n",
    "print(\"FGSM:\")\n",
    "train(None, VAL_FGSM_POSTHOC_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
