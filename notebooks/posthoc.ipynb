{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posthoc Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This extension reloads external Python files\n",
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino\n",
    "from src.model.data import create_loader, adv_dataset\n",
    "from src.model.eval import validate_network\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "username = getpass.getuser()\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "\n",
    "ORI_PATH = Path(DATA_PATH, 'ori_data/validation/')\n",
    "ORI_LABEL_PATH = Path(ORI_PATH,'correct_labels.txt')\n",
    "ORI_IMAGES_PATH = Path(ORI_PATH,'images')\n",
    "\n",
    "DN_PATH = Path(DATA_PATH, 'damageNet')\n",
    "DN_LABEL_PATH = Path(DN_PATH, 'val_damagenet.txt')\n",
    "DN_IMAGES_PATH = Path(DN_PATH, 'images')\n",
    "\n",
    "PGD_PATH = Path(DATA_PATH, 'adversarial_data/pgd_03/validation')\n",
    "PGD_LABEL_PATH = ORI_LABEL_PATH\n",
    "PGD_IMAGES_PATH = Path(PGD_PATH, 'images')\n",
    "\n",
    "POSTHOC_PATH = Path(DATA_PATH, 'posthoc/')\n",
    "\n",
    "DN_POSTHOC_PATH = Path(POSTHOC_PATH, 'damagenet')\n",
    "DN_POSTHOC_LABEL_PATH = Path(DN_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "PGD_POSTHOC_PATH = Path(POSTHOC_PATH, 'pgd')\n",
    "PGD_POSTHOC_LABEL_PATH = Path(PGD_POSTHOC_PATH, 'labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If CLASS_SUBSET is specified, INDEX_SUBSET will be ignored. Set CLASS_SUBSET=None if you want to use indexes.\n",
    "INDEX_SUBSET = get_random_indexes()\n",
    "CLASS_SUBSET = get_random_classes()\n",
    "INDEX_SUBSET = None\n",
    "CLASS_SUBSET = None\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!python $HOME/deeplearning/setup/collect_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import DINO\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "We load the reference pretrained linear weights.\n"
     ]
    }
   ],
   "source": [
    "model, linear_classifier = get_dino()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_loader = create_loader(ORI_IMAGES_PATH, ORI_LABEL_PATH, INDEX_SUBSET, CLASS_SUBSET, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_loader = create_loader(DN_IMAGES_PATH, DN_LABEL_PATH, INDEX_SUBSET, CLASS_SUBSET, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_loader = create_loader(PGD_IMAGES_PATH, PGD_LABEL_PATH, INDEX_SUBSET, CLASS_SUBSET, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Sample Tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs a forward pass given a sample `inp` and a classifier.\n",
    "def generate_model_output(inp, n=4):\n",
    "    inp = inp.to(\"cuda\")\n",
    "    # add one dimension to input image (get_intermediate_layers expects it)\n",
    "    inp = inp.unsqueeze(dim=0)\n",
    "    intermediate_output = model.get_intermediate_layers(inp, n)\n",
    "    return torch.cat([x[:, 0] for x in intermediate_output], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvTupleIterator:\n",
    "    def __init__(self, ori_loader, dn_loader, model, linear_classifier, max=50):\n",
    "        self.samples = adv_dataset(ori_loader, dn_loader, model, linear_classifier)\n",
    "        self.max = max\n",
    "        self.num = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.num = 0\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        if(self.max > 0 and self.num >= self.max):\n",
    "            raise StopIteration\n",
    "        self.num += 1\n",
    "        # payload can be original or adversarial.\n",
    "        sample, payload, label = next(self.samples)\n",
    "        payload_out = generate_model_output(payload)\n",
    "        return sample, payload_out, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payload 10/10 (label 1)"
     ]
    }
   ],
   "source": [
    "# testing iterator\n",
    "\n",
    "total=10\n",
    "samples = AdvTupleIterator(ori_loader, dn_loader, model, linear_classifier, max=50)\n",
    "#samples = AdvTupleIterator(ori_loader, pgd_loader, model, linear_classifier, max=50)\n",
    "\n",
    "for i in range(total):\n",
    "  num, payload, label = next(samples)\n",
    "  sys.stdout.write(f\"\\rpayload {i+1}/{total} (label {label})\")\n",
    "  sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to disk (once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2000/2000 ILSVRC2012_val_00001910 (label 1)"
     ]
    }
   ],
   "source": [
    "# save to disk!\n",
    "iterator = AdvTupleIterator(ori_loader, dn_loader, model, linear_classifier, max=0)\n",
    "names = []\n",
    "paths = []\n",
    "labels = []\n",
    "limit = 2000\n",
    "for i, (name, payload, label) in enumerate(iterator):\n",
    "    if i >= limit: break\n",
    "    sys.stdout.write(f\"\\r {i+1}/{limit} {name} (label {label})\")\n",
    "    sys.stdout.flush()\n",
    "    # original: 0, adversarial: 1\n",
    "    if label == 0:\n",
    "        path = f\"org/{name}\"\n",
    "    else:\n",
    "        path = f\"adv/{name}\"\n",
    "    paths.append(path)\n",
    "    names.append(name)\n",
    "    labels.append(label)\n",
    "    path = Path(DN_POSTHOC_PATH, path)\n",
    "    torch.save(payload, path)\n",
    "    \n",
    "df = pd.DataFrame(data={\"path\": paths, \"name\": names, \"label\": labels})\n",
    "df.to_csv(DN_POSTHOC_LABEL_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from disk (PyTorch dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_folder, index_df):\n",
    "        super().__init__()\n",
    "        self.img_folder = img_folder\n",
    "        self.index_df = index_df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename = self.index_df['path'].iloc[index]\n",
    "        label = self.index_df['label'].iloc[index]\n",
    "        payload = torch.load(Path(self.img_folder, filename))\n",
    "        return filename, payload, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posthoc Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Binary Classifier Network\n",
    "class SimpleBC(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(SimpleBC,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,64)\n",
    "        self.fc2 = nn.Linear(64,32)\n",
    "        self.fc3 = nn.Linear(32,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_df = pd.read_csv(DN_POSTHOC_LABEL_PATH)\n",
    "train_len = math.floor(len(index_df) * 0.7)\n",
    "test_len = len(index_df) - train_len\n",
    "\n",
    "train_dataset, test_dataset = random_split(AdvDataset(DN_POSTHOC_PATH, index_df), [train_len, test_len], generator=torch.Generator().manual_seed(42))\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep: 9\t Tr. Loss: 0.0001\t Tr. Acc: 1.0000\t T. Loss: 0.9811\t T. Acc: 0.9950: 100%|██████████| 10/10 [00:18<00:00,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10\n",
    "\n",
    "# Initialise network\n",
    "net = SimpleBC(1536)\n",
    "\n",
    "# Select device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "net.to(device)\n",
    "\n",
    "# Set model to train\n",
    "net.train()\n",
    "\n",
    "# define loss, optimizer, and scheduler\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.01, lr_decay=1e-08, weight_decay=0)\n",
    "# scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "\n",
    "losses = []\n",
    "accur = []\n",
    "\n",
    "train_files = []\n",
    "test_files = []\n",
    "\n",
    "# Train network\n",
    "pbar = tqdm(range(EPOCHS))\n",
    "for epoch in pbar:  # loop over the dataset multiple times\n",
    "\n",
    "    # Metrics\n",
    "    train_running_loss = 0.0\n",
    "    train_running_loss_mean = 0.0\n",
    "    train_acc = 0.0\n",
    "    train_acc_mean = 0.0\n",
    "    test_running_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "\n",
    "    for i, (filename, inputs, labels) in enumerate(train_loader, start=0):        \n",
    "        train_files.append(filename)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = net(inputs).float()\n",
    "        outputs = outputs.reshape(-1)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # Reset the gradient\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss train\n",
    "        train_running_loss += loss.item()\n",
    "        train_running_loss_mean = train_running_loss / (i+1)\n",
    "\n",
    "        # accuracy train\n",
    "        predicted = net(inputs).reshape(-1).detach().cpu().numpy().round()\n",
    "        acc_labels = labels\n",
    "        acc_labels = acc_labels.detach().cpu().numpy()\n",
    "        inter = np.equal(predicted, acc_labels)\n",
    "        train_acc += inter.sum()\n",
    "        #train_acc_mean = train_acc / (i+1)\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for filename, inputs, labels in test_loader:\n",
    "            test_files.append(filename)\n",
    "            try:\n",
    "                # get the inputs; data is a list of [inputs, labels] and write to device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).float()\n",
    "\n",
    "                # Forward Pass\n",
    "                outputs = net(inputs).float()\n",
    "                outputs = outputs.reshape(-1)\n",
    "\n",
    "                # loss test\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_running_loss += loss.item()\n",
    "\n",
    "                # accuracy test\n",
    "                outputs = outputs.detach().cpu().numpy().round()\n",
    "                comparison = np.equal(labels.detach().cpu().numpy(), outputs)\n",
    "                test_acc += comparison.sum()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Error: {}\".format(e))\n",
    "                pass\n",
    "\n",
    "    losses.append(train_running_loss_mean)\n",
    "    accur.append(train_acc_mean)\n",
    "    pbar.set_description(\"Ep: {}\\t Tr. Loss: {:.4f}\\t Tr. Acc: {:.4f}\\t T. Loss: {:.4f}\\t T. Acc: {:.4f}\".format(epoch, \n",
    "                                                                            train_running_loss_mean, \n",
    "                                                                            train_acc / len(train_loader.dataset), \n",
    "                                                                            test_running_loss, \n",
    "                                                                            test_acc / len(test_loader.dataset)))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flat = []\n",
    "for item in train_files:\n",
    "    train_flat += list(item)\n",
    "\n",
    "test_flat = []\n",
    "for item in test_files:\n",
    "    test_flat += list(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_flat).intersection(set(test_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
