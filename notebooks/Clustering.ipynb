{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino, ViTWrapper\n",
    "from src.model.data import create_loader\n",
    "from src.model.eval import validate_network\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "username = getpass.getuser()\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "\n",
    "ORI_PATH = Path(DATA_PATH, 'ori_data/validation')\n",
    "ORI_LABEL_PATH = Path(ORI_PATH,'correct_labels.txt')\n",
    "ORI_IMAGES_PATH = Path(ORI_PATH,'images')\n",
    "\n",
    "DN_PATH = Path(DATA_PATH, 'adversarial_data')\n",
    "DN_LABEL_PATH = Path(ORI_PATH,'correct_labels.txt')\n",
    "DN_IMAGES_PATH = Path(DN_PATH, 'pgd_03/validation/images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If CLASS_SUBSET is specified, INDEX_SUBSET will be ignored. Set CLASS_SUBSET=None if you want to use indexes.\n",
    "INDEX_SUBSET = get_random_indexes()\n",
    "CLASS_SUBSET = get_random_classes(number_of_classes=10)\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python $HOME/deeplearning/setup/collect_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import DINO\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights from dino_deitsmall16_pretrain/dino_deitsmall16_linearweights.pth.\n"
     ]
    }
   ],
   "source": [
    "model, linear_classifier = get_dino()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_loader = create_loader(ORI_IMAGES_PATH, ORI_LABEL_PATH, INDEX_SUBSET, CLASS_SUBSET, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_loader = create_loader(DN_IMAGES_PATH, DN_LABEL_PATH, INDEX_SUBSET, CLASS_SUBSET, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrap = ViTWrapper(model, linear_classifier, DEVICE, n_last_blocks=4, avgpool_patchtokens=False)\n",
    "model_wrap= model_wrap.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate input to linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "result = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(adv_loader):\n",
    "        x = model_wrap.transform(images).to(DEVICE)\n",
    "\n",
    "        # forward\n",
    "        intermediate_output = model_wrap.vits16.get_intermediate_layers(x, model_wrap.n_last_blocks)\n",
    "        output = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
    "        if model_wrap.avgpool_patchtokens:\n",
    "            output = torch.cat((output.unsqueeze(-1), torch.mean(intermediate_output[-1][:, 1:], dim=1).unsqueeze(-1)), dim=-1)\n",
    "            output = output.reshape(output.shape[0], -1)\n",
    "        \n",
    "        if result is None:\n",
    "            result = output\n",
    "        else:\n",
    "            result = torch.cat([result, output], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = self.transform(x)\n",
    "\n",
    "# forward\n",
    "intermediate_output = self.vits16.get_intermediate_layers(x, self.n_last_blocks)\n",
    "output = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
    "if self.avgpool_patchtokens:\n",
    "    output = torch.cat((output.unsqueeze(-1), torch.mean(intermediate_output[-1][:, 1:], dim=1).unsqueeze(-1)), dim=-1)\n",
    "    output = output.reshape(output.shape[0], -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
