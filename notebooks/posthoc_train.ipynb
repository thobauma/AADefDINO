{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This extension reloads external Python files\n",
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "from torch.utils.data import random_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino\n",
    "from src.model.train import *\n",
    "from src.model.data import *\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "username = getpass.getuser()\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "MAX_PATH = Path('/','cluster', 'scratch', 'mmathys', 'dl_data')\n",
    "# Path for intermediate outputs\n",
    "BASE_POSTHOC_PATH = Path(MAX_PATH, 'posthoc-fixed-labels/')\n",
    "#BASE_POSTHOC_PATH = Path(MAX_PATH, 'posthoc-subset/')\n",
    "POSTHOC_MODELS_PATH = Path(MAX_PATH,'posthoc-models')\n",
    "\n",
    "# Original Dataset\n",
    "ORI_PATH = Path(DATA_PATH, 'ori_data/')\n",
    "CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "\n",
    "TR_PATH = Path(ORI_PATH, 'train/')\n",
    "TR_ORI_LABEL_PATH = Path(TR_PATH,'correct_labels.txt')\n",
    "TR_ORI_IMAGES_PATH = Path(TR_PATH,'images')\n",
    "\n",
    "VAL_PATH = Path(ORI_PATH, 'validation/')\n",
    "VAL_ORI_LABEL_PATH = Path(VAL_PATH,'correct_labels.txt')\n",
    "VAL_ORI_IMAGES_PATH = Path(VAL_PATH,'images')\n",
    "\n",
    "# DAmageNet\n",
    "DN_PATH = Path(DATA_PATH, 'damageNet')\n",
    "DN_LABEL_PATH = Path(DN_PATH, 'val_damagenet.txt')\n",
    "DN_IMAGES_PATH = Path(DN_PATH, 'images')\n",
    "DN_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'damagenet')\n",
    "DN_POSTHOC_LABEL_PATH = Path(DN_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "# PGD\n",
    "TR_PGD_PATH = Path(MAX_PATH, 'adversarial_data/pgd_06/train')\n",
    "TR_PGD_LABEL_PATH = TR_ORI_LABEL_PATH\n",
    "TR_PGD_IMAGES_PATH = Path(TR_PGD_PATH, 'images')\n",
    "TR_PGD_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'pgd/train/')\n",
    "TR_PGD_POSTHOC_LABEL_PATH = Path(TR_PGD_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "VAL_PGD_PATH = Path(MAX_PATH, 'adversarial_data/pgd_06/validation')\n",
    "VAL_PGD_LABEL_PATH = VAL_ORI_LABEL_PATH\n",
    "VAL_PGD_IMAGES_PATH = Path(VAL_PGD_PATH, 'images')\n",
    "VAL_PGD_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'pgd/validation/')\n",
    "VAL_PGD_POSTHOC_LABEL_PATH = Path(VAL_PGD_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "# CW\n",
    "TR_CW_PATH = Path(MAX_PATH, 'adversarial_data/cw/train')\n",
    "TR_CW_LABEL_PATH = TR_ORI_LABEL_PATH\n",
    "TR_CW_IMAGES_PATH = Path(TR_CW_PATH, 'images')\n",
    "TR_CW_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'cw/train/')\n",
    "TR_CW_POSTHOC_LABEL_PATH = Path(TR_CW_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "VAL_CW_PATH = Path(MAX_PATH, 'adversarial_data/cw/validation')\n",
    "VAL_CW_LABEL_PATH = VAL_ORI_LABEL_PATH\n",
    "VAL_CW_IMAGES_PATH = Path(VAL_CW_PATH, 'images')\n",
    "VAL_CW_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'cw/validation/')\n",
    "VAL_CW_POSTHOC_LABEL_PATH = Path(VAL_CW_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "# FGSM\n",
    "TR_FGSM_PATH = Path(MAX_PATH, 'adversarial_data/fgsm_06/train')\n",
    "TR_FGSM_LABEL_PATH = TR_ORI_LABEL_PATH\n",
    "TR_FGSM_IMAGES_PATH = Path(TR_FGSM_PATH, 'images')\n",
    "TR_FGSM_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'fgsm/train/')\n",
    "TR_FGSM_POSTHOC_LABEL_PATH = Path(TR_FGSM_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "VAL_FGSM_PATH = Path(MAX_PATH, 'adversarial_data/fgsm_06/validation')\n",
    "VAL_FGSM_LABEL_PATH = VAL_ORI_LABEL_PATH\n",
    "VAL_FGSM_IMAGES_PATH = Path(VAL_FGSM_PATH, 'images')\n",
    "VAL_FGSM_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'fgsm/validation/')\n",
    "VAL_FGSM_POSTHOC_LABEL_PATH = Path(VAL_FGSM_POSTHOC_PATH, 'labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If CLASS_SUBSET is specified, INDEX_SUBSET will be ignored. Set CLASS_SUBSET=None if you want to use indexes.\n",
    "# INDEX_SUBSET = get_random_indexes(number_of_images = 50000, n_samples=1000)\n",
    "# CLASS_SUBSET = get_random_classes(number_of_classes = 25, min_rand_class = 1, max_rand_class = 1001)\n",
    "\n",
    "\n",
    "CLASS_SUBSET = np.load(CLASS_SUBSET_PATH)\n",
    "\n",
    "INDEX_SUBSET = None\n",
    "NUM_WORKERS= 0\n",
    "PIN_MEMORY=True\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_paths = {\n",
    "            'cw':{ \n",
    "                'b':{\n",
    "                    'train':{\n",
    "                        'label':TR_ORI_LABEL_PATH,\n",
    "                        'images':TR_CW_IMAGES_PATH\n",
    "                    },\n",
    "                    'val':\n",
    "                    {\n",
    "                        'label':VAL_ORI_LABEL_PATH,\n",
    "                        'images':VAL_CW_IMAGES_PATH\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'ori':{\n",
    "                'b':{\n",
    "                    'train':{\n",
    "                        'label':TR_ORI_LABEL_PATH,\n",
    "                        'images':TR_ORI_IMAGES_PATH\n",
    "                    },\n",
    "                    'val':{\n",
    "                        'label':VAL_ORI_LABEL_PATH,\n",
    "                        'images':VAL_ORI_IMAGES_PATH\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'dn':{\n",
    "                'b':{\n",
    "                    'train':{\n",
    "                        'label':TR_CW_PATH,\n",
    "                        'images':None\n",
    "                    },\n",
    "                    'val':\n",
    "                    {\n",
    "                        'label':VAL_ORI_LABEL_PATH,\n",
    "                        'images':DN_IMAGES_PATH\n",
    "                    }\n",
    "                 }\n",
    "            },\n",
    "            'fgsm_06':{\n",
    "                'b':{\n",
    "                    'train':{\n",
    "                        'label':TR_ORI_LABEL_PATH,\n",
    "                        'images':TR_FGSM_IMAGES_PATH\n",
    "                    },\n",
    "                    'val':\n",
    "                    {\n",
    "                        'label':VAL_ORI_LABEL_PATH,\n",
    "                        'images':VAL_FGSM_IMAGES_PATH\n",
    "                    }\n",
    "                 }\n",
    "            },\n",
    "            'pgd_06':{\n",
    "                'b':{\n",
    "                    'train':{\n",
    "                        'label':TR_ORI_LABEL_PATH,\n",
    "                        'images':TR_PGD_IMAGES_PATH\n",
    "                    },\n",
    "                    'val':\n",
    "                    {\n",
    "                        'label':VAL_ORI_LABEL_PATH,\n",
    "                        'images':VAL_PGD_IMAGES_PATH\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "}\n",
    "\n",
    "datasets = ['ori', 'cw', 'pgd_06', 'fgsm_06']\n",
    "for ds in datasets:\n",
    "    ds_dict = datasets_paths[ds]\n",
    "    ds_dict['p'] = {\n",
    "        'train': { \n",
    "            'images': Path(BASE_POSTHOC_PATH, ds, 'train', 'images'),\n",
    "            'label': Path(BASE_POSTHOC_PATH, ds, 'train', 'labels.csv')\n",
    "        },\n",
    "        'val': { \n",
    "            'images': Path(BASE_POSTHOC_PATH, ds, 'val', 'images'),\n",
    "            'label': Path(BASE_POSTHOC_PATH, ds, 'val', 'labels.csv')\n",
    "        }\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_datasets = ['cw', 'pgd_06', 'fgsm_06']\n",
    "\n",
    "train_dfs = {}\n",
    "for ds in adv_datasets:\n",
    "    train_dfs[ds] = pd.read_csv(Path(BASE_POSTHOC_PATH, ds, 'train', 'labels_merged.csv'))\n",
    "    \n",
    "val_dfs = {}\n",
    "for ds in adv_datasets:\n",
    "    val_dfs[ds] = pd.read_csv(Path(BASE_POSTHOC_PATH, ds, 'val', 'labels_merged.csv'))\n",
    "\n",
    "\n",
    "for name, df in train_dfs.items():\n",
    "    df=df[df['true_labels']==df['ori_pred']]\n",
    "    df=df[df['true_labels']!=df[name+'_pred']]\n",
    "    df =df[['file', 'true_labels', 'ori_pred', name+'_pred']]\n",
    "    train_dfs[name]=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, or_img_folder, adv_img_folder, index_df):\n",
    "        super().__init__()\n",
    "        self.or_img_folder = or_img_folder\n",
    "        self.adv_img_folder = adv_img_folder\n",
    "        self.index_df = index_df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index_df)*2\n",
    "    \n",
    "    def __getitem__(self, index):            \n",
    "        filename = self.index_df['file'].iloc[index%len(self.index_df)]\n",
    "        filename = filename.split('.')[0]+'.pt'\n",
    "        if index >= len(self.index_df):\n",
    "            payload = torch.load(Path(self.or_img_folder, filename)).cpu()\n",
    "            label = 0 #torch.tensor(0, dtype=torch.float32)\n",
    "        else:\n",
    "            payload = torch.load(Path(self.adv_img_folder, filename)).cpu()\n",
    "            label = 1 #torch.tensor(1, dtype=torch.float32)\n",
    "        return payload, label, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Binary Classifier\n",
    "class LinearBC(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        self.num_labels = 2\n",
    "        super(LinearBC,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cw\n",
      "Found checkpoint at /cluster/scratch/mmathys/dl_data/posthoc-models/cw/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/mmathys/dl_data/posthoc-models/cw/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      "=> loaded 'optimizer' from checkpoint: '/cluster/scratch/mmathys/dl_data/posthoc-models/cw/checkpoint.pth.tar'\n",
      "=> loaded 'scheduler' from checkpoint: '/cluster/scratch/mmathys/dl_data/posthoc-models/cw/checkpoint.pth.tar'\n",
      "Training of the supervised linear classifier on frozen features completed.\n",
      "Top-1 test accuracy: 96.6\n",
      "pgd_06\n",
      "Epoch: [0]  [  0/249]  eta: 0:05:12  lr: 0.001000  loss: 1.268624 (1.268624)  time: 1.253843  data: 1.245587  max mem: 2\n",
      "Epoch: [0]  [ 20/249]  eta: 0:04:35  lr: 0.001000  loss: 0.141415 (0.268872)  time: 1.201866  data: 1.200620  max mem: 2\n",
      "Epoch: [0]  [ 40/249]  eta: 0:04:08  lr: 0.001000  loss: 0.060325 (0.169525)  time: 1.175437  data: 1.174234  max mem: 2\n",
      "Epoch: [0]  [ 60/249]  eta: 0:03:52  lr: 0.001000  loss: 0.041023 (0.128723)  time: 1.314170  data: 1.312987  max mem: 2\n",
      "Epoch: [0]  [ 80/249]  eta: 0:03:34  lr: 0.001000  loss: 0.027516 (0.104581)  time: 1.385116  data: 1.383910  max mem: 2\n",
      "Epoch: [0]  [100/249]  eta: 0:03:12  lr: 0.001000  loss: 0.027459 (0.089619)  time: 1.390018  data: 1.388830  max mem: 2\n",
      "Epoch: [0]  [120/249]  eta: 0:02:49  lr: 0.001000  loss: 0.020484 (0.078577)  time: 1.410287  data: 1.409096  max mem: 2\n",
      "Epoch: [0]  [140/249]  eta: 0:02:24  lr: 0.001000  loss: 0.021366 (0.070617)  time: 1.424111  data: 1.422913  max mem: 2\n",
      "Epoch: [0]  [160/249]  eta: 0:01:59  lr: 0.001000  loss: 0.018444 (0.064538)  time: 1.443671  data: 1.442478  max mem: 2\n",
      "Epoch: [0]  [180/249]  eta: 0:01:33  lr: 0.001000  loss: 0.017582 (0.059625)  time: 1.411315  data: 1.410148  max mem: 2\n",
      "Epoch: [0]  [200/249]  eta: 0:01:06  lr: 0.001000  loss: 0.016179 (0.055763)  time: 1.443553  data: 1.442329  max mem: 2\n",
      "Epoch: [0]  [220/249]  eta: 0:00:39  lr: 0.001000  loss: 0.012041 (0.052160)  time: 1.432575  data: 1.431419  max mem: 2\n",
      "Epoch: [0]  [240/249]  eta: 0:00:12  lr: 0.001000  loss: 0.013244 (0.049242)  time: 1.399775  data: 1.398577  max mem: 2\n",
      "Epoch: [0]  [248/249]  eta: 0:00:01  lr: 0.001000  loss: 0.011319 (0.048135)  time: 1.383452  data: 1.382261  max mem: 2\n",
      "Epoch: [0] Total time: 0:05:40 (1.368526 s / it)\n",
      "Averaged stats: lr: 0.001000  loss: 0.011319 (0.048135)\n",
      "Test:  [ 0/10]  eta: 0:00:13  loss: 0.016971 (0.016971)  acc1: 100.000000 (100.000000)  time: 1.361878  data: 1.360718  max mem: 2\n",
      "Test:  [ 9/10]  eta: 0:00:01  loss: 0.007999 (0.009839)  acc1: 100.000000 (99.920000)  time: 1.375949  data: 1.375190  max mem: 2\n",
      "Test: Total time: 0:00:13 (1.376135 s / it)\n",
      "* Acc@1 99.920 loss 0.010\n",
      "Accuracy at epoch 0 of the network on the 10 test images: 99.9%\n",
      "Max accuracy so far: 99.92%\n",
      "Epoch: [1]  [  0/249]  eta: 0:05:03  lr: 0.000750  loss: 0.008951 (0.008951)  time: 1.220228  data: 1.219046  max mem: 2\n",
      "Epoch: [1]  [ 20/249]  eta: 0:04:47  lr: 0.000750  loss: 0.010816 (0.011919)  time: 1.259028  data: 1.257803  max mem: 2\n",
      "Epoch: [1]  [ 40/249]  eta: 0:04:22  lr: 0.000750  loss: 0.009326 (0.011425)  time: 1.256226  data: 1.255030  max mem: 2\n",
      "Epoch: [1]  [ 60/249]  eta: 0:04:01  lr: 0.000750  loss: 0.010041 (0.011623)  time: 1.315291  data: 1.314095  max mem: 2\n",
      "Epoch: [1]  [ 80/249]  eta: 0:03:36  lr: 0.000750  loss: 0.011434 (0.012176)  time: 1.303945  data: 1.302769  max mem: 2\n",
      "Epoch: [1]  [100/249]  eta: 0:03:11  lr: 0.000750  loss: 0.009543 (0.011967)  time: 1.309750  data: 1.308552  max mem: 2\n",
      "Epoch: [1]  [120/249]  eta: 0:02:47  lr: 0.000750  loss: 0.010197 (0.011757)  time: 1.334530  data: 1.333356  max mem: 2\n",
      "Epoch: [1]  [140/249]  eta: 0:02:21  lr: 0.000750  loss: 0.009659 (0.011496)  time: 1.339098  data: 1.337940  max mem: 2\n",
      "Epoch: [1]  [160/249]  eta: 0:01:56  lr: 0.000750  loss: 0.008920 (0.011395)  time: 1.379420  data: 1.378160  max mem: 2\n",
      "Epoch: [1]  [180/249]  eta: 0:01:30  lr: 0.000750  loss: 0.014525 (0.011797)  time: 1.350568  data: 1.349361  max mem: 2\n",
      "Epoch: [1]  [200/249]  eta: 0:01:04  lr: 0.000750  loss: 0.008994 (0.011840)  time: 1.332144  data: 1.330973  max mem: 2\n",
      "Epoch: [1]  [220/249]  eta: 0:00:38  lr: 0.000750  loss: 0.009300 (0.011804)  time: 1.338563  data: 1.337393  max mem: 2\n",
      "Epoch: [1]  [240/249]  eta: 0:00:11  lr: 0.000750  loss: 0.008658 (0.011621)  time: 1.347654  data: 1.346488  max mem: 2\n",
      "Epoch: [1]  [248/249]  eta: 0:00:01  lr: 0.000750  loss: 0.007989 (0.011596)  time: 1.328799  data: 1.327628  max mem: 2\n",
      "Epoch: [1] Total time: 0:05:28 (1.321011 s / it)\n",
      "Averaged stats: lr: 0.000750  loss: 0.007989 (0.011596)\n",
      "Test:  [ 0/10]  eta: 0:00:13  loss: 0.010382 (0.010382)  acc1: 100.000000 (100.000000)  time: 1.316556  data: 1.315839  max mem: 2\n",
      "Test:  [ 9/10]  eta: 0:00:01  loss: 0.004432 (0.006240)  acc1: 100.000000 (99.920000)  time: 1.291842  data: 1.291151  max mem: 2\n",
      "Test: Total time: 0:00:12 (1.292000 s / it)\n",
      "* Acc@1 99.920 loss 0.006\n",
      "Accuracy at epoch 1 of the network on the 10 test images: 99.9%\n",
      "Max accuracy so far: 99.92%\n",
      "Epoch: [2]  [  0/249]  eta: 0:04:56  lr: 0.000250  loss: 0.009497 (0.009497)  time: 1.192541  data: 1.191237  max mem: 2\n",
      "Epoch: [2]  [ 20/249]  eta: 0:04:42  lr: 0.000250  loss: 0.006464 (0.008329)  time: 1.235145  data: 1.233978  max mem: 2\n",
      "Epoch: [2]  [ 40/249]  eta: 0:04:20  lr: 0.000250  loss: 0.006409 (0.008253)  time: 1.255524  data: 1.254338  max mem: 2\n",
      "Epoch: [2]  [ 60/249]  eta: 0:03:55  lr: 0.000250  loss: 0.007311 (0.008713)  time: 1.250359  data: 1.249196  max mem: 2\n",
      "Epoch: [2]  [ 80/249]  eta: 0:03:30  lr: 0.000250  loss: 0.007891 (0.009315)  time: 1.249142  data: 1.247950  max mem: 2\n",
      "Epoch: [2]  [100/249]  eta: 0:03:07  lr: 0.000250  loss: 0.008083 (0.009577)  time: 1.303752  data: 1.302566  max mem: 2\n",
      "Epoch: [2]  [120/249]  eta: 0:02:43  lr: 0.000250  loss: 0.007858 (0.009695)  time: 1.302306  data: 1.301152  max mem: 2\n",
      "Epoch: [2]  [140/249]  eta: 0:02:18  lr: 0.000250  loss: 0.008138 (0.009607)  time: 1.318025  data: 1.316886  max mem: 2\n",
      "Epoch: [2]  [160/249]  eta: 0:01:54  lr: 0.000250  loss: 0.007567 (0.009367)  time: 1.342586  data: 1.341403  max mem: 2\n",
      "Epoch: [2]  [180/249]  eta: 0:01:29  lr: 0.000250  loss: 0.008334 (0.009385)  time: 1.382292  data: 1.381079  max mem: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger_dict = {}\n",
    "for ds in adv_datasets:\n",
    "    ds_p = datasets_paths[ds]['p']\n",
    "    print(f'''{ds}''')\n",
    "    \n",
    "    \n",
    "    # loaders\n",
    "    train_set = AdvDataset(datasets_paths['ori']['p']['train']['images'],ds_p['train']['images'], train_dfs[ds])\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=True)\n",
    "    val_set = AdvDataset(datasets_paths['ori']['p']['val']['images'],ds_p['val']['images'], val_dfs[ds])\n",
    "    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=False)\n",
    "    \n",
    "    # Initialise network\n",
    "    classifier = LinearBC(1536)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    classifier.cuda()\n",
    "    optimizer = torch.optim.Adagrad(classifier.parameters(), lr=0.001, lr_decay=1e-08, weight_decay=0)\n",
    "    logger_dict[ds] = train(model=None, \n",
    "                            classifier=classifier, \n",
    "                            train_loader=train_loader, \n",
    "                            validation_loader=val_loader, \n",
    "                            log_dir=Path(POSTHOC_MODELS_PATH,ds),\n",
    "                            tensor_dir=None, \n",
    "                            optimizer=optimizer, \n",
    "                            criterion=criterion, \n",
    "                            adversarial_attack=None, \n",
    "                            epochs=EPOCHS, \n",
    "                            val_freq=1, \n",
    "                            batch_size=16,  \n",
    "                            lr=0.001, \n",
    "                            to_restore = {\"epoch\": 0, \"best_acc\": 0.}, \n",
    "                            n=4, \n",
    "                            avgpool_patchtokens=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posthoc_forward_pass(datasets, datasets_paths):\n",
    "    for ds in datasets:\n",
    "        ds_b = datasets_paths[ds]['b']\n",
    "        ds_p = datasets_paths[ds]['p']\n",
    "        logger_dict[ds] = {}\n",
    "        transform = ONLY_NORMALIZE_TRANSFORM\n",
    "        if ds == 'ori':\n",
    "            transform = ORIGINAL_TRANSFORM\n",
    "        for tv in ['train', 'val']:\n",
    "            print(f'''images: {ds_b[tv]['images']}\\nlabel: {ds_b[tv]['label']}\\npred: {ds_p[tv]['label']}''')\n",
    "            data_set = ImageDataset(ds_b[tv]['images'], ds_b[tv]['label'], transform, class_subset=CLASS_SUBSET)\n",
    "            data_loader = DataLoader(data_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=False)\n",
    "            print(f'''{ds}: {tv} {len(data_set)}''')\n",
    "            logger_dict[ds][tv] = validate_network(model, linear_classifier, data_loader, adversarial_attack=None, tensor_dir=ds_p[tv]['images'], path_predictions=ds_p[tv]['label'])\n",
    "    return logger_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
