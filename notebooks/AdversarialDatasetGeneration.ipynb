{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino, ViTWrapper\n",
    "from src.model.data import *\n",
    "from src.model.eval import validate_network_dino\n",
    "\n",
    "# Custom imports\n",
    "import torchattacks\n",
    "from torchattacks import *\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as pth_transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "username = getpass.getuser()\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "MAX_PATH = Path('/','cluster', 'scratch', 'mmathys', 'dl_data')\n",
    "\n",
    "# Image Net\n",
    "ORI_PATH = Path(DATA_PATH, 'ori_data')\n",
    "CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "\n",
    "VAL_PATH = Path(ORI_PATH, 'validation')\n",
    "VAL_IMAGES_PATH = Path(VAL_PATH,'images')\n",
    "VAL_LABEL_PATH = Path(VAL_PATH, 'correct_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_SUBSET = None\n",
    "NUM_WORKERS= 0\n",
    "PIN_MEMORY=True\n",
    "\n",
    "BATCH_SIZE = 80\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python ../setup/collect_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights.\n"
     ]
    }
   ],
   "source": [
    "model, linear_classifier = get_dino(model_name='vit_small', patch_size=16, n_last_blocks=4, avgpool_patchtokens=False, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NO_NORM_TRANSFORM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-eb90175b493d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpth_transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAL_IMAGES_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAL_LABEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNO_NORM_TRANSFORM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_WORKERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIN_MEMORY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NO_NORM_TRANSFORM' is not defined"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms as pth_transforms\n",
    "\n",
    "val_dataset = ImageDataset(VAL_IMAGES_PATH, VAL_LABEL_PATH, NO_NORM_TRANSFORM, None, index_subset=None)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrap = ViTWrapper(model, linear_classifier, DEVICE, n_last_blocks=4, avgpool_patchtokens=False)\n",
    "model_wrap= model_wrap.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack model and generate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = [\n",
    "    #(PGD(model_wrap, eps=0.06, alpha=0.015, steps=20), 'pgd_06'),\n",
    "    #(CW(model_wrap, c=50, lr=0.0031, steps=30), 'cw'),\n",
    "    #(PGDL2(model_wrap, eps=0.03, alpha=0.015, steps=20), 'pgd_l2_03')\n",
    "    (FGSM(model_wrap, eps=0.06), 'fgsm_06')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(val_loader.dataset)\n",
    "\n",
    "for atk, name in attacks:\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "    print(atk)\n",
    "    \n",
    "    STORE_PATH = Path(MAX_PATH, f'adversarial_data/{name}')\n",
    "    STORE_LABEL_PATH = Path(STORE_PATH, 'validation/labels.txt')\n",
    "    STORE_IMAGES_PATH = Path(STORE_PATH, 'validation/images/')\n",
    "    adv_labels = {}\n",
    "    \n",
    "    if not os.path.exists(STORE_IMAGES_PATH):\n",
    "        os.makedirs(STORE_IMAGES_PATH)\n",
    "\n",
    "    correct = 0\n",
    "    clean_correct = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for images, labels, img_names in tqdm(val_loader):\n",
    "        \n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        adv_images = atk(images, labels)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model_wrap(adv_images)\n",
    "\n",
    "        _, pre = torch.max(outputs.data, 1)\n",
    "\n",
    "        correct += (pre == labels).sum()\n",
    "        \n",
    "        for i in range(adv_images.shape[0]):\n",
    "            save_image(adv_images[i,:,:,:], Path(STORE_IMAGES_PATH, Path(img_names[i])))\n",
    "            adv_labels[img_names[i]] = labels.cpu().numpy()[i]\n",
    "            \n",
    "        del images\n",
    "        del adv_images\n",
    "        del labels\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print('Total elapsed time (sec): %.2f' % (time.time() - start))\n",
    "    print('Accuracy against attack: %.2f %%' % (100 * float(correct) / total))\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(adv_labels, orient='index')\n",
    "    df.to_csv(STORE_LABEL_PATH, sep=\" \", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
