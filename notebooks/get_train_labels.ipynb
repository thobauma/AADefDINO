{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "DATA_PATH = Path('..', 'data_dir')\n",
    "MAX_PATH = DATA_PATH\n",
    "LOG_PATH = Path(DATA_PATH, 'logs')\n",
    "\n",
    "DN_PATH = Path(DATA_PATH, 'damageNet')\n",
    "DN_LABEL_PATH = Path(DN_PATH, 'val_damagenet.txt')\n",
    "DN_IMAGES_PATH = Path(DN_PATH, 'images')\n",
    "\n",
    "ORI_PATH = Path(DATA_PATH, 'ori')\n",
    "\n",
    "ORI_LABEL_PATH = Path(ORI_PATH,'labels.csv')\n",
    "ORI_IMAGES_PATH = Path(ORI_PATH,'train','images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from contextlib import contextmanager\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import tempfile\n",
    "from typing import Any, Dict, List, Iterator, Optional, Tuple\n",
    "import torch\n",
    "from torchvision.datasets.utils import check_integrity, extract_archive, verify_str_arg\n",
    "ARCHIVE_META = {\n",
    "    'train': ('ILSVRC2012_img_train.tar', '1d675b47d978889d74fa0da5fadfb00e'),\n",
    "    'val': ('ILSVRC2012_img_val.tar', '29b22e2961454d5413ddabcf34fc5622'),\n",
    "    'devkit': ('ILSVRC2012_devkit_t12.tar.gz', 'fa75699e90414af021442c21a62c3abf')\n",
    "}\n",
    "META_FILE = \"meta.bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _verify_archive(root: str, file: str, md5: str) -> None:\n",
    "    if not check_integrity(os.path.join(root, file), md5):\n",
    "        msg = (\"The archive {} is not present in the root directory or is corrupted. \"\n",
    "               \"You need to download it externally and place it in {}.\")\n",
    "        raise RuntimeError(msg.format(file, root))\n",
    "def parse_devkit_archive(root: str, file: Optional[str] = None) -> None:\n",
    "    \"\"\"Parse the devkit archive of the ImageNet2012 classification dataset and save\n",
    "    the meta information in a binary file.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory containing the devkit archive\n",
    "        file (str, optional): Name of devkit archive. Defaults to\n",
    "            'ILSVRC2012_devkit_t12.tar.gz'\n",
    "    \"\"\"\n",
    "    import scipy.io as sio\n",
    "\n",
    "    def parse_meta_mat(devkit_root: str) -> Tuple[Dict[int, str], Dict[str, str]]:\n",
    "        metafile = os.path.join(devkit_root, \"data\", \"meta.mat\")\n",
    "        meta = sio.loadmat(metafile, squeeze_me=True)['synsets']\n",
    "        nums_children = list(zip(*meta))[4]\n",
    "        meta = [meta[idx] for idx, num_children in enumerate(nums_children)\n",
    "                if num_children == 0]\n",
    "        idcs, wnids, classes = list(zip(*meta))[:3]\n",
    "        classes = [tuple(clss.split(', ')) for clss in classes]\n",
    "        idx_to_wnid = {idx: wnid for idx, wnid in zip(idcs, wnids)}\n",
    "        wnid_to_classes = {wnid: clss for wnid, clss in zip(wnids, classes)}\n",
    "        return idx_to_wnid, wnid_to_classes\n",
    "\n",
    "    def parse_val_groundtruth_txt(devkit_root: str) -> List[int]:\n",
    "        file = os.path.join(devkit_root, \"data\",\n",
    "                            \"ILSVRC2012_validation_ground_truth.txt\")\n",
    "        with open(file, 'r') as txtfh:\n",
    "            val_idcs = txtfh.readlines()\n",
    "        return [int(val_idx) for val_idx in val_idcs]\n",
    "\n",
    "    @contextmanager\n",
    "    def get_tmp_dir() -> Iterator[str]:\n",
    "        tmp_dir = tempfile.mkdtemp()\n",
    "        try:\n",
    "            yield tmp_dir\n",
    "        finally:\n",
    "            shutil.rmtree(tmp_dir)\n",
    "\n",
    "    archive_meta = ARCHIVE_META[\"devkit\"]\n",
    "    if file is None:\n",
    "        file = archive_meta[0]\n",
    "    md5 = archive_meta[1]\n",
    "\n",
    "    _verify_archive(root, file, md5)\n",
    "\n",
    "    with get_tmp_dir() as tmp_dir:\n",
    "        extract_archive(os.path.join(root, file), tmp_dir)\n",
    "\n",
    "        devkit_root = os.path.join(tmp_dir, \"ILSVRC2012_devkit_t12\")\n",
    "        idx_to_wnid, wnid_to_classes = parse_meta_mat(devkit_root)\n",
    "        val_idcs = parse_val_groundtruth_txt(devkit_root)\n",
    "        val_wnids = [idx_to_wnid[idx] for idx in val_idcs]\n",
    "\n",
    "        torch.save((wnid_to_classes, val_wnids), os.path.join(root, META_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "def parse_meta_mat(devkit_root: str) -> Tuple[Dict[int, str], Dict[str, str]]:\n",
    "        metafile = os.path.join(devkit_root, \"data\", \"meta.mat\")\n",
    "        meta = sio.loadmat(metafile, squeeze_me=True)['synsets']\n",
    "        nums_children = list(zip(*meta))[4]\n",
    "        meta = [meta[idx] for idx, num_children in enumerate(nums_children)\n",
    "                if num_children == 0]\n",
    "        idcs, wnids, classes = list(zip(*meta))[:3]\n",
    "        classes = [tuple(clss.split(', ')) for clss in classes]\n",
    "        idx_to_wnid = {idx: wnid for idx, wnid in zip(idcs, wnids)}\n",
    "        wnid_to_classes = {wnid: clss for wnid, clss in zip(wnids, classes)}\n",
    "        return idx_to_wnid, wnid_to_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_wnid, wnid_to_classes = parse_meta_mat(Path(ORI_PATH, 'info'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_wnid_map = idx_to_wnid.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_wnid_map = pd.DataFrame(idx_to_wnid_map, columns=['label', 'wnid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=[f.name for f in ORI_IMAGES_PATH.glob('*.JPEG')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(files, columns=['files'])\n",
    "label_wnid_map = pd.DataFrame(idx_to_wnid_map, columns=['label', 'wnid'])\n",
    "label_wnid_map['label'] = label_wnid_map['label'].astype(int)\n",
    "train['wnid'] = train['files'].str.split('_').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = pd.merge(train, label_wnid_map, how='left', on='wnid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = tt.drop(columns=['wnid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.to_csv(Path(ORI_PATH, 'train', 'labels.csv'), sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
