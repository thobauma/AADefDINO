{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Benchmarking across models\n",
    "\n",
    "This notebook includes the code to benchmark supervised and unsupervised ViTs against different adversarial attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino, ViTWrapper\n",
    "from src.model.train import validate_network\n",
    "from src.model.data import *\n",
    "\n",
    "# Custom imports\n",
    "import torchattacks\n",
    "from torchattacks import *\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as pth_transforms\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "username = getpass.getuser()\n",
    "DATA_PATH = Path('/cluster/scratch/thobauma/data/ori/')\n",
    "\n",
    "ORI_PATH = Path(DATA_PATH, 'validation')\n",
    "ORI_LABEL_PATH = Path(ORI_PATH,'labels.csv')\n",
    "ORI_IMAGES_PATH = Path(ORI_PATH,'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_SUBSET = get_random_indexes(n_samples=3000) # Randomly sample data\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "NUM_WORKERS= 0\n",
    "PIN_MEMORY=True\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(INDEX_SUBSET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AdvTrainingImageDataset(ORI_IMAGES_PATH, ORI_LABEL_PATH, ORIGINAL_TRANSFORM, index_subset=INDEX_SUBSET, return_reduced=False)\n",
    "loader = DataLoader(dataset, batch_size=50, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate DINO ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dino_small():\n",
    "    model_small, linear_classifier_small = get_dino(model_name='vit_small', patch_size=16, n_last_blocks=4, avgpool_patchtokens=False, device='cpu')\n",
    "    dino_small = ViTWrapper(model_small, linear_classifier_small, device='cpu', n_last_blocks=4, avgpool_patchtokens=False)\n",
    "    return dino_small\n",
    "\n",
    "def get_dino_base():\n",
    "    model_base, linear_classifier_base = get_dino(model_name='vit_base', patch_size=16, n_last_blocks=1, avgpool_patchtokens=True, device='cpu')\n",
    "    dino_base = ViTWrapper(model_base, linear_classifier_base, device='cpu', n_last_blocks=1, avgpool_patchtokens=True)\n",
    "    return dino_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrap models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_base built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights from dino_vitbase16_pretrain/dino_vitbase16_linearweights.pth.\n",
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights from dino_deitsmall16_pretrain/dino_deitsmall16_linearweights.pth.\n"
     ]
    }
   ],
   "source": [
    "#### DINO ViT/B-16\n",
    "dino_base = get_dino_base()\n",
    "\n",
    "#### DINO ViT/S-16\n",
    "dino_small = get_dino_small()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Compute clean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dino(model_wrap, loader):\n",
    "    clean_correct = 0\n",
    "    total = len(loader.dataset)\n",
    "    start = time.time()\n",
    "    model_wrap.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in tqdm(loader):\n",
    "\n",
    "            cuda_images = images.to(DEVICE)\n",
    "            clean_outputs = model_wrap(cuda_images)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            _, pre_clean = torch.max(clean_outputs.data, 1)\n",
    "\n",
    "            clean_correct += (pre_clean == labels).sum()\n",
    "\n",
    "    print('Total elapsed time (sec): %.2f' % (time.time() - start))\n",
    "    print('Clean accuracy: %.2f %%' % (100 * float(clean_correct) / total))\n",
    "    model_wrap.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:31<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time (sec): 31.52\n",
      "Clean accuracy: 78.47 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_dino(dino_small, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:39<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time (sec): 39.91\n",
      "Clean accuracy: 78.87 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_dino(dino_base, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate supervised ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model\n",
    "\n",
    "We use pretrained model from: https://github.com/lukemelas/PyTorch-Pretrained-ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from pytorch_pretrained_vit import ViT\n",
    "\n",
    "class CustomViT(ViT):\n",
    "    def __init__(self):\n",
    "        super().__init__('B_16_imagenet1k', pretrained=True)\n",
    "        self.transform = transforms.Compose([\n",
    "    transforms.Normalize(0.5, 0.5),\n",
    "])\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = self.transform(x)\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_supervised_vit():\n",
    "    sup_vit = CustomViT().to('cpu').eval()\n",
    "    return sup_vit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_dataset = AdvTrainingImageDataset(ORI_IMAGES_PATH, ORI_LABEL_PATH, transforms.Compose([\n",
    "    transforms.Resize((384, 384)), \n",
    "    transforms.ToTensor(),\n",
    "]), index_subset=INDEX_SUBSET, return_reduced=False)\n",
    "sup_loader = DataLoader(sup_dataset, batch_size=10, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Compute clean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:30<00:00,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time (sec): 90.57\n",
      "Clean accuracy: 84.13 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_supervised_vit(sup_vit, loader):\n",
    "    sup_vit.cuda()\n",
    "    clean_correct = 0\n",
    "    total = len(loader.dataset)\n",
    "    start = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in tqdm(loader):\n",
    "\n",
    "            cuda_images = images.to(DEVICE)\n",
    "            clean_outputs = sup_vit(cuda_images)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            _, pre_clean = torch.max(clean_outputs.data, 1)\n",
    "\n",
    "            clean_correct += (pre_clean == labels).sum()\n",
    "\n",
    "    sup_vit.cpu()\n",
    "    print('Total elapsed time (sec): %.2f' % (time.time() - start))\n",
    "    print('Clean accuracy: %.2f %%' % (100 * float(clean_correct) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_supervised_vit(sup_vit, sup_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import ResNet, Bottleneck\n",
    "\n",
    "class CustomResNet(ResNet):\n",
    "    def __init__(self, classifier=None):\n",
    "        super(CustomResNet, self).__init__(block=Bottleneck, layers=[3, 4, 6, 3])\n",
    "        self.load_state_dict(torch.load(\"/cluster/scratch/jrando/resnet/resnet.pth\"))\n",
    "        \n",
    "    def _forward_impl(self, x):\n",
    "        # Normalize\n",
    "        transform = pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        x = transform(x)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet():\n",
    "    return CustomResNet().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_resnet(resnet, loader):\n",
    "    resnet.cuda()\n",
    "    clean_correct = 0\n",
    "    total = len(loader.dataset)\n",
    "    start = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in tqdm(loader):\n",
    "            \n",
    "            cuda_images = images.to(DEVICE)\n",
    "            clean_outputs = resnet(cuda_images)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            _, pre_clean = torch.max(clean_outputs.data, 1)\n",
    "\n",
    "            clean_correct += (pre_clean == labels).sum()\n",
    "\n",
    "    resnet.cpu()\n",
    "    print('Total elapsed time (sec): %.2f' % (time.time() - start))\n",
    "    print('Clean accuracy: %.2f %%' % (100 * float(clean_correct) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:51<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time (sec): 51.16\n",
      "Clean accuracy: 75.43 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_resnet(resnet, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks\n",
    "We use TorchAttack library. See: https://github.com/Harry24k/adversarial-attacks-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create attacks and test on every model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete models if created before\n",
    "del resnet\n",
    "del sup_vit\n",
    "del dino_small\n",
    "del dino_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mapping = {'dino_base': get_dino_base,\n",
    "                'dino_small': get_dino_small,\n",
    "                'resnet': get_resnet,\n",
    "                'sup_vit': get_supervised_vit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attack(model, eps=0.03, alpha=(0.03*2)/3, steps=3):\n",
    "    return  PGD(model, eps=eps, alpha=alpha, steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_generated = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_base built.\n",
      "Embed dim 1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We load the reference pretrained linear weights from dino_vitbase16_pretrain/dino_vitbase16_linearweights.pth.\n",
      "----------------------------------------------------------------------\n",
      "dino_base PGD(model_name=ViTWrapper, device=cuda:0, eps=0.03, alpha=0.02, steps=3, random_start=True, attack_mode=default, return_type=float)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [02:46<00:00,  2.78s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights from dino_deitsmall16_pretrain/dino_deitsmall16_linearweights.pth.\n",
      "----------------------------------------------------------------------\n",
      "dino_small PGD(model_name=ViTWrapper, device=cuda:0, eps=0.03, alpha=0.02, steps=3, random_start=True, attack_mode=default, return_type=float)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 51/60 [01:02<00:11,  1.25s/it]"
     ]
    }
   ],
   "source": [
    "for model_name in model_mapping.keys():\n",
    "    img_loader = sup_loader if model_name==\"sup_vit\" else loader\n",
    "    images_generated[model_name] = []\n",
    "    model = model_mapping[model_name]().cuda()\n",
    "    atk = get_attack(model)\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "    print(model_name, atk)\n",
    "\n",
    "    for images, labels, _ in tqdm(img_loader):\n",
    "\n",
    "        labels = labels.to(DEVICE)\n",
    "        adv_images = atk(images, labels)\n",
    "        images_generated[model_name].append(adv_images.cpu())\n",
    "\n",
    "        del images\n",
    "        del labels\n",
    "        del adv_images\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    del model\n",
    "    del atk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
