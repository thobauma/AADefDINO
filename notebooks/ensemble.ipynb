{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "# from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino, ViTWrapper\n",
    "from src.model.data import *\n",
    "from src.model.train import *\n",
    "from src.model.multihead_model import *\n",
    "from src.helpers.helpers import create_paths\n",
    "\n",
    "from torchattacks import *\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "MAX_PATH = Path('/','cluster', 'scratch', 'mmathys', 'dl_data')\n",
    "\n",
    "BASE_ADV_PATH = Path(MAX_PATH, 'adversarial_data_tensors')\n",
    "BASE_POSTHOC_PATH = Path(MAX_PATH, 'posthoc_tensors')\n",
    "POSTHOC_MODELS_PATH = Path(MAX_PATH, 'posthoc_models')\n",
    "\n",
    "LINEAR_CLASSIFIER_MODELS_PATH = Path(MAX_PATH, 'linear_classifier_models')\n",
    "\n",
    "LINEAR_CLASSIFIER_EVAL_PATH = Path(MAX_PATH, 'linear_classifier_evaluation')\n",
    "LINEAR_CLASSIFIER_EVAL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MULTIHEAD_EVAL_PATH = Path(MAX_PATH, 'multihead_eval')\n",
    "MULTIHEAD_EVAL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ORI_PATH = Path(DATA_PATH, 'ori')\n",
    "CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "CLASS_SUBSET = np.load(CLASS_SUBSET_PATH)\n",
    "\n",
    "ADV_DATASETS = ['cw', 'fgsm_06', 'pgd_03']\n",
    "\n",
    "DATASETS = [*ADV_DATASETS, 'ori']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_SUBSET = None\n",
    "NUM_WORKERS= 0\n",
    "PIN_MEMORY=True\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS= 3\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATHS = create_paths(data_name='ori',\n",
    "                 datasets_paths=None,  \n",
    "                 initial_base_path=DATA_PATH, \n",
    "                 posthoc_base_path=BASE_POSTHOC_PATH, \n",
    "                 train_str='train', \n",
    "                 val_str='validation')\n",
    "\n",
    "for adv_ds in ADV_DATASETS:\n",
    "    DATA_PATHS = create_paths(data_name=adv_ds,\n",
    "                 datasets_paths=DATA_PATHS,  \n",
    "                 initial_base_path=BASE_ADV_PATH, \n",
    "                 posthoc_base_path=BASE_POSTHOC_PATH, \n",
    "                 train_str='train', \n",
    "                 val_str='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import DINO\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights.\n"
     ]
    }
   ],
   "source": [
    "model, linear_classifier = get_dino()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ori\n",
      "/cluster/scratch/thobauma/dl_data/ori/train/images\n",
      "/cluster/scratch/thobauma/dl_data/ori/train/labels.csv\n",
      "cw\n",
      "/cluster/scratch/mmathys/dl_data/adversarial_data_tensors/cw/train/images\n",
      "/cluster/scratch/mmathys/dl_data/adversarial_data_tensors/cw/train/labels.csv\n",
      "fgsm_06\n",
      "/cluster/scratch/mmathys/dl_data/adversarial_data_tensors/fgsm_06/train/images\n",
      "/cluster/scratch/mmathys/dl_data/adversarial_data_tensors/fgsm_06/train/labels.csv\n",
      "pgd_03\n",
      "/cluster/scratch/mmathys/dl_data/adversarial_data_tensors/pgd_03/train/images\n",
      "/cluster/scratch/mmathys/dl_data/adversarial_data_tensors/pgd_03/train/labels.csv\n"
     ]
    }
   ],
   "source": [
    "for k, v in DATA_PATHS.items():\n",
    "    print(k)\n",
    "    print(v[\"init\"][\"train\"][\"images\"])\n",
    "    print(v[\"init\"][\"train\"][\"label\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cw\n",
      "fgsm_06\n",
      "pgd_03\n"
     ]
    }
   ],
   "source": [
    "# Remember to set the correct transformation\n",
    "# encoder\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit([i for i in CLASS_SUBSET])\n",
    "\n",
    "loader_dict = defaultdict(dict)\n",
    "\n",
    "for k, v in DATA_PATHS.items():\n",
    "    if not k == \"ori\":\n",
    "        print(k)\n",
    "        adv_train_dataset = EnsembleDataset(v[\"init\"][\"train\"][\"images\"], \n",
    "                                            v[\"init\"][\"train\"][\"label\"])\n",
    "        \n",
    "        adv_val_dataset = EnsembleDataset(v[\"init\"][\"validation\"][\"images\"], \n",
    "                                          v[\"init\"][\"validation\"][\"label\"])\n",
    "\n",
    "        loader_dict[k][\"train\"] = DataLoader(adv_train_dataset, \n",
    "                                             batch_size=BATCH_SIZE, \n",
    "                                             num_workers=NUM_WORKERS, \n",
    "                                             pin_memory=PIN_MEMORY, \n",
    "                                             shuffle=True)\n",
    "        \n",
    "        loader_dict[k][\"validation\"] = DataLoader(adv_val_dataset, \n",
    "                                             batch_size=BATCH_SIZE, \n",
    "                                             num_workers=NUM_WORKERS, \n",
    "                                             pin_memory=PIN_MEMORY, \n",
    "                                             shuffle=False)\n",
    "    else:\n",
    "        clean_train_dataset = ImageDataset(v[\"init\"][\"train\"][\"images\"], \n",
    "                                           v[\"init\"][\"train\"][\"label\"], \n",
    "                                           ORIGINAL_TRANSFORM,\n",
    "                                           CLASS_SUBSET, \n",
    "                                           index_subset=None, \n",
    "                                           label_encoder=label_encoder)\n",
    "\n",
    "        clean_val_dataset = ImageDataset(v[\"init\"][\"validation\"][\"images\"], \n",
    "                                         v[\"init\"][\"validation\"][\"label\"],\n",
    "                                         ORIGINAL_TRANSFORM,\n",
    "                                         CLASS_SUBSET, \n",
    "                                         index_subset=None, \n",
    "                                         label_encoder=label_encoder)\n",
    "        \n",
    "        loader_dict[\"ori\"][\"train\"] = DataLoader(clean_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=True)\n",
    "        \n",
    "        loader_dict[\"ori\"][\"validation\"] = DataLoader(clean_val_dataset,\n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'last_hope'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(dim, num_labels)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # linear layer\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Binary Classifier\n",
    "class LinearBC(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        self.num_labels = 2\n",
    "        super(LinearBC,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train various classifiers on all adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_attacks = [\n",
    "            \"ori\", \n",
    "            \"cw\",\n",
    "#             \"fgsm_06\", \n",
    "#             \"pgd_03\"\n",
    "]\n",
    "\n",
    "for attack, loaders in loader_dict.items():\n",
    "    if attack not in run_attacks:\n",
    "        print(f'''Skipping for {attack}''')\n",
    "        continue\n",
    "        \n",
    "    # Initialise classifier\n",
    "    adv_linear_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                         num_labels=len(CLASS_SUBSET))\n",
    "    adv_linear_classifier = adv_linear_classifier.cuda()\n",
    "    \n",
    "    # Metric logger path\n",
    "#     LOG_PATH = Path(LOG_BASE_PATH, 'adv_classifier', version, attack)\n",
    "#     if not os.path.isdir(LOG_PATH):\n",
    "#         os.makedirs(LOG_PATH)\n",
    "    \n",
    "    # train\n",
    "    pstr = \"#\"*50 + f''' Training classifier for {attack} ''' + \"#\"*50\n",
    "    print(len(pstr)*\"#\")\n",
    "    print(pstr)\n",
    "    print(len(pstr)*\"#\")\n",
    "    \n",
    "    loggers = train(model, \n",
    "                    adv_linear_classifier, \n",
    "                    loaders[\"train\"], \n",
    "                    loaders[\"validation\"], \n",
    "                    log_dir=Path(LINEAR_CLASSIFIER_MODELS_PATH, version, attack),\n",
    "                    tensor_dir=None, \n",
    "                    optimizer=None, \n",
    "                    adversarial_attack=None,\n",
    "                    criterion=nn.CrossEntropyLoss(),\n",
    "                    epochs=1, \n",
    "                    val_freq=1, \n",
    "                    batch_size=BATCH_SIZE,  \n",
    "                    lr=0.001, \n",
    "                    to_restore = {\"epoch\": 0, \"best_acc\": 0.}, \n",
    "                    n=4, \n",
    "                    avgpool_patchtokens=False, \n",
    "                    show_image=False)\n",
    "    \n",
    "    # Save adversarial Classifier\n",
    "    save_path = Path(LINEAR_CLASSIFIER_MODELS_PATH, version, attack)\n",
    "    \n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    save_file_log = f\"log_{attack}.pt\"\n",
    "    torch.save(loggers, Path(save_path, save_file_log))\n",
    "    \n",
    "    print(f'''Finished Training classifier on {attack}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on all adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = [x for x in loader_dict.keys()]\n",
    "\n",
    "VERSION_EVAL_PATH = Path(LINEAR_CLASSIFIER_EVAL_PATH, version)\n",
    "VERSION_EVAL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "for attack in attacks:\n",
    "    if attack == \"ori\":\n",
    "        pstr = \"#\"*30 + f''' evaluating adv_classifier trained on {attack} ''' + \"#\"*30\n",
    "        print(len(pstr)*\"#\")\n",
    "        print(pstr)\n",
    "        print(len(pstr)*\"#\")\n",
    "        adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                 num_labels=len(CLASS_SUBSET))\n",
    "        adv_classifier.to(DEVICE)\n",
    "        \n",
    "\n",
    "        # load from checkpoint\n",
    "        log_dir = Path(LINEAR_CLASSIFIER_MODELS_PATH,version, attack)\n",
    "        to_restore={'epoch': 1}\n",
    "        utils.restart_from_checkpoint(\n",
    "            Path(log_dir, \"checkpoint.pth.tar\"),\n",
    "            run_variables=to_restore,\n",
    "            state_dict=adv_classifier\n",
    "        )\n",
    "\n",
    "        for applied_attack in attacks:\n",
    "            \n",
    "            print(\">\"*5 + f''' {applied_attack} dataset: {len(loader_dict[applied_attack][\"validation\"].dataset)} ''')\n",
    "            logger_dict, logger = validate_network(model, \n",
    "                                                   adv_classifier, \n",
    "                                                   loader_dict[applied_attack][\"validation\"], \n",
    "                                                   criterion=nn.CrossEntropyLoss(),\n",
    "                                                   tensor_dir=None, \n",
    "                                                   adversarial_attack=None, \n",
    "                                                   n=4, \n",
    "                                                   avgpool_patchtokens=False, \n",
    "                                                   path_predictions=Path(VERSION_EVAL_PATH, 'c_'+attack+'_d_'+applied_attack+'.csv'),\n",
    "                                                   log_interval = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on full pipeline with post-hoc as multiplexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearClassifier(\n",
       "  (linear): Linear(in_features=1536, out_features=25, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load clean_classifier\n",
    "clean_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                    num_labels=len(CLASS_SUBSET))\n",
    "clean_classifier.to(DEVICE)\n",
    "\n",
    "clean_classifier.load_state_dict(torch.load(\"/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes\" + \"/\" + \"clean.pt\"))\n",
    "clean_classifier.cuda()\n",
    "\n",
    "# log_dir = Path(LINEAR_CLASSIFIER_MODELS_PATH, version, \"ori\")\n",
    "# to_restore={'epoch': 1}\n",
    "# utils.restart_from_checkpoint(\n",
    "#     Path(log_dir, \"checkpoint.pth.tar\"),\n",
    "#     run_variables=to_restore,\n",
    "#     state_dict=clean_classifier\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found checkpoint at /cluster/scratch/mmathys/dl_data/posthoc_models/cw/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/mmathys/dl_data/posthoc_models/cw/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      "Found checkpoint at /cluster/scratch/mmathys/dl_data/linear_classifier_models/last_hope/cw/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/mmathys/dl_data/linear_classifier_models/last_hope/cw/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      "##################################################################################################################\n",
      "############################## Validating Posthoc: cw and adv_classifier: cw on ori ##############################\n",
      "##################################################################################################################\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'true_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-895e5b955bd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             log_dict, logger = validate_multihead_network(model, \n\u001b[0m\u001b[1;32m     40\u001b[0m                                                           \u001b[0mposthoc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                                           \u001b[0madv_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AADefDINO/src/model/multihead_model.py\u001b[0m in \u001b[0;36mvalidate_multihead_network\u001b[0;34m(model, posthoc, adv_classifier, clean_classifier, validation_loader, tensor_dir, adversarial_attack, n, avgpool, path_predictions)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_predictions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mtrue_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0madversarial_attack\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'true_labels' is not defined"
     ]
    }
   ],
   "source": [
    "# Load posthoc\n",
    "attacks=[\"cw\", \"fgsm_06\", \"pgd_03\"]\n",
    "\n",
    "# Perform validation on clean dataset\n",
    "for post_model in attacks:\n",
    "    \n",
    "    log_dir = Path(POSTHOC_MODELS_PATH, post_model)\n",
    "    \n",
    "    posthoc = LinearBC(1536)\n",
    "    posthoc.cuda()\n",
    "    to_restore={'epoch':3}\n",
    "    utils.restart_from_checkpoint(\n",
    "        Path(log_dir, \"checkpoint.pth.tar\"),\n",
    "        run_variables=to_restore,\n",
    "        state_dict=posthoc\n",
    "    )\n",
    "    \n",
    "    for adv_model in attacks:\n",
    "        adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                          num_labels=len(CLASS_SUBSET))\n",
    "        adv_classifier.to(DEVICE)\n",
    "        \n",
    "        log_dir = Path(LINEAR_CLASSIFIER_MODELS_PATH, version, adv_model)\n",
    "        to_restore={'epoch': 1}\n",
    "        \n",
    "        utils.restart_from_checkpoint(\n",
    "            Path(log_dir, \"checkpoint.pth.tar\"),\n",
    "            run_variables=to_restore,\n",
    "            state_dict=adv_classifier\n",
    "        )\n",
    "        \n",
    "        for attack, loaders in loader_dict.items():\n",
    "            \n",
    "            pstr = \"#\"*30 + f''' Validating Posthoc: {post_model} and adv_classifier: {adv_model} on {attack} ''' + \"#\"*30\n",
    "            print(len(pstr)*\"#\")\n",
    "            print(pstr)\n",
    "            print(len(pstr)*\"#\")\n",
    "            \n",
    "            log_dict, logger = validate_multihead_network(model, \n",
    "                                                          posthoc,\n",
    "                                                          adv_classifier,\n",
    "                                                          clean_classifier,\n",
    "                                                          loader_dict[attack][\"validation\"], \n",
    "                                                          tensor_dir=None, \n",
    "                                                          adversarial_attack=None, \n",
    "                                                          n=4, \n",
    "                                                          avgpool=False,\n",
    "                                                          path_predictions=Path(MULTIHEAD_EVAL_PATH, 'labels_p_'+ post_model +'_c_'+adv_model+'_d_'+attack+'.csv')\n",
    "                                                         )\n",
    "            \n",
    "            # Save adversarial Classifier\n",
    "            save_path = Path(MULTIHEAD_EVAL_PATH, version)\n",
    "            save_path.mkdir(parents=True, exist_ok=True)\n",
    "            save_file_log = f\"log_p_{post_model}_c_{adv_model}_d_{attack}.pt\"\n",
    "            torch.save(logger, Path(save_path, save_file_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(POSTHOC_MODELS_PATH, 'post_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on newly generated attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = [x for x in loader_dict.keys()]\n",
    "\n",
    "for attack in attacks:\n",
    "    \n",
    "    pstr = \"#\"*30 + f''' evaluating adv_classifier trained on {attack} ''' + \"#\"*30\n",
    "    print(len(pstr)*\"#\")\n",
    "    print(pstr)\n",
    "    print(len(pstr)*\"#\")\n",
    "    \n",
    "    adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                             num_labels=len(CLASS_SUBSET))\n",
    "    adv_classifier.to(DEVICE)\n",
    "    \n",
    "    log_dir = Path(LINEAR_CLASSIFIER_MODELS_PATH, attack)\n",
    "    to_restore={'epoch': 1}\n",
    "    utils.restart_from_checkpoint(\n",
    "        Path(log_dir, \"checkpoint.pth.tar\"),\n",
    "        run_variables=to_restore,\n",
    "        state_dict=adv_classifier\n",
    "    )\n",
    "    \n",
    "    vits = ViTWrapper(model, adv_classifier, transform=None)\n",
    "\n",
    "    for applied_attack in attacks:\n",
    "        \n",
    "        if applied_attack == \"pgd\":\n",
    "            ev_attack = PGD(vits, eps=0.3, alpha=6/255, steps=15)\n",
    "        elif applied_attack == \"cw\":\n",
    "            ev_attack = CW(vits, c=10, lr=0.003, steps=30)\n",
    "        elif applied_attack == \"fgsm\":\n",
    "            ev_attack = FGSM(vits, eps=0.03)\n",
    "        else:\n",
    "            continue\n",
    " \n",
    "        print(\">\"*5 + f''' applying attack: {ev_attack} ''')\n",
    "        logger_dict, logger = validate_network(model, \n",
    "                                               adv_classifier, \n",
    "                                               loader_dict[\"ori\"][\"validation\"],\n",
    "                                               criterion=nn.CrossEntropyLoss(),\n",
    "                                               tensor_dir=None, \n",
    "                                               adversarial_attack=ev_attack,\n",
    "                                               n=4, \n",
    "                                               avgpool_patchtokens=False, \n",
    "                                               path_predictions=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black Box on Multihead model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
