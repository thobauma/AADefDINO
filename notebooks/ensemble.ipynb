{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "# from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino, ViTWrapper\n",
    "from src.model.data import *\n",
    "from src.model.train import *\n",
    "from src.model.multihead_model import *\n",
    "\n",
    "from torchattacks import *\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "MAX_PATH = Path('/','cluster', 'scratch', 'mmathys', 'dl_data')\n",
    "\n",
    "LOG_BASE_PATH = Path(MAX_PATH, 'logs')\n",
    "\n",
    "# DamageNet\n",
    "DN_PATH = Path(DATA_PATH, 'damageNet')\n",
    "DN_LABEL_PATH = Path(DN_PATH, 'val_damagenet.txt')\n",
    "DN_IMAGES_PATH = Path(DN_PATH, 'images')\n",
    "\n",
    "# Image Net\n",
    "ORI_PATH = Path(DATA_PATH, 'ori_data')\n",
    "CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "\n",
    "VAL_PATH = Path(ORI_PATH, 'validation')\n",
    "VAL_IMAGES_PATH = Path(VAL_PATH,'images')\n",
    "VAL_LABEL_PATH = Path(VAL_PATH, 'correct_labels.txt')\n",
    "\n",
    "TRAIN_PATH = Path(ORI_PATH, 'train')\n",
    "TRAIN_IMAGES_PATH = Path(TRAIN_PATH,'images')\n",
    "TRAIN_LABEL_PATH = Path(TRAIN_PATH, 'correct_labels.txt')\n",
    "\n",
    "# Adversarial Data\n",
    "# PGD\n",
    "PGD_TRAIN_PATH = Path(MAX_PATH, 'adversarial_data', 'pgd_06', 'train')\n",
    "PGD_TRAIN_IMAGES_PATH = Path(PGD_TRAIN_PATH,'images')\n",
    "PGD_TRAIN_LABEL_PATH = Path(PGD_TRAIN_PATH, 'labels.txt')\n",
    "\n",
    "PGD_VAL_PATH = Path(MAX_PATH, 'adversarial_data', 'pgd_06', 'validation')\n",
    "PGD_VAL_IMAGES_PATH = Path(PGD_VAL_PATH,'images')\n",
    "PGD_VAL_LABEL_PATH = Path(PGD_VAL_PATH, 'labels.txt')\n",
    "\n",
    "# CW\n",
    "CW_TRAIN_PATH = Path(MAX_PATH, 'adversarial_data', 'cw', 'train')\n",
    "CW_TRAIN_IMAGES_PATH = Path(CW_TRAIN_PATH,'images')\n",
    "CW_TRAIN_LABEL_PATH = Path(CW_TRAIN_PATH, 'labels.txt')\n",
    "\n",
    "CW_VAL_PATH = Path(MAX_PATH, 'adversarial_data', 'cw', 'validation')\n",
    "CW_VAL_IMAGES_PATH = Path(CW_VAL_PATH,'images')\n",
    "CW_VAL_LABEL_PATH = Path(CW_VAL_PATH, 'labels.txt')\n",
    "\n",
    "# FGSM\n",
    "FGSM_TRAIN_PATH = Path(MAX_PATH, 'adversarial_data', 'fgsm_06', 'train')\n",
    "FGSM_TRAIN_IMAGES_PATH = Path(FGSM_TRAIN_PATH,'images')\n",
    "FGSM_TRAIN_LABEL_PATH = Path(FGSM_TRAIN_PATH, 'labels.txt')\n",
    "\n",
    "FGSM_VAL_PATH = Path(MAX_PATH, 'adversarial_data', 'fgsm_06', 'validation')\n",
    "FGSM_VAL_IMAGES_PATH = Path(FGSM_VAL_PATH,'images')\n",
    "FGSM_VAL_LABEL_PATH = Path(FGSM_VAL_PATH, 'labels.txt')\n",
    "\n",
    "\n",
    "# TB LOG\n",
    "TB_LOGS_BASE_PATH = Path(LOG_BASE_PATH, 'tb_logs')\n",
    "\n",
    "\n",
    "# Model save path\n",
    "ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH = Path(MAX_PATH, 'adversarial_data', 'adv_classifiers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If CLASS_SUBSET is specified, INDEX_SUBSET will be ignored. Set CLASS_SUBSET=None if you want to use indexes.\n",
    "# INDEX_SUBSET = get_random_indexes(number_of_images = 50000, n_samples=1000)\n",
    "# CLASS_SUBSET = get_random_classes(number_of_classes = 25, min_rand_class = 1, max_rand_class = 1001)\n",
    "\n",
    "\n",
    "CLASS_SUBSET = np.load(CLASS_SUBSET_PATH)\n",
    "\n",
    "# CLASS_SUBSET = CLASS_SUBSET[:25] \n",
    "\n",
    "NUM_WORKERS= 0\n",
    "PIN_MEMORY=True\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import DINO\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights.\n"
     ]
    }
   ],
   "source": [
    "model, linear_classifier = get_dino()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to set the correct transformation\n",
    "# encoder\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit([i for i in CLASS_SUBSET])\n",
    "\n",
    "\n",
    "# PGD\n",
    "pgd_train_dataset = AdvTrainingImageDataset(PGD_TRAIN_IMAGES_PATH, \n",
    "                                            PGD_TRAIN_LABEL_PATH, \n",
    "                                            ORIGINAL_TRANSFORM, \n",
    "                                            CLASS_SUBSET, \n",
    "                                            index_subset=None,\n",
    "                                            label_encoder=label_encoder\n",
    "                                           )\n",
    "\n",
    "pgd_val_dataset = AdvTrainingImageDataset(PGD_VAL_IMAGES_PATH, \n",
    "                                          PGD_VAL_LABEL_PATH, \n",
    "                                          ORIGINAL_TRANSFORM, \n",
    "                                          CLASS_SUBSET, \n",
    "                                          index_subset=None,\n",
    "                                          label_encoder=label_encoder\n",
    "                                         )\n",
    "\n",
    "\n",
    "pgd_train_loader = DataLoader(pgd_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "pgd_val_loader = DataLoader(pgd_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY, \n",
    "                            shuffle=False)\n",
    "\n",
    "# CW\n",
    "cw_train_dataset = AdvTrainingImageDataset(CW_TRAIN_IMAGES_PATH, \n",
    "                                            CW_TRAIN_LABEL_PATH, \n",
    "                                            ORIGINAL_TRANSFORM, \n",
    "                                            CLASS_SUBSET, \n",
    "                                            index_subset=None,\n",
    "                                           label_encoder=label_encoder\n",
    "                                          )\n",
    "\n",
    "cw_val_dataset = AdvTrainingImageDataset(CW_VAL_IMAGES_PATH, \n",
    "                                          CW_VAL_LABEL_PATH, \n",
    "                                          ORIGINAL_TRANSFORM, \n",
    "                                          CLASS_SUBSET, \n",
    "                                          index_subset=None,\n",
    "                                         label_encoder=label_encoder\n",
    "                                        )\n",
    "\n",
    "cw_train_loader = DataLoader(cw_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "cw_val_loader = DataLoader(cw_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY, \n",
    "                            shuffle=False)\n",
    "\n",
    "# FGSM\n",
    "fgsm_train_dataset = AdvTrainingImageDataset(FGSM_TRAIN_IMAGES_PATH, \n",
    "                                            FGSM_TRAIN_LABEL_PATH, \n",
    "                                            ORIGINAL_TRANSFORM, \n",
    "                                            CLASS_SUBSET, \n",
    "                                            index_subset=None,\n",
    "                                            label_encoder=label_encoder\n",
    "                                            )\n",
    "\n",
    "fgsm_val_dataset = AdvTrainingImageDataset(FGSM_VAL_IMAGES_PATH, \n",
    "                                          FGSM_VAL_LABEL_PATH, \n",
    "                                          ORIGINAL_TRANSFORM, \n",
    "                                          CLASS_SUBSET, \n",
    "                                          index_subset=None,\n",
    "                                          label_encoder=label_encoder\n",
    "                                          )\n",
    "\n",
    "fgsm_train_loader = DataLoader(fgsm_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "fgsm_val_loader = DataLoader(fgsm_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY, \n",
    "                            shuffle=False)\n",
    "\n",
    "# Clean\n",
    "clean_train_dataset = ImageDataset(TRAIN_IMAGES_PATH, \n",
    "                                  TRAIN_LABEL_PATH, \n",
    "                                  ORIGINAL_TRANSFORM,\n",
    "                                  CLASS_SUBSET, \n",
    "                                  index_subset=None, \n",
    "                                  label_encoder=label_encoder)\n",
    "\n",
    "clean_val_dataset = ImageDataset(VAL_IMAGES_PATH, \n",
    "                                  VAL_LABEL_PATH, \n",
    "                                  ORIGINAL_TRANSFORM,\n",
    "                                  CLASS_SUBSET, \n",
    "                                  index_subset=None, \n",
    "                                  label_encoder=label_encoder)\n",
    "\n",
    "clean_train_loader = DataLoader(clean_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "clean_val_loader = DataLoader(clean_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY,\n",
    "                            shuffle=False)\n",
    "\n",
    "\n",
    "loader_dict = {\n",
    "    \"pgd\" : {\n",
    "        \"train\" : pgd_train_loader,\n",
    "        \"validation\" : pgd_val_loader,\n",
    "    },\n",
    "    \"cw\" : {\n",
    "        \"train\" : cw_train_loader,\n",
    "        \"validation\" : cw_val_loader,\n",
    "    }, \n",
    "    \"fgsm\" : {\n",
    "        \"train\" : fgsm_train_loader,\n",
    "        \"validation\" : fgsm_val_loader,\n",
    "    },\n",
    "    \"clean\" : {\n",
    "        \"train\" : clean_train_loader,\n",
    "        \"validation\" : clean_val_loader,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2b5281a7fd30>"
      ]
     },
     "execution_count": 8,

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_dict[\"pgd\"][\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '25_classes_full_v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(dim, num_labels)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # linear layer\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Binary Classifier\n",
    "class LinearBC(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(LinearBC,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train various classifiers on all adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################################################################################################\n",
      "##################################################Training classifier for pgd##################################################\n",
      "###############################################################################################################################\n",
      "Found checkpoint at /cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/pgd/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/pgd/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      "=> loaded 'optimizer' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/pgd/checkpoint.pth.tar'\n",
      "=> loaded 'scheduler' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/pgd/checkpoint.pth.tar'\n",
      "Epoch: [1]  [ 0/82]  eta: 0:00:30  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.377378  data: 0.339144  max mem: 211\n",
      "Epoch: [1]  [20/82]  eta: 0:00:22  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.354450  data: 0.316195  max mem: 211\n",
      "Epoch: [1]  [40/82]  eta: 0:00:14  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.352559  data: 0.314284  max mem: 211\n",
      "Epoch: [1]  [60/82]  eta: 0:00:07  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.350409  data: 0.312152  max mem: 211\n",
      "Epoch: [1]  [80/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.350542  data: 0.312306  max mem: 211\n",
      "Epoch: [1]  [81/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.337431  data: 0.300519  max mem: 211\n",
      "Epoch: [1] Total time: 0:00:28 (0.349175 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:01  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.358613  data: 0.320376  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.277177  data: 0.245552  max mem: 211\n",
      "Test: Total time: 0:00:01 (0.277506 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 1 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 1 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [2]  [ 0/82]  eta: 0:00:19  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236319  data: 0.197987  max mem: 211\n",
      "Epoch: [2]  [20/82]  eta: 0:00:14  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236264  data: 0.197998  max mem: 211\n",
      "Epoch: [2]  [40/82]  eta: 0:00:09  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.235630  data: 0.197380  max mem: 211\n",
      "Epoch: [2]  [60/82]  eta: 0:00:05  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236204  data: 0.197931  max mem: 211\n",
      "Epoch: [2]  [80/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236336  data: 0.198089  max mem: 211\n",
      "Epoch: [2]  [81/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.227662  data: 0.190698  max mem: 211\n",
      "Epoch: [2] Total time: 0:00:19 (0.234068 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.242759  data: 0.204627  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.187176  data: 0.155723  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.187511 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 2 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 2 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [3]  [ 0/82]  eta: 0:00:19  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.243459  data: 0.205405  max mem: 211\n",
      "Epoch: [3]  [20/82]  eta: 0:00:14  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.236944  data: 0.198821  max mem: 211\n",
      "Epoch: [3]  [40/82]  eta: 0:00:09  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.235865  data: 0.197741  max mem: 211\n",
      "Epoch: [3]  [60/82]  eta: 0:00:05  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.237145  data: 0.198999  max mem: 211\n",
      "Epoch: [3]  [80/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.236505  data: 0.198364  max mem: 211\n",
      "Epoch: [3]  [81/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.227764  data: 0.190937  max mem: 211\n",
      "Epoch: [3] Total time: 0:00:19 (0.234638 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.243639  data: 0.205340  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.187040  data: 0.155561  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.187371 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 3 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 3 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [4]  [ 0/82]  eta: 0:00:19  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.237304  data: 0.199155  max mem: 211\n",
      "Epoch: [4]  [20/82]  eta: 0:00:14  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236897  data: 0.198766  max mem: 211\n",
      "Epoch: [4]  [40/82]  eta: 0:00:09  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236708  data: 0.198561  max mem: 211\n",
      "Epoch: [4]  [60/82]  eta: 0:00:05  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.235746  data: 0.197592  max mem: 211\n",
      "Epoch: [4]  [80/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236444  data: 0.198347  max mem: 211\n",
      "Epoch: [4]  [81/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.227742  data: 0.190964  max mem: 211\n",
      "Epoch: [4] Total time: 0:00:19 (0.234395 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.241399  data: 0.203145  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.187272  data: 0.155722  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.187605 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 4 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 4 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Training of the supervised linear classifier on frozen features completed.\n",
      "Top-1 test accuracy: 100.0\n",
      "Finished Training, saving model to /cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes_full_v1/pgd.pt and log to /cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes_full_v1/pgd.pt\n",
      "##############################################################################################################################\n",
      "##################################################Training classifier for cw##################################################\n",
      "##############################################################################################################################\n",
      "Found checkpoint at /cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/cw/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/cw/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      "=> loaded 'optimizer' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/cw/checkpoint.pth.tar'\n",
      "=> loaded 'scheduler' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/cw/checkpoint.pth.tar'\n",
      "Epoch: [1]  [ 0/82]  eta: 0:00:30  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.366465  data: 0.328399  max mem: 211\n",
      "Epoch: [1]  [20/82]  eta: 0:00:23  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.385364  data: 0.347210  max mem: 211\n",
      "Epoch: [1]  [40/82]  eta: 0:00:15  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.375541  data: 0.337434  max mem: 211\n",
      "Epoch: [1]  [60/82]  eta: 0:00:08  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.343131  data: 0.305093  max mem: 211\n",
      "Epoch: [1]  [80/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.338256  data: 0.300229  max mem: 211\n",
      "Epoch: [1]  [81/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.325455  data: 0.288810  max mem: 211\n",
      "Epoch: [1] Total time: 0:00:29 (0.357354 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:01  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.347097  data: 0.309057  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.348742  data: 0.317586  max mem: 211\n",
      "Test: Total time: 0:00:01 (0.349043 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 1 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 1 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [2]  [ 0/82]  eta: 0:00:19  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.233743  data: 0.195713  max mem: 211\n",
      "Epoch: [2]  [20/82]  eta: 0:00:14  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.226078  data: 0.188059  max mem: 211\n",
      "Epoch: [2]  [40/82]  eta: 0:00:09  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.233324  data: 0.195268  max mem: 211\n",
      "Epoch: [2]  [60/82]  eta: 0:00:05  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.238494  data: 0.200393  max mem: 211\n",
      "Epoch: [2]  [80/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.237795  data: 0.199652  max mem: 211\n",
      "Epoch: [2]  [81/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.229055  data: 0.192177  max mem: 211\n",
      "Epoch: [2] Total time: 0:00:19 (0.231958 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:01  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.253195  data: 0.215042  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.191940  data: 0.160324  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.192286 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 2 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 2 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [3]  [ 0/82]  eta: 0:00:19  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.242012  data: 0.203958  max mem: 211\n",
      "Epoch: [3]  [20/82]  eta: 0:00:14  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.236564  data: 0.198460  max mem: 211\n",
      "Epoch: [3]  [40/82]  eta: 0:00:09  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.234945  data: 0.196857  max mem: 211\n",
      "Epoch: [3]  [60/82]  eta: 0:00:05  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.235822  data: 0.197740  max mem: 211\n",
      "Epoch: [3]  [80/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.235935  data: 0.197810  max mem: 211\n",
      "Epoch: [3]  [81/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.227086  data: 0.190303  max mem: 211\n",
      "Epoch: [3] Total time: 0:00:19 (0.233841 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.242429  data: 0.204274  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.186432  data: 0.155138  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.186758 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 3 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 3 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [4]  [ 0/82]  eta: 0:00:19  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.241511  data: 0.203505  max mem: 211\n",
      "Epoch: [4]  [20/82]  eta: 0:00:14  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.234986  data: 0.196831  max mem: 211\n",
      "Epoch: [4]  [40/82]  eta: 0:00:09  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.235451  data: 0.197245  max mem: 211\n",
      "Epoch: [4]  [60/82]  eta: 0:00:05  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236556  data: 0.198440  max mem: 211\n",
      "Epoch: [4]  [80/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.235119  data: 0.197026  max mem: 211\n",
      "Epoch: [4]  [81/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.226421  data: 0.189643  max mem: 211\n",
      "Epoch: [4] Total time: 0:00:19 (0.233559 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.241777  data: 0.203600  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.187539  data: 0.155816  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.187884 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 4 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 4 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Training of the supervised linear classifier on frozen features completed.\n",
      "Top-1 test accuracy: 100.0\n",
      "Finished Training, saving model to /cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes_full_v1/cw.pt and log to /cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes_full_v1/cw.pt\n",
      "################################################################################################################################\n",
      "##################################################Training classifier for fgsm##################################################\n",
      "################################################################################################################################\n",
      "Found checkpoint at /cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/fgsm/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/fgsm/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      "=> loaded 'optimizer' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/fgsm/checkpoint.pth.tar'\n",
      "=> loaded 'scheduler' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/fgsm/checkpoint.pth.tar'\n",
      "Epoch: [1]  [ 0/82]  eta: 0:00:30  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.369044  data: 0.330962  max mem: 211\n",
      "Epoch: [1]  [20/82]  eta: 0:00:24  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.391692  data: 0.353577  max mem: 211\n",
      "Epoch: [1]  [40/82]  eta: 0:00:15  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.354034  data: 0.315903  max mem: 211\n",
      "Epoch: [1]  [60/82]  eta: 0:00:08  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.377947  data: 0.339829  max mem: 211\n",
      "Epoch: [1]  [80/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.354370  data: 0.316231  max mem: 211\n",
      "Epoch: [1]  [81/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.341205  data: 0.304371  max mem: 211\n",
      "Epoch: [1] Total time: 0:00:30 (0.366204 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:01  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.353285  data: 0.315068  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.277448  data: 0.245949  max mem: 211\n",
      "Test: Total time: 0:00:01 (0.277772 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 1 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 1 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [2]  [ 0/82]  eta: 0:00:19  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.243253  data: 0.205139  max mem: 211\n",
      "Epoch: [2]  [20/82]  eta: 0:00:14  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236905  data: 0.198773  max mem: 211\n",
      "Epoch: [2]  [40/82]  eta: 0:00:09  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.237417  data: 0.199229  max mem: 211\n",
      "Epoch: [2]  [60/82]  eta: 0:00:05  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.237800  data: 0.199664  max mem: 211\n",
      "Epoch: [2]  [80/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.238117  data: 0.199989  max mem: 211\n",
      "Epoch: [2]  [81/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.229282  data: 0.192448  max mem: 211\n",
      "Epoch: [2] Total time: 0:00:19 (0.235574 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.247326  data: 0.209194  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.190469  data: 0.158956  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.190884 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 2 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 2 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [3]  [ 0/82]  eta: 0:00:19  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.242464  data: 0.204284  max mem: 211\n",
      "Epoch: [3]  [20/82]  eta: 0:00:14  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.237374  data: 0.199240  max mem: 211\n",
      "Epoch: [3]  [40/82]  eta: 0:00:10  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.238866  data: 0.200749  max mem: 211\n",
      "Epoch: [3]  [60/82]  eta: 0:00:05  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.237324  data: 0.199200  max mem: 211\n",
      "Epoch: [3]  [80/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.238038  data: 0.199859  max mem: 211\n",
      "Epoch: [3]  [81/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.229211  data: 0.192389  max mem: 211\n",
      "Epoch: [3] Total time: 0:00:19 (0.235890 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.243826  data: 0.205690  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.187973  data: 0.156508  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.188302 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 3 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 3 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [4]  [ 0/82]  eta: 0:00:19  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.243892  data: 0.205813  max mem: 211\n",
      "Epoch: [4]  [20/82]  eta: 0:00:14  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.237372  data: 0.199194  max mem: 211\n",
      "Epoch: [4]  [40/82]  eta: 0:00:09  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.237349  data: 0.199198  max mem: 211\n",
      "Epoch: [4]  [60/82]  eta: 0:00:05  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.237702  data: 0.199592  max mem: 211\n",
      "Epoch: [4]  [80/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.237794  data: 0.199693  max mem: 211\n",
      "Epoch: [4]  [81/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.229052  data: 0.192263  max mem: 211\n",
      "Epoch: [4] Total time: 0:00:19 (0.235571 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.243511  data: 0.205322  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.188655  data: 0.157124  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.188982 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 4 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 4 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Training of the supervised linear classifier on frozen features completed.\n",
      "Top-1 test accuracy: 100.0\n",
      "Finished Training, saving model to /cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes_full_v1/fgsm.pt and log to /cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes_full_v1/fgsm.pt\n",
      "#################################################################################################################################\n",
      "##################################################Training classifier for clean##################################################\n",
      "#################################################################################################################################\n",
      "Found checkpoint at /cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/clean/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/clean/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      "=> loaded 'optimizer' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/clean/checkpoint.pth.tar'\n",
      "=> loaded 'scheduler' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/clean/checkpoint.pth.tar'\n",
      "Epoch: [1]  [   0/2012]  eta: 0:16:45  lr: 0.000000  loss: 0.005330 (0.005330)  time: 0.499512  data: 0.461465  max mem: 211\n",
      "Epoch: [1]  [  20/2012]  eta: 0:17:38  lr: 0.000000  loss: 0.011244 (0.051113)  time: 0.533071  data: 0.494918  max mem: 211\n",
      "Epoch: [1]  [  40/2012]  eta: 0:16:37  lr: 0.000000  loss: 0.053833 (0.071367)  time: 0.479311  data: 0.441185  max mem: 211\n",
      "Epoch: [1]  [  60/2012]  eta: 0:16:12  lr: 0.000000  loss: 0.027650 (0.078470)  time: 0.482569  data: 0.444440  max mem: 211\n",
      "Epoch: [1]  [  80/2012]  eta: 0:15:51  lr: 0.000000  loss: 0.070722 (0.084037)  time: 0.475324  data: 0.437212  max mem: 211\n",
      "Epoch: [1]  [ 100/2012]  eta: 0:15:51  lr: 0.000000  loss: 0.019478 (0.075089)  time: 0.516705  data: 0.478581  max mem: 211\n",
      "Epoch: [1]  [ 120/2012]  eta: 0:15:33  lr: 0.000000  loss: 0.034886 (0.075078)  time: 0.474049  data: 0.435946  max mem: 211\n",
      "Epoch: [1]  [ 140/2012]  eta: 0:15:44  lr: 0.000000  loss: 0.051577 (0.077865)  time: 0.569886  data: 0.531821  max mem: 211\n",
      "Epoch: [1]  [ 160/2012]  eta: 0:15:23  lr: 0.000000  loss: 0.041574 (0.075853)  time: 0.457652  data: 0.419659  max mem: 211\n",
      "Epoch: [1]  [ 180/2012]  eta: 0:15:12  lr: 0.000000  loss: 0.031845 (0.077014)  time: 0.496048  data: 0.457939  max mem: 211\n",
      "Epoch: [1]  [ 200/2012]  eta: 0:15:09  lr: 0.000000  loss: 0.013812 (0.074935)  time: 0.534538  data: 0.496385  max mem: 211\n",
      "Epoch: [1]  [ 220/2012]  eta: 0:14:54  lr: 0.000000  loss: 0.052742 (0.076821)  time: 0.474526  data: 0.436374  max mem: 211\n",
      "Epoch: [1]  [ 240/2012]  eta: 0:14:44  lr: 0.000000  loss: 0.027114 (0.075238)  time: 0.495445  data: 0.457301  max mem: 211\n",
      "Epoch: [1]  [ 260/2012]  eta: 0:14:33  lr: 0.000000  loss: 0.031278 (0.073430)  time: 0.492553  data: 0.454438  max mem: 211\n",
      "Epoch: [1]  [ 280/2012]  eta: 0:14:23  lr: 0.000000  loss: 0.049985 (0.075734)  time: 0.495358  data: 0.457229  max mem: 211\n",
      "Epoch: [1]  [ 300/2012]  eta: 0:14:14  lr: 0.000000  loss: 0.082680 (0.076830)  time: 0.507792  data: 0.469681  max mem: 211\n",
      "Epoch: [1]  [ 320/2012]  eta: 0:14:02  lr: 0.000000  loss: 0.028380 (0.075247)  time: 0.483601  data: 0.445479  max mem: 211\n",
      "Epoch: [1]  [ 340/2012]  eta: 0:13:52  lr: 0.000000  loss: 0.038142 (0.075684)  time: 0.495681  data: 0.457569  max mem: 211\n",
      "Epoch: [1]  [ 360/2012]  eta: 0:13:45  lr: 0.000000  loss: 0.032275 (0.076189)  time: 0.526778  data: 0.488626  max mem: 211\n",
      "Epoch: [1]  [ 380/2012]  eta: 0:13:34  lr: 0.000000  loss: 0.018004 (0.075316)  time: 0.496122  data: 0.458001  max mem: 211\n",
      "Epoch: [1]  [ 400/2012]  eta: 0:13:25  lr: 0.000000  loss: 0.036643 (0.076430)  time: 0.500914  data: 0.462815  max mem: 211\n",
      "Epoch: [1]  [ 420/2012]  eta: 0:13:13  lr: 0.000000  loss: 0.011514 (0.074363)  time: 0.476698  data: 0.438552  max mem: 211\n",
      "Epoch: [1]  [ 440/2012]  eta: 0:13:01  lr: 0.000000  loss: 0.064203 (0.074604)  time: 0.470230  data: 0.432101  max mem: 211\n",
      "Epoch: [1]  [ 460/2012]  eta: 0:12:51  lr: 0.000000  loss: 0.017533 (0.073647)  time: 0.490987  data: 0.452852  max mem: 211\n",
      "Epoch: [1]  [ 480/2012]  eta: 0:12:39  lr: 0.000000  loss: 0.046839 (0.074022)  time: 0.467338  data: 0.429224  max mem: 211\n",
      "Epoch: [1]  [ 500/2012]  eta: 0:12:28  lr: 0.000000  loss: 0.084284 (0.076365)  time: 0.476285  data: 0.438157  max mem: 211\n",
      "Epoch: [1]  [ 520/2012]  eta: 0:12:16  lr: 0.000000  loss: 0.022889 (0.076581)  time: 0.470263  data: 0.432125  max mem: 211\n",
      "Epoch: [1]  [ 540/2012]  eta: 0:12:06  lr: 0.000000  loss: 0.049436 (0.076635)  time: 0.478737  data: 0.440596  max mem: 211\n",
      "Epoch: [1]  [ 560/2012]  eta: 0:11:55  lr: 0.000000  loss: 0.048562 (0.076729)  time: 0.474662  data: 0.436527  max mem: 211\n",
      "Epoch: [1]  [ 580/2012]  eta: 0:11:44  lr: 0.000000  loss: 0.043978 (0.077596)  time: 0.473206  data: 0.435080  max mem: 211\n",
      "Epoch: [1]  [ 600/2012]  eta: 0:11:32  lr: 0.000000  loss: 0.027769 (0.076507)  time: 0.453324  data: 0.415262  max mem: 211\n",
      "Epoch: [1]  [ 620/2012]  eta: 0:11:22  lr: 0.000000  loss: 0.014957 (0.076705)  time: 0.470661  data: 0.432603  max mem: 211\n",
      "Epoch: [1]  [ 640/2012]  eta: 0:11:11  lr: 0.000000  loss: 0.012808 (0.076122)  time: 0.468050  data: 0.430005  max mem: 211\n",
      "Epoch: [1]  [ 660/2012]  eta: 0:11:00  lr: 0.000000  loss: 0.020078 (0.074974)  time: 0.468444  data: 0.430297  max mem: 211\n",
      "Epoch: [1]  [ 680/2012]  eta: 0:10:50  lr: 0.000000  loss: 0.072095 (0.075940)  time: 0.464289  data: 0.426146  max mem: 211\n",
      "Epoch: [1]  [ 700/2012]  eta: 0:10:39  lr: 0.000000  loss: 0.091372 (0.077354)  time: 0.464567  data: 0.426456  max mem: 211\n",
      "Epoch: [1]  [ 720/2012]  eta: 0:10:28  lr: 0.000000  loss: 0.028162 (0.077480)  time: 0.469942  data: 0.431810  max mem: 211\n",
      "Epoch: [1]  [ 740/2012]  eta: 0:10:18  lr: 0.000000  loss: 0.057996 (0.077324)  time: 0.465674  data: 0.427539  max mem: 211\n",
      "Epoch: [1]  [ 760/2012]  eta: 0:10:08  lr: 0.000000  loss: 0.026980 (0.076474)  time: 0.472900  data: 0.434784  max mem: 211\n",
      "Epoch: [1]  [ 780/2012]  eta: 0:09:57  lr: 0.000000  loss: 0.024923 (0.076175)  time: 0.461499  data: 0.423393  max mem: 211\n",
      "Epoch: [1]  [ 800/2012]  eta: 0:09:47  lr: 0.000000  loss: 0.030287 (0.076533)  time: 0.459599  data: 0.421476  max mem: 211\n",
      "Epoch: [1]  [ 820/2012]  eta: 0:09:37  lr: 0.000000  loss: 0.067242 (0.076408)  time: 0.467226  data: 0.429083  max mem: 211\n",
      "Epoch: [1]  [ 840/2012]  eta: 0:09:27  lr: 0.000000  loss: 0.032722 (0.076614)  time: 0.485193  data: 0.447076  max mem: 211\n",
      "Epoch: [1]  [ 860/2012]  eta: 0:09:18  lr: 0.000000  loss: 0.028994 (0.076656)  time: 0.522404  data: 0.484277  max mem: 211\n",
      "Epoch: [1]  [ 880/2012]  eta: 0:09:10  lr: 0.000000  loss: 0.039631 (0.076865)  time: 0.525979  data: 0.487823  max mem: 211\n",
      "Epoch: [1]  [ 900/2012]  eta: 0:09:01  lr: 0.000000  loss: 0.036611 (0.076398)  time: 0.536935  data: 0.498807  max mem: 211\n",
      "Epoch: [1]  [ 920/2012]  eta: 0:08:52  lr: 0.000000  loss: 0.054139 (0.076988)  time: 0.488143  data: 0.449989  max mem: 211\n",
      "Epoch: [1]  [ 940/2012]  eta: 0:08:42  lr: 0.000000  loss: 0.026851 (0.076389)  time: 0.479873  data: 0.441726  max mem: 211\n",
      "Epoch: [1]  [ 960/2012]  eta: 0:08:32  lr: 0.000000  loss: 0.072742 (0.077106)  time: 0.489240  data: 0.451088  max mem: 211\n",
      "Epoch: [1]  [ 980/2012]  eta: 0:08:22  lr: 0.000000  loss: 0.055265 (0.077408)  time: 0.487031  data: 0.448882  max mem: 211\n",
      "Epoch: [1]  [1000/2012]  eta: 0:08:13  lr: 0.000000  loss: 0.043211 (0.077559)  time: 0.508327  data: 0.470206  max mem: 211\n",
      "Epoch: [1]  [1020/2012]  eta: 0:08:03  lr: 0.000000  loss: 0.035788 (0.078257)  time: 0.489984  data: 0.451847  max mem: 211\n",
      "Epoch: [1]  [1040/2012]  eta: 0:07:53  lr: 0.000000  loss: 0.027212 (0.077775)  time: 0.465540  data: 0.427389  max mem: 211\n",
      "Epoch: [1]  [1060/2012]  eta: 0:07:44  lr: 0.000000  loss: 0.056084 (0.077934)  time: 0.505489  data: 0.467355  max mem: 211\n",
      "Epoch: [1]  [1080/2012]  eta: 0:07:33  lr: 0.000000  loss: 0.009794 (0.077663)  time: 0.457698  data: 0.419643  max mem: 211\n",
      "Epoch: [1]  [1100/2012]  eta: 0:07:24  lr: 0.000000  loss: 0.029961 (0.077447)  time: 0.504119  data: 0.466104  max mem: 211\n",
      "Epoch: [1]  [1120/2012]  eta: 0:07:14  lr: 0.000000  loss: 0.024783 (0.077802)  time: 0.495445  data: 0.457326  max mem: 211\n",
      "Epoch: [1]  [1140/2012]  eta: 0:07:05  lr: 0.000000  loss: 0.057797 (0.078059)  time: 0.490419  data: 0.452276  max mem: 211\n",
      "Epoch: [1]  [1160/2012]  eta: 0:06:55  lr: 0.000000  loss: 0.025613 (0.077871)  time: 0.502233  data: 0.464139  max mem: 211\n",
      "Epoch: [1]  [1180/2012]  eta: 0:06:46  lr: 0.000000  loss: 0.022035 (0.077293)  time: 0.539294  data: 0.501186  max mem: 211\n",
      "Epoch: [1]  [1200/2012]  eta: 0:06:37  lr: 0.000000  loss: 0.079833 (0.077926)  time: 0.536213  data: 0.498103  max mem: 211\n",
      "Epoch: [1]  [1220/2012]  eta: 0:06:27  lr: 0.000000  loss: 0.020836 (0.077931)  time: 0.513274  data: 0.475132  max mem: 211\n",
      "Epoch: [1]  [1240/2012]  eta: 0:06:17  lr: 0.000000  loss: 0.020617 (0.077950)  time: 0.480064  data: 0.441922  max mem: 211\n",
      "Epoch: [1]  [1260/2012]  eta: 0:06:08  lr: 0.000000  loss: 0.018655 (0.077876)  time: 0.479584  data: 0.441459  max mem: 211\n",
      "Epoch: [1]  [1280/2012]  eta: 0:05:58  lr: 0.000000  loss: 0.026419 (0.077606)  time: 0.471583  data: 0.433416  max mem: 211\n",
      "Epoch: [1]  [1300/2012]  eta: 0:05:48  lr: 0.000000  loss: 0.015994 (0.077119)  time: 0.465282  data: 0.427172  max mem: 211\n",
      "Epoch: [1]  [1320/2012]  eta: 0:05:38  lr: 0.000000  loss: 0.036624 (0.077074)  time: 0.527704  data: 0.489554  max mem: 211\n",
      "Epoch: [1]  [1340/2012]  eta: 0:05:28  lr: 0.000000  loss: 0.047064 (0.076954)  time: 0.468755  data: 0.430618  max mem: 211\n",
      "Epoch: [1]  [1360/2012]  eta: 0:05:18  lr: 0.000000  loss: 0.031782 (0.076795)  time: 0.468204  data: 0.430078  max mem: 211\n",
      "Epoch: [1]  [1380/2012]  eta: 0:05:08  lr: 0.000000  loss: 0.012382 (0.076786)  time: 0.470069  data: 0.431926  max mem: 211\n",
      "Epoch: [1]  [1400/2012]  eta: 0:04:58  lr: 0.000000  loss: 0.014999 (0.076370)  time: 0.466150  data: 0.428018  max mem: 211\n",
      "Epoch: [1]  [1420/2012]  eta: 0:04:48  lr: 0.000000  loss: 0.046547 (0.077104)  time: 0.462353  data: 0.424212  max mem: 211\n",
      "Epoch: [1]  [1440/2012]  eta: 0:04:38  lr: 0.000000  loss: 0.025886 (0.076740)  time: 0.475199  data: 0.437050  max mem: 211\n",
      "Epoch: [1]  [1460/2012]  eta: 0:04:29  lr: 0.000000  loss: 0.030186 (0.076399)  time: 0.471511  data: 0.433382  max mem: 211\n",
      "Epoch: [1]  [1480/2012]  eta: 0:04:19  lr: 0.000000  loss: 0.033346 (0.076269)  time: 0.466734  data: 0.428640  max mem: 211\n",
      "Epoch: [1]  [1500/2012]  eta: 0:04:09  lr: 0.000000  loss: 0.048453 (0.075978)  time: 0.473828  data: 0.435703  max mem: 211\n",
      "Epoch: [1]  [1520/2012]  eta: 0:03:59  lr: 0.000000  loss: 0.027140 (0.075860)  time: 0.477579  data: 0.439458  max mem: 211\n",
      "Epoch: [1]  [1540/2012]  eta: 0:03:49  lr: 0.000000  loss: 0.044412 (0.075787)  time: 0.465358  data: 0.427314  max mem: 211\n",
      "Epoch: [1]  [1560/2012]  eta: 0:03:39  lr: 0.000000  loss: 0.026552 (0.075848)  time: 0.462834  data: 0.424822  max mem: 211\n",
      "Epoch: [1]  [1580/2012]  eta: 0:03:30  lr: 0.000000  loss: 0.041059 (0.076395)  time: 0.476395  data: 0.438330  max mem: 211\n",
      "Epoch: [1]  [1600/2012]  eta: 0:03:20  lr: 0.000000  loss: 0.030246 (0.076227)  time: 0.500448  data: 0.462270  max mem: 211\n",
      "Epoch: [1]  [1620/2012]  eta: 0:03:10  lr: 0.000000  loss: 0.036634 (0.076097)  time: 0.524348  data: 0.486232  max mem: 211\n",
      "Epoch: [1]  [1640/2012]  eta: 0:03:01  lr: 0.000000  loss: 0.057421 (0.076271)  time: 0.523289  data: 0.485171  max mem: 211\n",
      "Epoch: [1]  [1660/2012]  eta: 0:02:51  lr: 0.000000  loss: 0.044258 (0.076283)  time: 0.541265  data: 0.503144  max mem: 211\n",
      "Epoch: [1]  [1680/2012]  eta: 0:02:42  lr: 0.000000  loss: 0.026157 (0.076041)  time: 0.587744  data: 0.549617  max mem: 211\n",
      "Epoch: [1]  [1700/2012]  eta: 0:02:32  lr: 0.000000  loss: 0.039392 (0.076022)  time: 0.555425  data: 0.517258  max mem: 211\n",
      "Epoch: [1]  [1720/2012]  eta: 0:02:23  lr: 0.000000  loss: 0.036162 (0.075790)  time: 0.510644  data: 0.472463  max mem: 211\n",
      "Epoch: [1]  [1740/2012]  eta: 0:02:13  lr: 0.000000  loss: 0.040149 (0.075956)  time: 0.515032  data: 0.476923  max mem: 211\n",
      "Epoch: [1]  [1760/2012]  eta: 0:02:03  lr: 0.000000  loss: 0.046009 (0.076032)  time: 0.514229  data: 0.476139  max mem: 211\n",
      "Epoch: [1]  [1780/2012]  eta: 0:01:54  lr: 0.000000  loss: 0.029062 (0.076271)  time: 0.616757  data: 0.578642  max mem: 211\n",
      "Epoch: [1]  [1800/2012]  eta: 0:01:44  lr: 0.000000  loss: 0.020641 (0.075976)  time: 0.610260  data: 0.572089  max mem: 211\n",
      "Epoch: [1]  [1820/2012]  eta: 0:01:34  lr: 0.000000  loss: 0.018579 (0.075854)  time: 0.506266  data: 0.468139  max mem: 211\n",
      "Epoch: [1]  [1840/2012]  eta: 0:01:24  lr: 0.000000  loss: 0.034909 (0.075934)  time: 0.505565  data: 0.467402  max mem: 211\n",
      "Epoch: [1]  [1860/2012]  eta: 0:01:15  lr: 0.000000  loss: 0.046678 (0.076177)  time: 0.519024  data: 0.480893  max mem: 211\n",
      "Epoch: [1]  [1880/2012]  eta: 0:01:05  lr: 0.000000  loss: 0.049599 (0.076225)  time: 0.518161  data: 0.480037  max mem: 211\n",
      "Epoch: [1]  [1900/2012]  eta: 0:00:55  lr: 0.000000  loss: 0.032178 (0.075862)  time: 0.474464  data: 0.436313  max mem: 211\n",
      "Epoch: [1]  [1920/2012]  eta: 0:00:45  lr: 0.000000  loss: 0.035471 (0.075578)  time: 0.481592  data: 0.443470  max mem: 211\n",
      "Epoch: [1]  [1940/2012]  eta: 0:00:35  lr: 0.000000  loss: 0.020236 (0.075240)  time: 0.505226  data: 0.467086  max mem: 211\n",
      "Epoch: [1]  [1960/2012]  eta: 0:00:25  lr: 0.000000  loss: 0.023877 (0.075541)  time: 0.495802  data: 0.457709  max mem: 211\n",
      "Epoch: [1]  [1980/2012]  eta: 0:00:15  lr: 0.000000  loss: 0.040097 (0.075366)  time: 0.465226  data: 0.427181  max mem: 211\n",
      "Epoch: [1]  [2000/2012]  eta: 0:00:05  lr: 0.000000  loss: 0.017719 (0.074966)  time: 0.469580  data: 0.431517  max mem: 211\n",
      "Epoch: [1]  [2011/2012]  eta: 0:00:00  lr: 0.000000  loss: 0.042188 (0.074969)  time: 0.454249  data: 0.417486  max mem: 211\n",
      "Epoch: [1] Total time: 0:16:32 (0.493205 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.042188 (0.074969)\n",
      "Test:  [ 0/79]  eta: 0:00:38  loss: 0.014553 (0.014553)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.493172  data: 0.455018  max mem: 211\n",
      "Test:  [20/79]  eta: 0:00:28  loss: 0.065969 (0.117765)  acc1: 93.750000 (96.428571)  acc5: 100.000000 (100.000000)  time: 0.488015  data: 0.449838  max mem: 211\n",
      "Test:  [40/79]  eta: 0:00:19  loss: 0.064802 (0.099378)  acc1: 100.000000 (96.798780)  acc5: 100.000000 (100.000000)  time: 0.518520  data: 0.480362  max mem: 211\n",
      "Test:  [60/79]  eta: 0:00:09  loss: 0.085115 (0.103508)  acc1: 93.750000 (96.516393)  acc5: 100.000000 (99.897541)  time: 0.542683  data: 0.504556  max mem: 211\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 0.024753 (0.100158)  acc1: 100.000000 (97.200000)  acc5: 100.000000 (99.920000)  time: 0.516772  data: 0.479912  max mem: 211\n",
      "Test: Total time: 0:00:40 (0.517174 s / it)\n",
      "* Acc@1 97.200 Acc@5 99.920 loss 0.100\n",
      "Accuracy at epoch 1 of the network on the 79 test images: 97.2%\n",
      "Accuracy at epoch 1 of the network on the 1250 test images: 97.2%\n",
      "Max accuracy so far: 97.20%\n",
      "Epoch: [2]  [   0/2012]  eta: 0:15:28  lr: 0.000063  loss: 0.005330 (0.005330)  time: 0.461684  data: 0.423612  max mem: 211\n",
      "Epoch: [2]  [  20/2012]  eta: 0:19:08  lr: 0.000063  loss: 0.011232 (0.049991)  time: 0.582087  data: 0.543944  max mem: 211\n",
      "Epoch: [2]  [  40/2012]  eta: 0:17:26  lr: 0.000063  loss: 0.040048 (0.068912)  time: 0.482205  data: 0.444092  max mem: 211\n",
      "Epoch: [2]  [  60/2012]  eta: 0:16:46  lr: 0.000063  loss: 0.034136 (0.080130)  time: 0.485972  data: 0.447847  max mem: 211\n",
      "Epoch: [2]  [  80/2012]  eta: 0:16:12  lr: 0.000063  loss: 0.059056 (0.080550)  time: 0.464718  data: 0.426619  max mem: 211\n",
      "Epoch: [2]  [ 100/2012]  eta: 0:16:02  lr: 0.000063  loss: 0.016719 (0.071608)  time: 0.503762  data: 0.465653  max mem: 211\n",
      "Epoch: [2]  [ 120/2012]  eta: 0:16:07  lr: 0.000063  loss: 0.033637 (0.073345)  time: 0.551544  data: 0.513315  max mem: 211\n",
      "Epoch: [2]  [ 140/2012]  eta: 0:15:56  lr: 0.000063  loss: 0.056505 (0.076377)  time: 0.507732  data: 0.469605  max mem: 211\n",
      "Epoch: [2]  [ 160/2012]  eta: 0:16:02  lr: 0.000063  loss: 0.035610 (0.074340)  time: 0.582563  data: 0.544438  max mem: 211\n",
      "Epoch: [2]  [ 180/2012]  eta: 0:15:42  lr: 0.000063  loss: 0.026446 (0.074191)  time: 0.469931  data: 0.431815  max mem: 211\n",
      "Epoch: [2]  [ 200/2012]  eta: 0:15:25  lr: 0.000063  loss: 0.018662 (0.072253)  time: 0.477220  data: 0.439084  max mem: 211\n",
      "Epoch: [2]  [ 220/2012]  eta: 0:15:17  lr: 0.000063  loss: 0.053119 (0.072730)  time: 0.524496  data: 0.486384  max mem: 211\n",
      "Epoch: [2]  [ 240/2012]  eta: 0:15:04  lr: 0.000063  loss: 0.028623 (0.071049)  time: 0.498048  data: 0.459925  max mem: 211\n",
      "Epoch: [2]  [ 260/2012]  eta: 0:14:57  lr: 0.000063  loss: 0.048758 (0.069422)  time: 0.532254  data: 0.494134  max mem: 211\n",
      "Epoch: [2]  [ 280/2012]  eta: 0:14:48  lr: 0.000063  loss: 0.034510 (0.070606)  time: 0.525718  data: 0.487540  max mem: 211\n",
      "Epoch: [2]  [ 300/2012]  eta: 0:14:42  lr: 0.000063  loss: 0.036258 (0.070129)  time: 0.542394  data: 0.504262  max mem: 211\n",
      "Epoch: [2]  [ 320/2012]  eta: 0:14:37  lr: 0.000063  loss: 0.021990 (0.069451)  time: 0.570177  data: 0.532064  max mem: 211\n",
      "Epoch: [2]  [ 340/2012]  eta: 0:14:25  lr: 0.000063  loss: 0.025445 (0.069650)  time: 0.497724  data: 0.459662  max mem: 211\n",
      "Epoch: [2]  [ 360/2012]  eta: 0:14:18  lr: 0.000063  loss: 0.027199 (0.070140)  time: 0.555658  data: 0.517621  max mem: 211\n",
      "Epoch: [2]  [ 380/2012]  eta: 0:14:05  lr: 0.000063  loss: 0.012806 (0.069742)  time: 0.492697  data: 0.454531  max mem: 211\n",
      "Epoch: [2]  [ 400/2012]  eta: 0:13:55  lr: 0.000063  loss: 0.029276 (0.070466)  time: 0.517090  data: 0.478962  max mem: 211\n",
      "Epoch: [2]  [ 420/2012]  eta: 0:13:42  lr: 0.000063  loss: 0.010286 (0.068517)  time: 0.493473  data: 0.455319  max mem: 211\n",
      "Epoch: [2]  [ 440/2012]  eta: 0:13:31  lr: 0.000063  loss: 0.059503 (0.068469)  time: 0.498167  data: 0.460024  max mem: 211\n",
      "Epoch: [2]  [ 460/2012]  eta: 0:13:20  lr: 0.000063  loss: 0.013551 (0.068006)  time: 0.508078  data: 0.469933  max mem: 211\n",
      "Epoch: [2]  [ 480/2012]  eta: 0:13:08  lr: 0.000063  loss: 0.062677 (0.068631)  time: 0.487558  data: 0.449415  max mem: 211\n",
      "Epoch: [2]  [ 500/2012]  eta: 0:12:57  lr: 0.000063  loss: 0.042332 (0.070876)  time: 0.510627  data: 0.472509  max mem: 211\n",
      "Epoch: [2]  [ 520/2012]  eta: 0:12:46  lr: 0.000063  loss: 0.030824 (0.071282)  time: 0.494052  data: 0.455903  max mem: 211\n",
      "Epoch: [2]  [ 540/2012]  eta: 0:12:33  lr: 0.000063  loss: 0.033984 (0.071479)  time: 0.476527  data: 0.438383  max mem: 211\n",
      "Epoch: [2]  [ 560/2012]  eta: 0:12:21  lr: 0.000063  loss: 0.046995 (0.071873)  time: 0.472945  data: 0.434790  max mem: 211\n",
      "Epoch: [2]  [ 580/2012]  eta: 0:12:09  lr: 0.000063  loss: 0.052934 (0.072372)  time: 0.469609  data: 0.431496  max mem: 211\n",
      "Epoch: [2]  [ 600/2012]  eta: 0:11:56  lr: 0.000063  loss: 0.017810 (0.071314)  time: 0.458975  data: 0.420831  max mem: 211\n",
      "Epoch: [2]  [ 620/2012]  eta: 0:11:45  lr: 0.000063  loss: 0.036237 (0.071650)  time: 0.480462  data: 0.442337  max mem: 211\n",
      "Epoch: [2]  [ 640/2012]  eta: 0:11:33  lr: 0.000063  loss: 0.021361 (0.071409)  time: 0.456873  data: 0.418738  max mem: 211\n",
      "Epoch: [2]  [ 660/2012]  eta: 0:11:21  lr: 0.000063  loss: 0.011988 (0.070150)  time: 0.459927  data: 0.421805  max mem: 211\n",
      "Epoch: [2]  [ 680/2012]  eta: 0:11:09  lr: 0.000063  loss: 0.058832 (0.071337)  time: 0.462602  data: 0.424486  max mem: 211\n",
      "Epoch: [2]  [ 700/2012]  eta: 0:10:58  lr: 0.000063  loss: 0.057940 (0.072770)  time: 0.463538  data: 0.425418  max mem: 211\n",
      "Epoch: [2]  [ 720/2012]  eta: 0:10:46  lr: 0.000063  loss: 0.031155 (0.072884)  time: 0.467830  data: 0.429704  max mem: 211\n",
      "Epoch: [2]  [ 740/2012]  eta: 0:10:35  lr: 0.000063  loss: 0.037619 (0.072458)  time: 0.464011  data: 0.425884  max mem: 211\n",
      "Epoch: [2]  [ 760/2012]  eta: 0:10:24  lr: 0.000063  loss: 0.020229 (0.071500)  time: 0.471624  data: 0.433522  max mem: 211\n",
      "Epoch: [2]  [ 780/2012]  eta: 0:10:14  lr: 0.000063  loss: 0.021169 (0.071190)  time: 0.481989  data: 0.443865  max mem: 211\n",
      "Epoch: [2]  [ 800/2012]  eta: 0:10:02  lr: 0.000063  loss: 0.026221 (0.071476)  time: 0.448653  data: 0.410606  max mem: 211\n",
      "Epoch: [2]  [ 820/2012]  eta: 0:09:51  lr: 0.000063  loss: 0.042556 (0.071334)  time: 0.469429  data: 0.431391  max mem: 211\n",
      "Epoch: [2]  [ 840/2012]  eta: 0:09:41  lr: 0.000063  loss: 0.029400 (0.071516)  time: 0.486100  data: 0.448020  max mem: 211\n",
      "Epoch: [2]  [ 860/2012]  eta: 0:09:31  lr: 0.000063  loss: 0.027855 (0.071900)  time: 0.473283  data: 0.435138  max mem: 211\n",
      "Epoch: [2]  [ 880/2012]  eta: 0:09:20  lr: 0.000063  loss: 0.037516 (0.071962)  time: 0.473051  data: 0.434931  max mem: 211\n",
      "Epoch: [2]  [ 900/2012]  eta: 0:09:10  lr: 0.000063  loss: 0.036583 (0.071513)  time: 0.473606  data: 0.435427  max mem: 211\n",
      "Epoch: [2]  [ 920/2012]  eta: 0:08:59  lr: 0.000063  loss: 0.049218 (0.072046)  time: 0.480191  data: 0.442057  max mem: 211\n",
      "Epoch: [2]  [ 940/2012]  eta: 0:08:49  lr: 0.000063  loss: 0.019372 (0.071332)  time: 0.489098  data: 0.450961  max mem: 211\n",
      "Epoch: [2]  [ 960/2012]  eta: 0:08:39  lr: 0.000063  loss: 0.049971 (0.071580)  time: 0.470359  data: 0.432259  max mem: 211\n",
      "Epoch: [2]  [ 980/2012]  eta: 0:08:29  lr: 0.000063  loss: 0.020712 (0.071232)  time: 0.464866  data: 0.426719  max mem: 211\n",
      "Epoch: [2]  [1000/2012]  eta: 0:08:19  lr: 0.000063  loss: 0.032101 (0.071528)  time: 0.496290  data: 0.458176  max mem: 211\n",
      "Epoch: [2]  [1020/2012]  eta: 0:08:08  lr: 0.000063  loss: 0.030765 (0.072492)  time: 0.463409  data: 0.425248  max mem: 211\n",
      "Epoch: [2]  [1040/2012]  eta: 0:07:58  lr: 0.000063  loss: 0.022046 (0.072028)  time: 0.465283  data: 0.427149  max mem: 211\n",
      "Epoch: [2]  [1060/2012]  eta: 0:07:48  lr: 0.000063  loss: 0.039600 (0.071888)  time: 0.473125  data: 0.435010  max mem: 211\n",
      "Epoch: [2]  [1080/2012]  eta: 0:07:38  lr: 0.000063  loss: 0.007381 (0.071632)  time: 0.478295  data: 0.440162  max mem: 211\n",
      "Epoch: [2]  [1100/2012]  eta: 0:07:27  lr: 0.000063  loss: 0.021214 (0.071511)  time: 0.465617  data: 0.427455  max mem: 211\n",
      "Epoch: [2]  [1120/2012]  eta: 0:07:18  lr: 0.000063  loss: 0.018610 (0.071977)  time: 0.514680  data: 0.476545  max mem: 211\n",
      "Epoch: [2]  [1140/2012]  eta: 0:07:08  lr: 0.000063  loss: 0.047304 (0.072188)  time: 0.481394  data: 0.443263  max mem: 211\n",
      "Epoch: [2]  [1160/2012]  eta: 0:06:58  lr: 0.000063  loss: 0.019383 (0.071988)  time: 0.479906  data: 0.441791  max mem: 211\n",
      "Epoch: [2]  [1180/2012]  eta: 0:06:48  lr: 0.000063  loss: 0.017545 (0.071248)  time: 0.487190  data: 0.449074  max mem: 211\n",
      "Epoch: [2]  [1200/2012]  eta: 0:06:38  lr: 0.000063  loss: 0.054178 (0.071701)  time: 0.487335  data: 0.449227  max mem: 211\n",
      "Epoch: [2]  [1220/2012]  eta: 0:06:29  lr: 0.000063  loss: 0.013325 (0.071586)  time: 0.500304  data: 0.462127  max mem: 211\n",
      "Epoch: [2]  [1240/2012]  eta: 0:06:19  lr: 0.000063  loss: 0.018722 (0.071645)  time: 0.524259  data: 0.486110  max mem: 211\n",
      "Epoch: [2]  [1260/2012]  eta: 0:06:09  lr: 0.000063  loss: 0.014971 (0.071615)  time: 0.462469  data: 0.424368  max mem: 211\n",
      "Epoch: [2]  [1280/2012]  eta: 0:05:59  lr: 0.000063  loss: 0.016886 (0.071190)  time: 0.467855  data: 0.429806  max mem: 211\n",
      "Epoch: [2]  [1300/2012]  eta: 0:05:49  lr: 0.000063  loss: 0.012625 (0.070724)  time: 0.455569  data: 0.417570  max mem: 211\n",
      "Epoch: [2]  [1320/2012]  eta: 0:05:39  lr: 0.000063  loss: 0.025025 (0.070540)  time: 0.496277  data: 0.458166  max mem: 211\n",
      "Epoch: [2]  [1340/2012]  eta: 0:05:29  lr: 0.000063  loss: 0.027733 (0.070470)  time: 0.531857  data: 0.493733  max mem: 211\n",
      "Epoch: [2]  [1360/2012]  eta: 0:05:20  lr: 0.000063  loss: 0.018313 (0.070229)  time: 0.525020  data: 0.486863  max mem: 211\n",
      "Epoch: [2]  [1380/2012]  eta: 0:05:10  lr: 0.000063  loss: 0.007769 (0.070234)  time: 0.504997  data: 0.466884  max mem: 211\n",
      "Epoch: [2]  [1400/2012]  eta: 0:05:01  lr: 0.000063  loss: 0.012247 (0.069895)  time: 0.556108  data: 0.517859  max mem: 211\n",
      "Epoch: [2]  [1420/2012]  eta: 0:04:51  lr: 0.000063  loss: 0.045171 (0.070942)  time: 0.509601  data: 0.471473  max mem: 211\n",
      "Epoch: [2]  [1440/2012]  eta: 0:04:42  lr: 0.000063  loss: 0.019995 (0.070601)  time: 0.509975  data: 0.471840  max mem: 211\n",
      "Epoch: [2]  [1460/2012]  eta: 0:04:32  lr: 0.000063  loss: 0.030798 (0.070163)  time: 0.499550  data: 0.461444  max mem: 211\n",
      "Epoch: [2]  [1480/2012]  eta: 0:04:22  lr: 0.000063  loss: 0.028319 (0.069952)  time: 0.467639  data: 0.429497  max mem: 211\n",
      "Epoch: [2]  [1500/2012]  eta: 0:04:12  lr: 0.000063  loss: 0.011941 (0.069560)  time: 0.472132  data: 0.433991  max mem: 211\n",
      "Epoch: [2]  [1520/2012]  eta: 0:04:02  lr: 0.000063  loss: 0.021357 (0.069536)  time: 0.477636  data: 0.439506  max mem: 211\n",
      "Epoch: [2]  [1540/2012]  eta: 0:03:52  lr: 0.000063  loss: 0.024866 (0.069462)  time: 0.476835  data: 0.438713  max mem: 211\n",
      "Epoch: [2]  [1560/2012]  eta: 0:03:42  lr: 0.000063  loss: 0.034800 (0.069659)  time: 0.474800  data: 0.436664  max mem: 211\n",
      "Epoch: [2]  [1580/2012]  eta: 0:03:32  lr: 0.000063  loss: 0.025376 (0.070220)  time: 0.484517  data: 0.446356  max mem: 211\n",
      "Epoch: [2]  [1600/2012]  eta: 0:03:22  lr: 0.000063  loss: 0.027032 (0.069947)  time: 0.476725  data: 0.438595  max mem: 211\n",
      "Epoch: [2]  [1620/2012]  eta: 0:03:12  lr: 0.000063  loss: 0.023113 (0.069758)  time: 0.475564  data: 0.437437  max mem: 211\n",
      "Epoch: [2]  [1640/2012]  eta: 0:03:03  lr: 0.000063  loss: 0.042453 (0.069823)  time: 0.566013  data: 0.527892  max mem: 211\n",
      "Epoch: [2]  [1660/2012]  eta: 0:02:53  lr: 0.000063  loss: 0.034084 (0.069711)  time: 0.539145  data: 0.501035  max mem: 211\n",
      "Epoch: [2]  [1680/2012]  eta: 0:02:43  lr: 0.000063  loss: 0.008973 (0.069426)  time: 0.480144  data: 0.442023  max mem: 211\n",
      "Epoch: [2]  [1700/2012]  eta: 0:02:33  lr: 0.000063  loss: 0.045585 (0.069549)  time: 0.467702  data: 0.429570  max mem: 211\n",
      "Epoch: [2]  [1720/2012]  eta: 0:02:23  lr: 0.000063  loss: 0.028024 (0.069396)  time: 0.469830  data: 0.431732  max mem: 211\n",
      "Epoch: [2]  [1740/2012]  eta: 0:02:13  lr: 0.000063  loss: 0.031484 (0.069616)  time: 0.506894  data: 0.468842  max mem: 211\n",
      "Epoch: [2]  [1760/2012]  eta: 0:02:04  lr: 0.000063  loss: 0.019868 (0.069622)  time: 0.467625  data: 0.429603  max mem: 211\n",
      "Epoch: [2]  [1780/2012]  eta: 0:01:54  lr: 0.000063  loss: 0.032058 (0.069921)  time: 0.533776  data: 0.495622  max mem: 211\n",
      "Epoch: [2]  [1800/2012]  eta: 0:01:44  lr: 0.000063  loss: 0.012463 (0.069582)  time: 0.486881  data: 0.448756  max mem: 211\n",
      "Epoch: [2]  [1820/2012]  eta: 0:01:34  lr: 0.000063  loss: 0.034465 (0.069673)  time: 0.478028  data: 0.439872  max mem: 211\n",
      "Epoch: [2]  [1840/2012]  eta: 0:01:24  lr: 0.000063  loss: 0.030474 (0.069838)  time: 0.474387  data: 0.436284  max mem: 211\n",
      "Epoch: [2]  [1860/2012]  eta: 0:01:14  lr: 0.000063  loss: 0.037637 (0.070027)  time: 0.485641  data: 0.447516  max mem: 211\n",
      "Epoch: [2]  [1880/2012]  eta: 0:01:04  lr: 0.000063  loss: 0.030390 (0.070037)  time: 0.496936  data: 0.458808  max mem: 211\n",
      "Epoch: [2]  [1900/2012]  eta: 0:00:55  lr: 0.000063  loss: 0.014511 (0.069720)  time: 0.474809  data: 0.436708  max mem: 211\n",
      "Epoch: [2]  [1920/2012]  eta: 0:00:45  lr: 0.000063  loss: 0.028199 (0.069500)  time: 0.498999  data: 0.460877  max mem: 211\n",
      "Epoch: [2]  [1940/2012]  eta: 0:00:35  lr: 0.000063  loss: 0.019109 (0.069221)  time: 0.482665  data: 0.444523  max mem: 211\n",
      "Epoch: [2]  [1960/2012]  eta: 0:00:25  lr: 0.000063  loss: 0.013329 (0.069583)  time: 0.493394  data: 0.455281  max mem: 211\n",
      "Epoch: [2]  [1980/2012]  eta: 0:00:15  lr: 0.000063  loss: 0.031219 (0.069615)  time: 0.474704  data: 0.436601  max mem: 211\n",
      "Epoch: [2]  [2000/2012]  eta: 0:00:05  lr: 0.000063  loss: 0.019140 (0.069292)  time: 0.497489  data: 0.459383  max mem: 211\n",
      "Epoch: [2]  [2011/2012]  eta: 0:00:00  lr: 0.000063  loss: 0.032366 (0.069309)  time: 0.464966  data: 0.428192  max mem: 211\n",
      "Epoch: [2] Total time: 0:16:29 (0.491777 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 0.032366 (0.069309)\n",
      "Test:  [ 0/79]  eta: 0:00:37  loss: 0.006180 (0.006180)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.480270  data: 0.442019  max mem: 211\n",
      "Test:  [20/79]  eta: 0:00:30  loss: 0.064749 (0.105674)  acc1: 93.750000 (96.726190)  acc5: 100.000000 (100.000000)  time: 0.510449  data: 0.472301  max mem: 211\n",
      "Test:  [40/79]  eta: 0:00:20  loss: 0.057951 (0.084190)  acc1: 100.000000 (97.256098)  acc5: 100.000000 (100.000000)  time: 0.528801  data: 0.490662  max mem: 211\n",
      "Test:  [60/79]  eta: 0:00:09  loss: 0.068573 (0.089204)  acc1: 100.000000 (97.028689)  acc5: 100.000000 (100.000000)  time: 0.501176  data: 0.463043  max mem: 211\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 0.017033 (0.085295)  acc1: 100.000000 (97.600000)  acc5: 100.000000 (100.000000)  time: 0.496465  data: 0.459642  max mem: 211\n",
      "Test: Total time: 0:00:40 (0.509758 s / it)\n",
      "* Acc@1 97.600 Acc@5 100.000 loss 0.085\n",
      "Accuracy at epoch 2 of the network on the 79 test images: 97.6%\n",
      "Accuracy at epoch 2 of the network on the 1250 test images: 97.6%\n",
      "Max accuracy so far: 97.60%\n",
      "Epoch: [3]  [   0/2012]  eta: 0:15:35  lr: 0.000000  loss: 0.003600 (0.003600)  time: 0.465183  data: 0.427027  max mem: 211\n",
      "Epoch: [3]  [  20/2012]  eta: 0:16:14  lr: 0.000000  loss: 0.009508 (0.038205)  time: 0.490659  data: 0.452530  max mem: 211\n",
      "Epoch: [3]  [  40/2012]  eta: 0:16:45  lr: 0.000000  loss: 0.030892 (0.053350)  time: 0.531678  data: 0.493560  max mem: 211\n",
      "Epoch: [3]  [  60/2012]  eta: 0:16:07  lr: 0.000000  loss: 0.015574 (0.059352)  time: 0.466366  data: 0.428265  max mem: 211\n",
      "Epoch: [3]  [  80/2012]  eta: 0:15:44  lr: 0.000000  loss: 0.046274 (0.064307)  time: 0.467105  data: 0.428950  max mem: 211\n",
      "Epoch: [3]  [ 100/2012]  eta: 0:15:28  lr: 0.000000  loss: 0.019046 (0.057295)  time: 0.472556  data: 0.434447  max mem: 211\n",
      "Epoch: [3]  [ 120/2012]  eta: 0:15:07  lr: 0.000000  loss: 0.026059 (0.056625)  time: 0.450477  data: 0.412418  max mem: 211\n",
      "Epoch: [3]  [ 140/2012]  eta: 0:14:55  lr: 0.000000  loss: 0.034326 (0.058339)  time: 0.468633  data: 0.430597  max mem: 211\n",
      "Epoch: [3]  [ 160/2012]  eta: 0:14:44  lr: 0.000000  loss: 0.029218 (0.056910)  time: 0.475475  data: 0.437287  max mem: 211\n",
      "Epoch: [3]  [ 180/2012]  eta: 0:14:32  lr: 0.000000  loss: 0.023541 (0.058028)  time: 0.464835  data: 0.426707  max mem: 211\n",
      "Epoch: [3]  [ 200/2012]  eta: 0:14:22  lr: 0.000000  loss: 0.006622 (0.055576)  time: 0.473286  data: 0.435179  max mem: 211\n",
      "Epoch: [3]  [ 220/2012]  eta: 0:14:12  lr: 0.000000  loss: 0.028221 (0.056738)  time: 0.473441  data: 0.435306  max mem: 211\n",
      "Epoch: [3]  [ 240/2012]  eta: 0:14:08  lr: 0.000000  loss: 0.021684 (0.055589)  time: 0.511244  data: 0.473035  max mem: 211\n",
      "Epoch: [3]  [ 260/2012]  eta: 0:14:06  lr: 0.000000  loss: 0.018865 (0.054218)  time: 0.532909  data: 0.494799  max mem: 211\n",
      "Epoch: [3]  [ 280/2012]  eta: 0:13:58  lr: 0.000000  loss: 0.026977 (0.056421)  time: 0.501063  data: 0.462950  max mem: 211\n",
      "Epoch: [3]  [ 300/2012]  eta: 0:13:53  lr: 0.000000  loss: 0.047479 (0.057051)  time: 0.525526  data: 0.487372  max mem: 211\n",
      "Epoch: [3]  [ 320/2012]  eta: 0:13:51  lr: 0.000000  loss: 0.021277 (0.056149)  time: 0.558329  data: 0.520212  max mem: 211\n",
      "Epoch: [3]  [ 340/2012]  eta: 0:13:42  lr: 0.000000  loss: 0.027217 (0.056580)  time: 0.499959  data: 0.461842  max mem: 211\n",
      "Epoch: [3]  [ 360/2012]  eta: 0:13:36  lr: 0.000000  loss: 0.021522 (0.056750)  time: 0.535678  data: 0.497550  max mem: 211\n",
      "Epoch: [3]  [ 380/2012]  eta: 0:13:26  lr: 0.000000  loss: 0.010315 (0.056142)  time: 0.490327  data: 0.452197  max mem: 211\n",
      "Epoch: [3]  [ 400/2012]  eta: 0:13:15  lr: 0.000000  loss: 0.027218 (0.056917)  time: 0.481844  data: 0.443712  max mem: 211\n",
      "Epoch: [3]  [ 420/2012]  eta: 0:13:04  lr: 0.000000  loss: 0.007350 (0.055340)  time: 0.475830  data: 0.437690  max mem: 211\n",
      "Epoch: [3]  [ 440/2012]  eta: 0:12:52  lr: 0.000000  loss: 0.033942 (0.055380)  time: 0.469105  data: 0.430973  max mem: 211\n",
      "Epoch: [3]  [ 460/2012]  eta: 0:12:43  lr: 0.000000  loss: 0.010317 (0.054829)  time: 0.495944  data: 0.457836  max mem: 211\n",
      "Epoch: [3]  [ 480/2012]  eta: 0:12:33  lr: 0.000000  loss: 0.022922 (0.054989)  time: 0.493474  data: 0.455350  max mem: 211\n",
      "Epoch: [3]  [ 500/2012]  eta: 0:12:23  lr: 0.000000  loss: 0.066588 (0.056650)  time: 0.486945  data: 0.448796  max mem: 211\n",
      "Epoch: [3]  [ 520/2012]  eta: 0:12:13  lr: 0.000000  loss: 0.018139 (0.056882)  time: 0.482788  data: 0.444658  max mem: 211\n",
      "Epoch: [3]  [ 540/2012]  eta: 0:12:06  lr: 0.000000  loss: 0.029950 (0.056836)  time: 0.544125  data: 0.505957  max mem: 211\n",
      "Epoch: [3]  [ 560/2012]  eta: 0:11:58  lr: 0.000000  loss: 0.020898 (0.056736)  time: 0.532191  data: 0.494082  max mem: 211\n",
      "Epoch: [3]  [ 580/2012]  eta: 0:11:46  lr: 0.000000  loss: 0.036453 (0.057247)  time: 0.460001  data: 0.421975  max mem: 211\n",
      "Epoch: [3]  [ 600/2012]  eta: 0:11:35  lr: 0.000000  loss: 0.013033 (0.056416)  time: 0.472787  data: 0.434727  max mem: 211\n",
      "Epoch: [3]  [ 620/2012]  eta: 0:11:25  lr: 0.000000  loss: 0.014908 (0.056580)  time: 0.492280  data: 0.454142  max mem: 211\n",
      "Epoch: [3]  [ 640/2012]  eta: 0:11:15  lr: 0.000000  loss: 0.010955 (0.056227)  time: 0.481630  data: 0.443514  max mem: 211\n",
      "Epoch: [3]  [ 660/2012]  eta: 0:11:05  lr: 0.000000  loss: 0.012592 (0.055304)  time: 0.495142  data: 0.456981  max mem: 211\n",
      "Epoch: [3]  [ 680/2012]  eta: 0:10:55  lr: 0.000000  loss: 0.054012 (0.056015)  time: 0.478050  data: 0.439923  max mem: 211\n",
      "Epoch: [3]  [ 700/2012]  eta: 0:10:44  lr: 0.000000  loss: 0.067475 (0.057059)  time: 0.468603  data: 0.430468  max mem: 211\n",
      "Epoch: [3]  [ 720/2012]  eta: 0:10:34  lr: 0.000000  loss: 0.020208 (0.057309)  time: 0.486572  data: 0.448435  max mem: 211\n",
      "Epoch: [3]  [ 740/2012]  eta: 0:10:24  lr: 0.000000  loss: 0.031100 (0.057090)  time: 0.481854  data: 0.443700  max mem: 211\n",
      "Epoch: [3]  [ 760/2012]  eta: 0:10:15  lr: 0.000000  loss: 0.017383 (0.056447)  time: 0.520957  data: 0.482842  max mem: 211\n",
      "Epoch: [3]  [ 780/2012]  eta: 0:10:05  lr: 0.000000  loss: 0.023912 (0.056289)  time: 0.493378  data: 0.455251  max mem: 211\n",
      "Epoch: [3]  [ 800/2012]  eta: 0:09:55  lr: 0.000000  loss: 0.017862 (0.056529)  time: 0.481905  data: 0.443767  max mem: 211\n",
      "Epoch: [3]  [ 820/2012]  eta: 0:09:45  lr: 0.000000  loss: 0.050289 (0.056430)  time: 0.489566  data: 0.451429  max mem: 211\n",
      "Epoch: [3]  [ 840/2012]  eta: 0:09:36  lr: 0.000000  loss: 0.021370 (0.056560)  time: 0.501692  data: 0.463580  max mem: 211\n",
      "Epoch: [3]  [ 860/2012]  eta: 0:09:27  lr: 0.000000  loss: 0.023749 (0.056641)  time: 0.521589  data: 0.483470  max mem: 211\n",
      "Epoch: [3]  [ 880/2012]  eta: 0:09:17  lr: 0.000000  loss: 0.021738 (0.056740)  time: 0.493061  data: 0.454954  max mem: 211\n",
      "Epoch: [3]  [ 900/2012]  eta: 0:09:08  lr: 0.000000  loss: 0.020590 (0.056353)  time: 0.516601  data: 0.478471  max mem: 211\n",
      "Epoch: [3]  [ 920/2012]  eta: 0:08:58  lr: 0.000000  loss: 0.036567 (0.056661)  time: 0.487136  data: 0.449004  max mem: 211\n",
      "Epoch: [3]  [ 940/2012]  eta: 0:08:49  lr: 0.000000  loss: 0.020678 (0.056174)  time: 0.531707  data: 0.493611  max mem: 211\n",
      "Epoch: [3]  [ 960/2012]  eta: 0:08:38  lr: 0.000000  loss: 0.055749 (0.056865)  time: 0.470245  data: 0.432096  max mem: 211\n",
      "Epoch: [3]  [ 980/2012]  eta: 0:08:28  lr: 0.000000  loss: 0.040700 (0.057096)  time: 0.487578  data: 0.449443  max mem: 211\n",
      "Epoch: [3]  [1000/2012]  eta: 0:08:20  lr: 0.000000  loss: 0.025287 (0.057140)  time: 0.565441  data: 0.527308  max mem: 211\n",
      "Epoch: [3]  [1020/2012]  eta: 0:08:10  lr: 0.000000  loss: 0.036310 (0.057662)  time: 0.464563  data: 0.426509  max mem: 211\n",
      "Epoch: [3]  [1040/2012]  eta: 0:07:59  lr: 0.000000  loss: 0.014531 (0.057241)  time: 0.452833  data: 0.414763  max mem: 211\n",
      "Epoch: [3]  [1060/2012]  eta: 0:07:49  lr: 0.000000  loss: 0.040839 (0.057269)  time: 0.502075  data: 0.464012  max mem: 211\n",
      "Epoch: [3]  [1080/2012]  eta: 0:07:39  lr: 0.000000  loss: 0.005158 (0.057187)  time: 0.473683  data: 0.435542  max mem: 211\n",
      "Epoch: [3]  [1100/2012]  eta: 0:07:29  lr: 0.000000  loss: 0.014396 (0.057011)  time: 0.481601  data: 0.443489  max mem: 211\n",
      "Epoch: [3]  [1120/2012]  eta: 0:07:19  lr: 0.000000  loss: 0.011872 (0.057394)  time: 0.467606  data: 0.429474  max mem: 211\n",
      "Epoch: [3]  [1140/2012]  eta: 0:07:09  lr: 0.000000  loss: 0.036419 (0.057518)  time: 0.491467  data: 0.453296  max mem: 211\n",
      "Epoch: [3]  [1160/2012]  eta: 0:07:00  lr: 0.000000  loss: 0.019895 (0.057384)  time: 0.554222  data: 0.516092  max mem: 211\n",
      "Epoch: [3]  [1180/2012]  eta: 0:06:50  lr: 0.000000  loss: 0.012028 (0.056959)  time: 0.522108  data: 0.483996  max mem: 211\n",
      "Epoch: [3]  [1200/2012]  eta: 0:06:41  lr: 0.000000  loss: 0.058910 (0.057431)  time: 0.525749  data: 0.487622  max mem: 211\n",
      "Epoch: [3]  [1220/2012]  eta: 0:06:31  lr: 0.000000  loss: 0.015148 (0.057453)  time: 0.487047  data: 0.448915  max mem: 211\n",
      "Epoch: [3]  [1240/2012]  eta: 0:06:22  lr: 0.000000  loss: 0.015378 (0.057388)  time: 0.540472  data: 0.502347  max mem: 211\n",
      "Epoch: [3]  [1260/2012]  eta: 0:06:12  lr: 0.000000  loss: 0.010592 (0.057357)  time: 0.477414  data: 0.439314  max mem: 211\n",
      "Epoch: [3]  [1280/2012]  eta: 0:06:02  lr: 0.000000  loss: 0.016861 (0.057168)  time: 0.517397  data: 0.479290  max mem: 211\n",
      "Epoch: [3]  [1300/2012]  eta: 0:05:52  lr: 0.000000  loss: 0.009491 (0.056845)  time: 0.483400  data: 0.445272  max mem: 211\n",
      "Epoch: [3]  [1320/2012]  eta: 0:05:42  lr: 0.000000  loss: 0.026272 (0.056814)  time: 0.491488  data: 0.453362  max mem: 211\n",
      "Epoch: [3]  [1340/2012]  eta: 0:05:32  lr: 0.000000  loss: 0.025679 (0.056649)  time: 0.515898  data: 0.477777  max mem: 211\n",
      "Epoch: [3]  [1360/2012]  eta: 0:05:23  lr: 0.000000  loss: 0.024966 (0.056505)  time: 0.549213  data: 0.511085  max mem: 211\n",
      "Epoch: [3]  [1380/2012]  eta: 0:05:13  lr: 0.000000  loss: 0.009605 (0.056499)  time: 0.496015  data: 0.457901  max mem: 211\n",
      "Epoch: [3]  [1400/2012]  eta: 0:05:03  lr: 0.000000  loss: 0.007985 (0.056194)  time: 0.475873  data: 0.437766  max mem: 211\n",
      "Epoch: [3]  [1420/2012]  eta: 0:04:53  lr: 0.000000  loss: 0.030790 (0.056965)  time: 0.496044  data: 0.457929  max mem: 211\n",
      "Epoch: [3]  [1440/2012]  eta: 0:04:43  lr: 0.000000  loss: 0.015318 (0.056683)  time: 0.481896  data: 0.443729  max mem: 211\n",
      "Epoch: [3]  [1460/2012]  eta: 0:04:33  lr: 0.000000  loss: 0.019720 (0.056383)  time: 0.533903  data: 0.495798  max mem: 211\n",
      "Epoch: [3]  [1480/2012]  eta: 0:04:23  lr: 0.000000  loss: 0.024277 (0.056284)  time: 0.452448  data: 0.414386  max mem: 211\n",
      "Epoch: [3]  [1500/2012]  eta: 0:04:13  lr: 0.000000  loss: 0.029838 (0.056079)  time: 0.461863  data: 0.423823  max mem: 211\n",
      "Epoch: [3]  [1520/2012]  eta: 0:04:03  lr: 0.000000  loss: 0.016986 (0.055970)  time: 0.481652  data: 0.443534  max mem: 211\n",
      "Epoch: [3]  [1540/2012]  eta: 0:03:53  lr: 0.000000  loss: 0.032077 (0.055872)  time: 0.479930  data: 0.441784  max mem: 211\n",
      "Epoch: [3]  [1560/2012]  eta: 0:03:43  lr: 0.000000  loss: 0.019866 (0.055941)  time: 0.553900  data: 0.515786  max mem: 211\n",
      "Epoch: [3]  [1580/2012]  eta: 0:03:34  lr: 0.000000  loss: 0.026858 (0.056402)  time: 0.519036  data: 0.480929  max mem: 211\n",
      "Epoch: [3]  [1600/2012]  eta: 0:03:24  lr: 0.000000  loss: 0.023661 (0.056265)  time: 0.529105  data: 0.490963  max mem: 211\n",
      "Epoch: [3]  [1620/2012]  eta: 0:03:14  lr: 0.000000  loss: 0.028841 (0.056115)  time: 0.509634  data: 0.471506  max mem: 211\n",
      "Epoch: [3]  [1640/2012]  eta: 0:03:04  lr: 0.000000  loss: 0.029731 (0.056238)  time: 0.486424  data: 0.448319  max mem: 211\n",
      "Epoch: [3]  [1660/2012]  eta: 0:02:54  lr: 0.000000  loss: 0.032154 (0.056262)  time: 0.475841  data: 0.437740  max mem: 211\n",
      "Epoch: [3]  [1680/2012]  eta: 0:02:44  lr: 0.000000  loss: 0.016911 (0.056048)  time: 0.467086  data: 0.428943  max mem: 211\n",
      "Epoch: [3]  [1700/2012]  eta: 0:02:34  lr: 0.000000  loss: 0.025945 (0.056039)  time: 0.503928  data: 0.465808  max mem: 211\n",
      "Epoch: [3]  [1720/2012]  eta: 0:02:24  lr: 0.000000  loss: 0.018659 (0.055798)  time: 0.537077  data: 0.498927  max mem: 211\n",
      "Epoch: [3]  [1740/2012]  eta: 0:02:14  lr: 0.000000  loss: 0.026275 (0.055964)  time: 0.485901  data: 0.447781  max mem: 211\n",
      "Epoch: [3]  [1760/2012]  eta: 0:02:04  lr: 0.000000  loss: 0.019765 (0.055994)  time: 0.485053  data: 0.446927  max mem: 211\n",
      "Epoch: [3]  [1780/2012]  eta: 0:01:55  lr: 0.000000  loss: 0.024555 (0.056216)  time: 0.528430  data: 0.490307  max mem: 211\n",
      "Epoch: [3]  [1800/2012]  eta: 0:01:45  lr: 0.000000  loss: 0.011816 (0.056000)  time: 0.505385  data: 0.467284  max mem: 211\n",
      "Epoch: [3]  [1820/2012]  eta: 0:01:35  lr: 0.000000  loss: 0.015423 (0.055872)  time: 0.495031  data: 0.456942  max mem: 211\n",
      "Epoch: [3]  [1840/2012]  eta: 0:01:25  lr: 0.000000  loss: 0.024642 (0.055960)  time: 0.475618  data: 0.437470  max mem: 211\n",
      "Epoch: [3]  [1860/2012]  eta: 0:01:15  lr: 0.000000  loss: 0.028956 (0.056100)  time: 0.470932  data: 0.432810  max mem: 211\n",
      "Epoch: [3]  [1880/2012]  eta: 0:01:05  lr: 0.000000  loss: 0.031389 (0.056104)  time: 0.471567  data: 0.433440  max mem: 211\n",
      "Epoch: [3]  [1900/2012]  eta: 0:00:55  lr: 0.000000  loss: 0.019559 (0.055802)  time: 0.537279  data: 0.499146  max mem: 211\n",
      "Epoch: [3]  [1920/2012]  eta: 0:00:45  lr: 0.000000  loss: 0.024595 (0.055556)  time: 0.512752  data: 0.474678  max mem: 211\n",
      "Epoch: [3]  [1940/2012]  eta: 0:00:35  lr: 0.000000  loss: 0.011221 (0.055276)  time: 0.529335  data: 0.491295  max mem: 211\n",
      "Epoch: [3]  [1960/2012]  eta: 0:00:25  lr: 0.000000  loss: 0.012080 (0.055450)  time: 0.521597  data: 0.483578  max mem: 211\n",
      "Epoch: [3]  [1980/2012]  eta: 0:00:15  lr: 0.000000  loss: 0.032499 (0.055303)  time: 0.533256  data: 0.495097  max mem: 211\n",
      "Epoch: [3]  [2000/2012]  eta: 0:00:05  lr: 0.000000  loss: 0.014571 (0.054978)  time: 0.511535  data: 0.473386  max mem: 211\n",
      "Epoch: [3]  [2011/2012]  eta: 0:00:00  lr: 0.000000  loss: 0.022406 (0.054966)  time: 0.476461  data: 0.439618  max mem: 211\n",
      "Epoch: [3] Total time: 0:16:40 (0.497086 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.022406 (0.054966)\n",
      "Test:  [ 0/79]  eta: 0:00:38  loss: 0.006180 (0.006180)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.488317  data: 0.450113  max mem: 211\n",
      "Test:  [20/79]  eta: 0:00:32  loss: 0.064749 (0.105674)  acc1: 93.750000 (96.726190)  acc5: 100.000000 (100.000000)  time: 0.547606  data: 0.509498  max mem: 211\n",
      "Test:  [40/79]  eta: 0:00:21  loss: 0.057951 (0.084190)  acc1: 100.000000 (97.256098)  acc5: 100.000000 (100.000000)  time: 0.535859  data: 0.497712  max mem: 211\n",
      "Test:  [60/79]  eta: 0:00:10  loss: 0.068573 (0.089204)  acc1: 100.000000 (97.028689)  acc5: 100.000000 (100.000000)  time: 0.504568  data: 0.466430  max mem: 211\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 0.017033 (0.085295)  acc1: 100.000000 (97.600000)  acc5: 100.000000 (100.000000)  time: 0.473925  data: 0.437110  max mem: 211\n",
      "Test: Total time: 0:00:40 (0.516050 s / it)\n",
      "* Acc@1 97.600 Acc@5 100.000 loss 0.085\n",
      "Accuracy at epoch 3 of the network on the 79 test images: 97.6%\n",
      "Accuracy at epoch 3 of the network on the 1250 test images: 97.6%\n",
      "Max accuracy so far: 97.60%\n",
      "Epoch: [4]  [   0/2012]  eta: 0:15:29  lr: 0.000063  loss: 0.003600 (0.003600)  time: 0.462043  data: 0.423894  max mem: 211\n",
      "Epoch: [4]  [  20/2012]  eta: 0:16:17  lr: 0.000063  loss: 0.009492 (0.037619)  time: 0.492328  data: 0.454170  max mem: 211\n",
      "Epoch: [4]  [  40/2012]  eta: 0:16:15  lr: 0.000063  loss: 0.028145 (0.051514)  time: 0.498349  data: 0.460196  max mem: 211\n",
      "Epoch: [4]  [  60/2012]  eta: 0:16:13  lr: 0.000063  loss: 0.015275 (0.060694)  time: 0.507618  data: 0.469494  max mem: 211\n",
      "Epoch: [4]  [  80/2012]  eta: 0:15:53  lr: 0.000063  loss: 0.033200 (0.061361)  time: 0.477290  data: 0.439190  max mem: 211\n",
      "Epoch: [4]  [ 100/2012]  eta: 0:15:48  lr: 0.000063  loss: 0.012312 (0.054097)  time: 0.506314  data: 0.468215  max mem: 211\n",
      "Epoch: [4]  [ 120/2012]  eta: 0:15:36  lr: 0.000063  loss: 0.026422 (0.055009)  time: 0.491174  data: 0.453054  max mem: 211\n",
      "Epoch: [4]  [ 140/2012]  eta: 0:15:26  lr: 0.000063  loss: 0.034937 (0.057157)  time: 0.492172  data: 0.454056  max mem: 211\n",
      "Epoch: [4]  [ 160/2012]  eta: 0:15:32  lr: 0.000063  loss: 0.021426 (0.055744)  time: 0.565884  data: 0.527760  max mem: 211\n",
      "Epoch: [4]  [ 180/2012]  eta: 0:15:14  lr: 0.000063  loss: 0.018799 (0.055746)  time: 0.464596  data: 0.426467  max mem: 211\n",
      "Epoch: [4]  [ 200/2012]  eta: 0:14:59  lr: 0.000063  loss: 0.010096 (0.053638)  time: 0.472757  data: 0.434623  max mem: 211\n",
      "Epoch: [4]  [ 220/2012]  eta: 0:14:47  lr: 0.000063  loss: 0.033122 (0.053651)  time: 0.478165  data: 0.440059  max mem: 211\n",
      "Epoch: [4]  [ 240/2012]  eta: 0:14:42  lr: 0.000063  loss: 0.023429 (0.052342)  time: 0.533465  data: 0.495332  max mem: 211\n",
      "Epoch: [4]  [ 260/2012]  eta: 0:14:49  lr: 0.000063  loss: 0.031591 (0.051004)  time: 0.619963  data: 0.581822  max mem: 211\n",
      "Epoch: [4]  [ 280/2012]  eta: 0:14:37  lr: 0.000063  loss: 0.023539 (0.052159)  time: 0.498725  data: 0.460672  max mem: 211\n",
      "Epoch: [4]  [ 300/2012]  eta: 0:14:32  lr: 0.000063  loss: 0.030350 (0.051494)  time: 0.549176  data: 0.511140  max mem: 211\n",
      "Epoch: [4]  [ 320/2012]  eta: 0:14:19  lr: 0.000063  loss: 0.012550 (0.051408)  time: 0.484111  data: 0.446030  max mem: 211\n",
      "Epoch: [4]  [ 340/2012]  eta: 0:14:07  lr: 0.000063  loss: 0.017149 (0.051692)  time: 0.487002  data: 0.448847  max mem: 211\n",
      "Epoch: [4]  [ 360/2012]  eta: 0:14:05  lr: 0.000063  loss: 0.015036 (0.052064)  time: 0.592168  data: 0.554022  max mem: 211\n",
      "Epoch: [4]  [ 380/2012]  eta: 0:13:53  lr: 0.000063  loss: 0.008271 (0.051987)  time: 0.491264  data: 0.453146  max mem: 211\n",
      "Epoch: [4]  [ 400/2012]  eta: 0:13:40  lr: 0.000063  loss: 0.036893 (0.052437)  time: 0.479674  data: 0.441578  max mem: 211\n",
      "Epoch: [4]  [ 420/2012]  eta: 0:13:27  lr: 0.000063  loss: 0.006644 (0.050967)  time: 0.476266  data: 0.438156  max mem: 211\n",
      "Epoch: [4]  [ 440/2012]  eta: 0:13:14  lr: 0.000063  loss: 0.029580 (0.050857)  time: 0.468887  data: 0.430755  max mem: 211\n",
      "Epoch: [4]  [ 460/2012]  eta: 0:13:03  lr: 0.000063  loss: 0.009167 (0.050751)  time: 0.490476  data: 0.452368  max mem: 211\n",
      "Epoch: [4]  [ 480/2012]  eta: 0:12:51  lr: 0.000063  loss: 0.029968 (0.051210)  time: 0.467114  data: 0.429004  max mem: 211\n",
      "Epoch: [4]  [ 500/2012]  eta: 0:12:43  lr: 0.000063  loss: 0.035518 (0.052810)  time: 0.536264  data: 0.498121  max mem: 211\n",
      "Epoch: [4]  [ 520/2012]  eta: 0:12:31  lr: 0.000063  loss: 0.023884 (0.053184)  time: 0.468942  data: 0.430793  max mem: 211\n",
      "Epoch: [4]  [ 540/2012]  eta: 0:12:19  lr: 0.000063  loss: 0.021676 (0.053238)  time: 0.477753  data: 0.439626  max mem: 211\n",
      "Epoch: [4]  [ 560/2012]  eta: 0:12:08  lr: 0.000063  loss: 0.028069 (0.053506)  time: 0.473160  data: 0.435021  max mem: 211\n",
      "Epoch: [4]  [ 580/2012]  eta: 0:11:56  lr: 0.000063  loss: 0.042072 (0.053932)  time: 0.472390  data: 0.434261  max mem: 211\n",
      "Epoch: [4]  [ 600/2012]  eta: 0:11:44  lr: 0.000063  loss: 0.009897 (0.053129)  time: 0.466297  data: 0.428163  max mem: 211\n",
      "Epoch: [4]  [ 620/2012]  eta: 0:11:34  lr: 0.000063  loss: 0.026515 (0.053439)  time: 0.488735  data: 0.450612  max mem: 211\n",
      "Epoch: [4]  [ 640/2012]  eta: 0:11:23  lr: 0.000063  loss: 0.010954 (0.053293)  time: 0.477122  data: 0.439014  max mem: 211\n",
      "Epoch: [4]  [ 660/2012]  eta: 0:11:12  lr: 0.000063  loss: 0.008750 (0.052313)  time: 0.474459  data: 0.436337  max mem: 211\n",
      "Epoch: [4]  [ 680/2012]  eta: 0:11:01  lr: 0.000063  loss: 0.043976 (0.053289)  time: 0.465029  data: 0.426782  max mem: 211\n",
      "Epoch: [4]  [ 700/2012]  eta: 0:10:51  lr: 0.000063  loss: 0.045588 (0.054465)  time: 0.487333  data: 0.449220  max mem: 211\n",
      "Epoch: [4]  [ 720/2012]  eta: 0:10:42  lr: 0.000063  loss: 0.016254 (0.054693)  time: 0.519697  data: 0.481579  max mem: 211\n",
      "Epoch: [4]  [ 740/2012]  eta: 0:10:30  lr: 0.000063  loss: 0.022660 (0.054329)  time: 0.462725  data: 0.424643  max mem: 211\n",
      "Epoch: [4]  [ 760/2012]  eta: 0:10:22  lr: 0.000063  loss: 0.010576 (0.053642)  time: 0.546028  data: 0.507990  max mem: 211\n",
      "Epoch: [4]  [ 780/2012]  eta: 0:10:12  lr: 0.000063  loss: 0.014388 (0.053456)  time: 0.493275  data: 0.455212  max mem: 211\n",
      "Epoch: [4]  [ 800/2012]  eta: 0:10:04  lr: 0.000063  loss: 0.016072 (0.053727)  time: 0.571169  data: 0.533019  max mem: 211\n",
      "Epoch: [4]  [ 820/2012]  eta: 0:09:54  lr: 0.000063  loss: 0.028679 (0.053663)  time: 0.487460  data: 0.449331  max mem: 211\n",
      "Epoch: [4]  [ 840/2012]  eta: 0:09:44  lr: 0.000063  loss: 0.022200 (0.053707)  time: 0.508056  data: 0.469938  max mem: 211\n",
      "Epoch: [4]  [ 860/2012]  eta: 0:09:34  lr: 0.000063  loss: 0.024343 (0.054137)  time: 0.503487  data: 0.465364  max mem: 211\n",
      "Epoch: [4]  [ 880/2012]  eta: 0:09:25  lr: 0.000063  loss: 0.022653 (0.054200)  time: 0.509791  data: 0.471665  max mem: 211\n",
      "Epoch: [4]  [ 900/2012]  eta: 0:09:16  lr: 0.000063  loss: 0.029308 (0.053877)  time: 0.555469  data: 0.517340  max mem: 211\n",
      "Epoch: [4]  [ 920/2012]  eta: 0:09:06  lr: 0.000063  loss: 0.039394 (0.054233)  time: 0.494508  data: 0.456403  max mem: 211\n",
      "Epoch: [4]  [ 940/2012]  eta: 0:08:56  lr: 0.000063  loss: 0.012745 (0.053659)  time: 0.481901  data: 0.443791  max mem: 211\n",
      "Epoch: [4]  [ 960/2012]  eta: 0:08:45  lr: 0.000063  loss: 0.032216 (0.053933)  time: 0.468848  data: 0.430686  max mem: 211\n",
      "Epoch: [4]  [ 980/2012]  eta: 0:08:35  lr: 0.000063  loss: 0.015984 (0.053673)  time: 0.521484  data: 0.483368  max mem: 211\n",
      "Epoch: [4]  [1000/2012]  eta: 0:08:25  lr: 0.000063  loss: 0.021394 (0.053867)  time: 0.487125  data: 0.449026  max mem: 211\n",
      "Epoch: [4]  [1020/2012]  eta: 0:08:15  lr: 0.000063  loss: 0.029352 (0.054693)  time: 0.477545  data: 0.439435  max mem: 211\n",
      "Epoch: [4]  [1040/2012]  eta: 0:08:04  lr: 0.000063  loss: 0.012560 (0.054342)  time: 0.483418  data: 0.445255  max mem: 211\n",
      "Epoch: [4]  [1060/2012]  eta: 0:07:54  lr: 0.000063  loss: 0.028478 (0.054151)  time: 0.487565  data: 0.449442  max mem: 211\n",
      "Epoch: [4]  [1080/2012]  eta: 0:07:44  lr: 0.000063  loss: 0.005226 (0.054077)  time: 0.498183  data: 0.460051  max mem: 211\n",
      "Epoch: [4]  [1100/2012]  eta: 0:07:34  lr: 0.000063  loss: 0.016802 (0.053962)  time: 0.472907  data: 0.434793  max mem: 211\n",
      "Epoch: [4]  [1120/2012]  eta: 0:07:24  lr: 0.000063  loss: 0.011477 (0.054498)  time: 0.484753  data: 0.446639  max mem: 211\n",
      "Epoch: [4]  [1140/2012]  eta: 0:07:15  lr: 0.000063  loss: 0.031665 (0.054673)  time: 0.568509  data: 0.530371  max mem: 211\n",
      "Epoch: [4]  [1160/2012]  eta: 0:07:05  lr: 0.000063  loss: 0.017599 (0.054571)  time: 0.501365  data: 0.463231  max mem: 211\n",
      "Epoch: [4]  [1180/2012]  eta: 0:06:55  lr: 0.000063  loss: 0.014306 (0.054006)  time: 0.529609  data: 0.491480  max mem: 211\n",
      "Epoch: [4]  [1200/2012]  eta: 0:06:46  lr: 0.000063  loss: 0.037315 (0.054340)  time: 0.518485  data: 0.480454  max mem: 211\n",
      "Epoch: [4]  [1220/2012]  eta: 0:06:35  lr: 0.000063  loss: 0.009919 (0.054272)  time: 0.490570  data: 0.452532  max mem: 211\n",
      "Epoch: [4]  [1240/2012]  eta: 0:06:26  lr: 0.000063  loss: 0.015122 (0.054316)  time: 0.516221  data: 0.478072  max mem: 211\n",
      "Epoch: [4]  [1260/2012]  eta: 0:06:16  lr: 0.000063  loss: 0.011163 (0.054342)  time: 0.523048  data: 0.484908  max mem: 211\n",
      "Epoch: [4]  [1280/2012]  eta: 0:06:06  lr: 0.000063  loss: 0.014179 (0.054027)  time: 0.470870  data: 0.432728  max mem: 211\n",
      "Epoch: [4]  [1300/2012]  eta: 0:05:55  lr: 0.000063  loss: 0.008473 (0.053682)  time: 0.489631  data: 0.451485  max mem: 211\n",
      "Epoch: [4]  [1320/2012]  eta: 0:05:46  lr: 0.000063  loss: 0.018787 (0.053558)  time: 0.513623  data: 0.475501  max mem: 211\n",
      "Epoch: [4]  [1340/2012]  eta: 0:05:35  lr: 0.000063  loss: 0.019410 (0.053487)  time: 0.470379  data: 0.432275  max mem: 211\n",
      "Epoch: [4]  [1360/2012]  eta: 0:05:25  lr: 0.000063  loss: 0.011468 (0.053338)  time: 0.467526  data: 0.429374  max mem: 211\n",
      "Epoch: [4]  [1380/2012]  eta: 0:05:15  lr: 0.000063  loss: 0.004870 (0.053404)  time: 0.499062  data: 0.460945  max mem: 211\n",
      "Epoch: [4]  [1400/2012]  eta: 0:05:05  lr: 0.000063  loss: 0.009009 (0.053168)  time: 0.467627  data: 0.429456  max mem: 211\n",
      "Epoch: [4]  [1420/2012]  eta: 0:04:54  lr: 0.000063  loss: 0.034312 (0.054224)  time: 0.464274  data: 0.426129  max mem: 211\n",
      "Epoch: [4]  [1440/2012]  eta: 0:04:44  lr: 0.000063  loss: 0.015006 (0.053945)  time: 0.473903  data: 0.435769  max mem: 211\n",
      "Epoch: [4]  [1460/2012]  eta: 0:04:34  lr: 0.000063  loss: 0.018036 (0.053597)  time: 0.470738  data: 0.432597  max mem: 211\n",
      "Epoch: [4]  [1480/2012]  eta: 0:04:24  lr: 0.000063  loss: 0.023375 (0.053422)  time: 0.510014  data: 0.471900  max mem: 211\n",
      "Epoch: [4]  [1500/2012]  eta: 0:04:15  lr: 0.000063  loss: 0.010209 (0.053136)  time: 0.602873  data: 0.564764  max mem: 211\n",
      "Epoch: [4]  [1520/2012]  eta: 0:04:05  lr: 0.000063  loss: 0.015585 (0.053099)  time: 0.536510  data: 0.498365  max mem: 211\n",
      "Epoch: [4]  [1540/2012]  eta: 0:03:55  lr: 0.000063  loss: 0.022277 (0.053042)  time: 0.510499  data: 0.472372  max mem: 211\n",
      "Epoch: [4]  [1560/2012]  eta: 0:03:46  lr: 0.000063  loss: 0.023127 (0.053242)  time: 0.546805  data: 0.508674  max mem: 211\n",
      "Epoch: [4]  [1580/2012]  eta: 0:03:36  lr: 0.000063  loss: 0.021125 (0.053766)  time: 0.527376  data: 0.489265  max mem: 211\n",
      "Epoch: [4]  [1600/2012]  eta: 0:03:26  lr: 0.000063  loss: 0.021988 (0.053564)  time: 0.514026  data: 0.475882  max mem: 211\n",
      "Epoch: [4]  [1620/2012]  eta: 0:03:16  lr: 0.000063  loss: 0.018096 (0.053425)  time: 0.477377  data: 0.439282  max mem: 211\n",
      "Epoch: [4]  [1640/2012]  eta: 0:03:06  lr: 0.000063  loss: 0.027763 (0.053506)  time: 0.461952  data: 0.423879  max mem: 211\n",
      "Epoch: [4]  [1660/2012]  eta: 0:02:55  lr: 0.000063  loss: 0.029215 (0.053415)  time: 0.467867  data: 0.429786  max mem: 211\n",
      "Epoch: [4]  [1680/2012]  eta: 0:02:45  lr: 0.000063  loss: 0.006132 (0.053223)  time: 0.465425  data: 0.427401  max mem: 211\n",
      "Epoch: [4]  [1700/2012]  eta: 0:02:35  lr: 0.000063  loss: 0.033552 (0.053345)  time: 0.483969  data: 0.445805  max mem: 211\n",
      "Epoch: [4]  [1720/2012]  eta: 0:02:25  lr: 0.000063  loss: 0.018446 (0.053174)  time: 0.498591  data: 0.460467  max mem: 211\n",
      "Epoch: [4]  [1740/2012]  eta: 0:02:15  lr: 0.000063  loss: 0.021073 (0.053392)  time: 0.527865  data: 0.489754  max mem: 211\n",
      "Epoch: [4]  [1760/2012]  eta: 0:02:05  lr: 0.000063  loss: 0.012688 (0.053386)  time: 0.488265  data: 0.450158  max mem: 211\n",
      "Epoch: [4]  [1780/2012]  eta: 0:01:55  lr: 0.000063  loss: 0.023648 (0.053700)  time: 0.537364  data: 0.499232  max mem: 211\n",
      "Epoch: [4]  [1800/2012]  eta: 0:01:46  lr: 0.000063  loss: 0.007668 (0.053445)  time: 0.527048  data: 0.488931  max mem: 211\n",
      "Epoch: [4]  [1820/2012]  eta: 0:01:35  lr: 0.000063  loss: 0.030751 (0.053525)  time: 0.489409  data: 0.451261  max mem: 211\n",
      "Epoch: [4]  [1840/2012]  eta: 0:01:26  lr: 0.000063  loss: 0.018717 (0.053743)  time: 0.510461  data: 0.472348  max mem: 211\n",
      "Epoch: [4]  [1860/2012]  eta: 0:01:15  lr: 0.000063  loss: 0.028200 (0.053882)  time: 0.471443  data: 0.433301  max mem: 211\n",
      "Epoch: [4]  [1880/2012]  eta: 0:01:05  lr: 0.000063  loss: 0.022269 (0.053893)  time: 0.491378  data: 0.453235  max mem: 211\n",
      "Epoch: [4]  [1900/2012]  eta: 0:00:55  lr: 0.000063  loss: 0.013572 (0.053646)  time: 0.491302  data: 0.453167  max mem: 211\n",
      "Epoch: [4]  [1920/2012]  eta: 0:00:45  lr: 0.000063  loss: 0.018515 (0.053472)  time: 0.523858  data: 0.485713  max mem: 211\n",
      "Epoch: [4]  [1940/2012]  eta: 0:00:35  lr: 0.000063  loss: 0.014792 (0.053265)  time: 0.469240  data: 0.431115  max mem: 211\n",
      "Epoch: [4]  [1960/2012]  eta: 0:00:25  lr: 0.000063  loss: 0.009645 (0.053542)  time: 0.469107  data: 0.430981  max mem: 211\n",
      "Epoch: [4]  [1980/2012]  eta: 0:00:15  lr: 0.000063  loss: 0.029118 (0.053558)  time: 0.465374  data: 0.427249  max mem: 211\n",
      "Epoch: [4]  [2000/2012]  eta: 0:00:05  lr: 0.000063  loss: 0.016491 (0.053302)  time: 0.477235  data: 0.439102  max mem: 211\n",
      "Epoch: [4]  [2011/2012]  eta: 0:00:00  lr: 0.000063  loss: 0.023966 (0.053332)  time: 0.469810  data: 0.432988  max mem: 211\n",
      "Epoch: [4] Total time: 0:16:43 (0.498526 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 0.023966 (0.053332)\n",
      "Test:  [ 0/79]  eta: 0:00:37  loss: 0.003950 (0.003950)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.480791  data: 0.442617  max mem: 211\n",
      "Test:  [20/79]  eta: 0:00:31  loss: 0.060876 (0.102474)  acc1: 93.750000 (96.726190)  acc5: 100.000000 (100.000000)  time: 0.538307  data: 0.500164  max mem: 211\n",
      "Test:  [40/79]  eta: 0:00:20  loss: 0.038588 (0.078956)  acc1: 100.000000 (97.713415)  acc5: 100.000000 (100.000000)  time: 0.513151  data: 0.475019  max mem: 211\n",
      "Test:  [60/79]  eta: 0:00:09  loss: 0.061207 (0.084029)  acc1: 100.000000 (97.540984)  acc5: 100.000000 (100.000000)  time: 0.500913  data: 0.462762  max mem: 211\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 0.015308 (0.080008)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (100.000000)  time: 0.469882  data: 0.433146  max mem: 211\n",
      "Test: Total time: 0:00:39 (0.506188 s / it)\n",
      "* Acc@1 98.000 Acc@5 100.000 loss 0.080\n",
      "Accuracy at epoch 4 of the network on the 79 test images: 98.0%\n",
      "Accuracy at epoch 4 of the network on the 1250 test images: 98.0%\n",
      "Max accuracy so far: 98.00%\n",
      "Training of the supervised linear classifier on frozen features completed.\n",
      "Top-1 test accuracy: 98.0\n",
      "Finished Training, saving model to /cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes_full_v1/clean.pt and log to /cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes_full_v1/clean.pt\n"
     ]
    }
   ],
   "source": [
    "for attack, loaders in loader_dict.items():\n",
    "    \n",
    "    # Initialise classifier\n",
    "    adv_linear_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                         num_labels=len(CLASS_SUBSET))\n",
    "    adv_linear_classifier = adv_linear_classifier.cuda()\n",
    "\n",
    "    # Metric logger path\n",
    "    LOG_PATH = Path(LOG_BASE_PATH, 'adv_classifier', version, attack)\n",
    "    if not os.path.isdir(LOG_PATH):\n",
    "        os.makedirs(LOG_PATH)\n",
    "    \n",
    "    # train\n",
    "    pstr = \"#\"*50 + f''' Training classifier for {attack} ''' + \"#\"*50\n",
    "    print(len(pstr)*\"#\")\n",
    "    print(pstr)\n",
    "    print(len(pstr)*\"#\")\n",
    "    loggers = train(model, \n",
    "                    adv_linear_classifier, \n",
    "                    loaders[\"train\"], \n",
    "                    loaders[\"validation\"], \n",
    "                    log_dir=LOG_PATH, \n",
    "                    tensor_dir=None, \n",
    "                    optimizer=None, \n",
    "                    adversarial_attack=None,\n",
    "                    criterion=nn.CrossEntropyLoss(),\n",
    "                    epochs=5, \n",
    "                    val_freq=1, \n",
    "                    batch_size=16,  \n",
    "                    lr=0.001, \n",
    "                    to_restore = {\"epoch\": 0, \"best_acc\": 0.}, \n",
    "                    n=4, \n",
    "                    avgpool_patchtokens=False, \n",
    "                    show_image=False)\n",
    "    \n",
    "    # Save adversarial Classifier\n",
    "    save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version)\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    save_file_model = f\"{attack}.pt\"\n",
    "    save_file_log = f\"log_{attack}.pt\"\n",
    "    torch.save(adv_linear_classifier.state_dict(), str(save_path) + \"/\" + save_file_model)\n",
    "    torch.save(loggers, str(save_path) + \"/\" + save_file_log)\n",
    "    print(f'Finished Training, saving model to {str(save_path)}/{save_file_model} and log to {str(save_path)}/{save_file_model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on all adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################################################\n",
      "############################## evaluating adv_classifier trained on pgd ##############################\n",
      "######################################################################################################\n",
      "-------------------------------------------------- pgd dataset --------------------------------------------------\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.190948  data: 0.129737  max mem: 192\n",
      "Test:  [1/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.169447  data: 0.119832  max mem: 192\n",
      "Test:  [2/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.161091  data: 0.115359  max mem: 192\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.127296  data: 0.089931  max mem: 192\n",
      "Test: Total time: 0:00:00 (0.127893 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "-------------------------------------------------- cw dataset --------------------------------------------------\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000001 (0.000001)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.142073  data: 0.104120  max mem: 192\n",
      "Test:  [1/4]  eta: 0:00:00  loss: 0.000001 (0.000001)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.138418  data: 0.100321  max mem: 192\n",
      "Test:  [2/4]  eta: 0:00:00  loss: 0.000001 (0.000001)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.139801  data: 0.101739  max mem: 192\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.110389  data: 0.079090  max mem: 192\n",
      "Test: Total time: 0:00:00 (0.110947 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "-------------------------------------------------- fgsm dataset --------------------------------------------------\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.150275  data: 0.112258  max mem: 192\n",
      "Test:  [1/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.148908  data: 0.110905  max mem: 192\n",
      "Test:  [2/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.148614  data: 0.110548  max mem: 192\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.117627  data: 0.086505  max mem: 192\n",
      "Test: Total time: 0:00:00 (0.118239 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "-------------------------------------------------- clean dataset --------------------------------------------------\n",
      "Test:  [ 0/79]  eta: 0:00:18  loss: 11.795971 (11.795971)  acc1: 12.500000 (12.500000)  acc5: 25.000000 (25.000000)  time: 0.228471  data: 0.190242  max mem: 192\n",
      "Test:  [ 5/79]  eta: 0:00:16  loss: 11.018626 (11.406765)  acc1: 6.250000 (8.333333)  acc5: 18.750000 (20.833333)  time: 0.220927  data: 0.182855  max mem: 192\n",
      "Test:  [10/79]  eta: 0:00:15  loss: 11.580111 (11.467379)  acc1: 6.250000 (5.681818)  acc5: 18.750000 (20.454545)  time: 0.218670  data: 0.180592  max mem: 192\n",
      "Test:  [15/79]  eta: 0:00:13  loss: 11.580111 (11.560608)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (21.093750)  time: 0.216265  data: 0.178175  max mem: 192\n",
      "Test:  [20/79]  eta: 0:00:13  loss: 11.416915 (11.553357)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (21.428571)  time: 0.221763  data: 0.183689  max mem: 192\n",
      "Test:  [25/79]  eta: 0:00:12  loss: 11.416915 (11.531420)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (20.673077)  time: 0.232510  data: 0.194444  max mem: 192\n",
      "Test:  [30/79]  eta: 0:00:11  loss: 11.416915 (11.530670)  acc1: 0.000000 (3.830645)  acc5: 18.750000 (20.766129)  time: 0.237119  data: 0.199072  max mem: 192\n",
      "Test:  [35/79]  eta: 0:00:10  loss: 11.416915 (11.576890)  acc1: 0.000000 (3.993056)  acc5: 18.750000 (20.833333)  time: 0.238298  data: 0.200285  max mem: 192\n",
      "Test:  [40/79]  eta: 0:00:09  loss: 11.172411 (11.517733)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.426829)  time: 0.243454  data: 0.205450  max mem: 192\n",
      "Test:  [45/79]  eta: 0:00:07  loss: 11.421421 (11.511050)  acc1: 0.000000 (3.940217)  acc5: 18.750000 (19.565217)  time: 0.239013  data: 0.201007  max mem: 192\n",
      "Test:  [50/79]  eta: 0:00:06  loss: 10.739025 (11.458579)  acc1: 6.250000 (4.044118)  acc5: 18.750000 (20.465686)  time: 0.233494  data: 0.195482  max mem: 192\n",
      "Test:  [55/79]  eta: 0:00:05  loss: 11.313252 (11.433147)  acc1: 6.250000 (4.129464)  acc5: 12.500000 (20.200893)  time: 0.248588  data: 0.210573  max mem: 192\n",
      "Test:  [60/79]  eta: 0:00:04  loss: 11.313252 (11.424241)  acc1: 6.250000 (4.200820)  acc5: 12.500000 (20.696721)  time: 0.235688  data: 0.197681  max mem: 192\n",
      "Test:  [65/79]  eta: 0:00:03  loss: 11.313252 (11.414984)  acc1: 0.000000 (3.882576)  acc5: 18.750000 (20.454545)  time: 0.236014  data: 0.198008  max mem: 192\n",
      "Test:  [70/79]  eta: 0:00:02  loss: 11.385376 (11.439243)  acc1: 0.000000 (3.697183)  acc5: 12.500000 (19.894366)  time: 0.238977  data: 0.200968  max mem: 192\n",
      "Test:  [75/79]  eta: 0:00:00  loss: 11.385376 (11.404295)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (20.312500)  time: 0.227258  data: 0.189250  max mem: 192\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 11.222515 (11.402881)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (20.240000)  time: 0.219380  data: 0.182755  max mem: 192\n",
      "Test: Total time: 0:00:18 (0.230700 s / it)\n",
      "* Acc@1 4.000 Acc@5 20.240 loss 11.403\n",
      "#####################################################################################################\n",
      "############################## evaluating adv_classifier trained on cw ##############################\n",
      "#####################################################################################################\n",
      "-------------------------------------------------- pgd dataset --------------------------------------------------\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.082417  data: 0.044463  max mem: 192\n",
      "Test:  [1/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.082348  data: 0.044391  max mem: 192\n",
      "Test:  [2/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.082120  data: 0.044167  max mem: 192\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.065571  data: 0.034600  max mem: 192\n",
      "Test: Total time: 0:00:00 (0.066106 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "-------------------------------------------------- cw dataset --------------------------------------------------\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.081103  data: 0.043148  max mem: 192\n",
      "Test:  [1/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.078673  data: 0.043367  max mem: 192\n",
      "Test:  [2/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.077797  data: 0.043482  max mem: 192\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.062872  data: 0.034107  max mem: 192\n",
      "Test: Total time: 0:00:00 (0.063424 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "-------------------------------------------------- fgsm dataset --------------------------------------------------\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000001 (0.000001)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.077885  data: 0.045505  max mem: 192\n",
      "Test:  [1/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.077498  data: 0.045178  max mem: 192\n",
      "Test:  [2/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.077478  data: 0.045159  max mem: 192\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.062127  data: 0.035434  max mem: 192\n",
      "Test: Total time: 0:00:00 (0.062749 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "-------------------------------------------------- clean dataset --------------------------------------------------\n",
      "Test:  [ 0/79]  eta: 0:00:11  loss: 13.525923 (13.525923)  acc1: 12.500000 (12.500000)  acc5: 18.750000 (18.750000)  time: 0.143489  data: 0.113487  max mem: 192\n",
      "Test:  [ 5/79]  eta: 0:00:10  loss: 13.525923 (14.304344)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (26.041667)  time: 0.146776  data: 0.116772  max mem: 192\n",
      "Test:  [10/79]  eta: 0:00:10  loss: 14.369545 (14.516430)  acc1: 6.250000 (5.681818)  acc5: 25.000000 (23.295455)  time: 0.147464  data: 0.117425  max mem: 192\n",
      "Test:  [15/79]  eta: 0:00:09  loss: 14.498019 (15.058972)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (19.140625)  time: 0.147243  data: 0.115551  max mem: 192\n",
      "Test:  [20/79]  eta: 0:00:09  loss: 14.728826 (15.022107)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (19.940476)  time: 0.155570  data: 0.122179  max mem: 192\n",
      "Test:  [25/79]  eta: 0:00:08  loss: 14.875450 (14.944617)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (20.192308)  time: 0.169009  data: 0.133611  max mem: 192\n",
      "Test:  [30/79]  eta: 0:00:08  loss: 15.143551 (14.882747)  acc1: 0.000000 (3.830645)  acc5: 18.750000 (19.959677)  time: 0.175640  data: 0.138257  max mem: 192\n",
      "Test:  [35/79]  eta: 0:00:07  loss: 15.143551 (14.951828)  acc1: 0.000000 (3.993056)  acc5: 18.750000 (20.833333)  time: 0.178267  data: 0.140196  max mem: 192\n",
      "Test:  [40/79]  eta: 0:00:06  loss: 14.830462 (14.869930)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.884146)  time: 0.183900  data: 0.145846  max mem: 192\n",
      "Test:  [45/79]  eta: 0:00:05  loss: 14.137597 (14.812295)  acc1: 0.000000 (3.940217)  acc5: 25.000000 (21.331522)  time: 0.179918  data: 0.141861  max mem: 192\n",
      "Test:  [50/79]  eta: 0:00:04  loss: 14.249453 (14.806942)  acc1: 6.250000 (4.044118)  acc5: 25.000000 (21.323529)  time: 0.174126  data: 0.136076  max mem: 192\n",
      "Test:  [55/79]  eta: 0:00:04  loss: 13.856272 (14.717068)  acc1: 6.250000 (4.129464)  acc5: 18.750000 (21.428571)  time: 0.188743  data: 0.150698  max mem: 192\n",
      "Test:  [60/79]  eta: 0:00:03  loss: 13.856272 (14.738635)  acc1: 6.250000 (4.200820)  acc5: 18.750000 (21.004098)  time: 0.175889  data: 0.137846  max mem: 192\n",
      "Test:  [65/79]  eta: 0:00:02  loss: 14.457070 (14.790233)  acc1: 0.000000 (3.882576)  acc5: 18.750000 (20.928030)  time: 0.176155  data: 0.138096  max mem: 192\n",
      "Test:  [70/79]  eta: 0:00:01  loss: 14.620044 (14.808658)  acc1: 0.000000 (3.697183)  acc5: 18.750000 (21.302817)  time: 0.179378  data: 0.141313  max mem: 192\n",
      "Test:  [75/79]  eta: 0:00:00  loss: 14.662786 (14.750938)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (21.299342)  time: 0.167442  data: 0.129389  max mem: 192\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 14.662786 (14.743645)  acc1: 0.000000 (4.000000)  acc5: 25.000000 (21.200000)  time: 0.161886  data: 0.125254  max mem: 192\n",
      "Test: Total time: 0:00:13 (0.169591 s / it)\n",
      "* Acc@1 4.000 Acc@5 21.200 loss 14.744\n",
      "#######################################################################################################\n",
      "############################## evaluating adv_classifier trained on fgsm ##############################\n",
      "#######################################################################################################\n",
      "-------------------------------------------------- pgd dataset --------------------------------------------------\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.081568  data: 0.043594  max mem: 192\n",
      "Test:  [1/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.081560  data: 0.043586  max mem: 192\n",
      "Test:  [2/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.082247  data: 0.044168  max mem: 192\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.065762  data: 0.034659  max mem: 192\n",
      "Test: Total time: 0:00:00 (0.066364 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "-------------------------------------------------- cw dataset --------------------------------------------------\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.075925  data: 0.043732  max mem: 192\n",
      "Test:  [1/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.075674  data: 0.043609  max mem: 192\n",
      "Test:  [2/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.075535  data: 0.043535  max mem: 192\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.060457  data: 0.034062  max mem: 192\n",
      "Test: Total time: 0:00:00 (0.060990 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "-------------------------------------------------- fgsm dataset --------------------------------------------------\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.076913  data: 0.045048  max mem: 192\n",
      "Test:  [1/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.076842  data: 0.044836  max mem: 192\n",
      "Test:  [2/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.076277  data: 0.044943  max mem: 192\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.061218  data: 0.035231  max mem: 192\n",
      "Test: Total time: 0:00:00 (0.061888 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "-------------------------------------------------- clean dataset --------------------------------------------------\n",
      "Test:  [ 0/79]  eta: 0:00:11  loss: 11.006817 (11.006817)  acc1: 12.500000 (12.500000)  acc5: 37.500000 (37.500000)  time: 0.143064  data: 0.112845  max mem: 192\n",
      "Test:  [ 5/79]  eta: 0:00:10  loss: 11.006817 (11.411271)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (25.000000)  time: 0.146536  data: 0.116500  max mem: 192\n",
      "Test:  [10/79]  eta: 0:00:10  loss: 11.917227 (11.825965)  acc1: 6.250000 (5.681818)  acc5: 25.000000 (24.431818)  time: 0.147359  data: 0.117338  max mem: 192\n",
      "Test:  [15/79]  eta: 0:00:09  loss: 12.243925 (12.332522)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (22.265625)  time: 0.148423  data: 0.115898  max mem: 192\n",
      "Test:  [20/79]  eta: 0:00:09  loss: 12.243925 (12.178569)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (22.916667)  time: 0.155911  data: 0.121883  max mem: 192\n",
      "Test:  [25/79]  eta: 0:00:08  loss: 12.243925 (12.168630)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (23.317308)  time: 0.169275  data: 0.133223  max mem: 192\n",
      "Test:  [30/79]  eta: 0:00:08  loss: 12.282470 (12.195343)  acc1: 0.000000 (3.830645)  acc5: 25.000000 (24.395161)  time: 0.175832  data: 0.137780  max mem: 192\n",
      "Test:  [35/79]  eta: 0:00:07  loss: 12.161050 (12.265634)  acc1: 0.000000 (3.993056)  acc5: 25.000000 (23.784722)  time: 0.177562  data: 0.139513  max mem: 192\n",
      "Test:  [40/79]  eta: 0:00:06  loss: 12.146588 (12.163893)  acc1: 0.000000 (3.963415)  acc5: 25.000000 (23.628049)  time: 0.183883  data: 0.145820  max mem: 192\n",
      "Test:  [45/79]  eta: 0:00:05  loss: 12.004870 (12.097364)  acc1: 0.000000 (3.940217)  acc5: 25.000000 (23.913043)  time: 0.179651  data: 0.141607  max mem: 192\n",
      "Test:  [50/79]  eta: 0:00:04  loss: 11.792065 (12.083799)  acc1: 6.250000 (4.044118)  acc5: 25.000000 (24.877451)  time: 0.174095  data: 0.136036  max mem: 192\n",
      "Test:  [55/79]  eta: 0:00:04  loss: 11.646162 (12.092948)  acc1: 6.250000 (4.129464)  acc5: 25.000000 (24.218750)  time: 0.188494  data: 0.150414  max mem: 192\n",
      "Test:  [60/79]  eta: 0:00:03  loss: 11.950278 (12.096414)  acc1: 6.250000 (4.200820)  acc5: 25.000000 (24.077869)  time: 0.175683  data: 0.137626  max mem: 192\n",
      "Test:  [65/79]  eta: 0:00:02  loss: 12.443351 (12.123376)  acc1: 0.000000 (3.882576)  acc5: 25.000000 (23.674242)  time: 0.176421  data: 0.138350  max mem: 192\n",
      "Test:  [70/79]  eta: 0:00:01  loss: 12.285851 (12.124089)  acc1: 0.000000 (3.697183)  acc5: 18.750000 (23.767606)  time: 0.179466  data: 0.141408  max mem: 192\n",
      "Test:  [75/79]  eta: 0:00:00  loss: 12.285851 (12.092785)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (24.177632)  time: 0.167488  data: 0.129453  max mem: 192\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 12.237529 (12.061623)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (24.240000)  time: 0.161858  data: 0.125234  max mem: 192\n",
      "Test: Total time: 0:00:13 (0.169550 s / it)\n",
      "* Acc@1 4.000 Acc@5 24.240 loss 12.062\n",
      "########################################################################################################\n",
      "############################## evaluating adv_classifier trained on clean ##############################\n",
      "########################################################################################################\n",
      "-------------------------------------------------- pgd dataset --------------------------------------------------\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 6.568069 (6.568069)  acc1: 6.250000 (6.250000)  acc5: 43.750000 (43.750000)  time: 0.081685  data: 0.043679  max mem: 192\n",
      "Test:  [1/4]  eta: 0:00:00  loss: 6.568069 (6.610362)  acc1: 0.000000 (3.125000)  acc5: 43.750000 (43.750000)  time: 0.081839  data: 0.043845  max mem: 192\n",
      "Test:  [2/4]  eta: 0:00:00  loss: 6.652654 (6.676901)  acc1: 6.250000 (4.166667)  acc5: 43.750000 (45.833333)  time: 0.081842  data: 0.043751  max mem: 192\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 6.568069 (5.804280)  acc1: 6.250000 (6.000000)  acc5: 43.750000 (48.000000)  time: 0.065422  data: 0.034340  max mem: 192\n",
      "Test: Total time: 0:00:00 (0.065966 s / it)\n",
      "* Acc@1 6.000 Acc@5 48.000 loss 5.804\n",
      "-------------------------------------------------- cw dataset --------------------------------------------------\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 11.931582 (11.931582)  acc1: 0.000000 (0.000000)  acc5: 31.250000 (31.250000)  time: 0.080939  data: 0.042965  max mem: 192\n",
      "Test:  [1/4]  eta: 0:00:00  loss: 10.953486 (11.442534)  acc1: 0.000000 (0.000000)  acc5: 31.250000 (40.625000)  time: 0.080840  data: 0.042711  max mem: 192\n",
      "Test:  [2/4]  eta: 0:00:00  loss: 11.931582 (11.937072)  acc1: 0.000000 (0.000000)  acc5: 37.500000 (39.583333)  time: 0.081086  data: 0.042974  max mem: 192\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 10.953486 (10.572840)  acc1: 0.000000 (0.000000)  acc5: 37.500000 (42.000000)  time: 0.064665  data: 0.033666  max mem: 192\n",
      "Test: Total time: 0:00:00 (0.065202 s / it)\n",
      "* Acc@1 0.000 Acc@5 42.000 loss 10.573\n",
      "-------------------------------------------------- fgsm dataset --------------------------------------------------\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 7.155120 (7.155120)  acc1: 6.250000 (6.250000)  acc5: 37.500000 (37.500000)  time: 0.080228  data: 0.044365  max mem: 192\n",
      "Test:  [1/4]  eta: 0:00:00  loss: 6.868680 (7.011900)  acc1: 0.000000 (3.125000)  acc5: 37.500000 (43.750000)  time: 0.078926  data: 0.044510  max mem: 192\n",
      "Test:  [2/4]  eta: 0:00:00  loss: 7.155120 (7.571811)  acc1: 0.000000 (2.083333)  acc5: 37.500000 (41.666667)  time: 0.078326  data: 0.044485  max mem: 192\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 6.868680 (6.558726)  acc1: 0.000000 (2.000000)  acc5: 37.500000 (44.000000)  time: 0.062791  data: 0.034856  max mem: 192\n",
      "Test: Total time: 0:00:00 (0.063322 s / it)\n",
      "* Acc@1 2.000 Acc@5 44.000 loss 6.559\n",
      "-------------------------------------------------- clean dataset --------------------------------------------------\n",
      "Test:  [ 0/79]  eta: 0:00:11  loss: 0.003950 (0.003950)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.144048  data: 0.112925  max mem: 192\n",
      "Test:  [ 5/79]  eta: 0:00:10  loss: 0.050309 (0.075488)  acc1: 93.750000 (96.875000)  acc5: 100.000000 (100.000000)  time: 0.147823  data: 0.116649  max mem: 192\n",
      "Test:  [10/79]  eta: 0:00:10  loss: 0.050309 (0.083421)  acc1: 100.000000 (97.159091)  acc5: 100.000000 (100.000000)  time: 0.149177  data: 0.118015  max mem: 192\n",
      "Test:  [15/79]  eta: 0:00:09  loss: 0.050309 (0.093181)  acc1: 93.750000 (96.875000)  acc5: 100.000000 (100.000000)  time: 0.149420  data: 0.116513  max mem: 192\n",
      "Test:  [20/79]  eta: 0:00:09  loss: 0.060876 (0.102474)  acc1: 93.750000 (96.726190)  acc5: 100.000000 (100.000000)  time: 0.157443  data: 0.123161  max mem: 192\n",
      "Test:  [25/79]  eta: 0:00:08  loss: 0.038588 (0.089864)  acc1: 100.000000 (97.115385)  acc5: 100.000000 (100.000000)  time: 0.170891  data: 0.134895  max mem: 192\n",
      "Test:  [30/79]  eta: 0:00:08  loss: 0.038588 (0.084775)  acc1: 100.000000 (97.379032)  acc5: 100.000000 (100.000000)  time: 0.177181  data: 0.139456  max mem: 192\n",
      "Test:  [35/79]  eta: 0:00:07  loss: 0.029115 (0.080989)  acc1: 100.000000 (97.569444)  acc5: 100.000000 (100.000000)  time: 0.178757  data: 0.140705  max mem: 192\n",
      "Test:  [40/79]  eta: 0:00:06  loss: 0.038588 (0.078956)  acc1: 100.000000 (97.713415)  acc5: 100.000000 (100.000000)  time: 0.184231  data: 0.146189  max mem: 192\n",
      "Test:  [45/79]  eta: 0:00:05  loss: 0.056058 (0.083779)  acc1: 100.000000 (97.690217)  acc5: 100.000000 (100.000000)  time: 0.180011  data: 0.141964  max mem: 192\n",
      "Test:  [50/79]  eta: 0:00:04  loss: 0.048692 (0.080641)  acc1: 100.000000 (97.794118)  acc5: 100.000000 (100.000000)  time: 0.174391  data: 0.136351  max mem: 192\n",
      "Test:  [55/79]  eta: 0:00:04  loss: 0.059690 (0.082373)  acc1: 100.000000 (97.656250)  acc5: 100.000000 (100.000000)  time: 0.189554  data: 0.151527  max mem: 192\n",
      "Test:  [60/79]  eta: 0:00:03  loss: 0.061207 (0.084029)  acc1: 100.000000 (97.540984)  acc5: 100.000000 (100.000000)  time: 0.176865  data: 0.138818  max mem: 192\n",
      "Test:  [65/79]  eta: 0:00:02  loss: 0.021203 (0.079007)  acc1: 100.000000 (97.727273)  acc5: 100.000000 (100.000000)  time: 0.177624  data: 0.139588  max mem: 192\n",
      "Test:  [70/79]  eta: 0:00:01  loss: 0.021203 (0.075005)  acc1: 100.000000 (97.887324)  acc5: 100.000000 (100.000000)  time: 0.180662  data: 0.142622  max mem: 192\n",
      "Test:  [75/79]  eta: 0:00:00  loss: 0.016655 (0.071368)  acc1: 100.000000 (98.026316)  acc5: 100.000000 (100.000000)  time: 0.169118  data: 0.131064  max mem: 192\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 0.015308 (0.080008)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (100.000000)  time: 0.163885  data: 0.127210  max mem: 192\n",
      "Test: Total time: 0:00:13 (0.170878 s / it)\n",
      "* Acc@1 98.000 Acc@5 100.000 loss 0.080\n"

     ]
    }
   ],
   "source": [
    "attacks = [x for x in loader_dict.keys()]\n",
    "\n",
    "for attack in attacks:\n",
    "    pstr = \"#\"*30 + f''' evaluating adv_classifier trained on {attack} ''' + \"#\"*30\n",
    "    print(len(pstr)*\"#\")\n",
    "    print(pstr)\n",
    "    print(len(pstr)*\"#\")\n",
    "    adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                             num_labels=len(CLASS_SUBSET))\n",
    "    adv_classifier.to(DEVICE)\n",
    "    \n",
    "    save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version)\n",
    "    save_file = f\"{attack}.pt\"\n",
    "    adv_classifier.load_state_dict(torch.load(str(save_path) + \"/\" + save_file))\n",
    "    \n",
    "    for applied_attack in attacks:\n",
    "        \n",
    "#         if applied_attack == attack:\n",
    "#             continue\n",
    "        \n",
    "        print(\"-\"*50 + f\" {applied_attack} dataset \" + \"-\"*50)\n",
    "        logger_dict, logger = validate_network(model, \n",
    "                                               adv_classifier, \n",
    "                                               loader_dict[applied_attack][\"validation\"], \n",
    "                                               criterion=nn.CrossEntropyLoss(),\n",
    "                                               tensor_dir=None, \n",
    "                                               adversarial_attack=None, \n",
    "                                               n=4, \n",
    "                                               avgpool_patchtokens=False, \n",
    "                                               path_predictions=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on newly generated attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################################################\n",
      "############################## evaluating adv_classifier trained on pgd ##############################\n",
      "######################################################################################################\n",
      ">>>>> applying attack: PGD(model_name=ViTWrapper, device=cuda:0, eps=0.3, alpha=0.023529411764705882, steps=15, random_start=True, attack_mode=default, return_type=float) \n",
      "Test:  [ 0/79]  eta: 0:01:44  loss: 11.795971 (11.795971)  acc1: 12.500000 (12.500000)  acc5: 25.000000 (25.000000)  adv_loss: 45.421654 (45.421654)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.323473  data: 0.112878  max mem: 1585\n",
      "Test:  [ 5/79]  eta: 0:01:35  loss: 11.018626 (11.406765)  acc1: 6.250000 (8.333333)  acc5: 18.750000 (20.833333)  adv_loss: 45.421654 (46.137278)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.291224  data: 0.116435  max mem: 1585\n",
      "Test:  [10/79]  eta: 0:01:28  loss: 11.580111 (11.467379)  acc1: 6.250000 (5.681818)  acc5: 18.750000 (20.454545)  adv_loss: 46.910950 (46.758542)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.288591  data: 0.116637  max mem: 1585\n",
      "Test:  [15/79]  eta: 0:01:22  loss: 11.580111 (11.560608)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (21.093750)  adv_loss: 47.550251 (47.213219)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.285920  data: 0.114830  max mem: 1585\n",
      "Test:  [20/79]  eta: 0:01:16  loss: 11.416915 (11.553357)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (21.428571)  adv_loss: 48.045464 (47.036743)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.290149  data: 0.121413  max mem: 1585\n",
      "Test:  [25/79]  eta: 0:01:10  loss: 11.416915 (11.531420)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (20.673077)  adv_loss: 48.045464 (47.007138)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.301896  data: 0.132788  max mem: 1585\n",
      "Test:  [30/79]  eta: 0:01:03  loss: 11.416915 (11.530670)  acc1: 0.000000 (3.830645)  acc5: 18.750000 (20.766129)  adv_loss: 48.045464 (47.124936)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.307455  data: 0.138148  max mem: 1585\n",
      "Test:  [35/79]  eta: 0:00:57  loss: 11.416915 (11.576890)  acc1: 0.000000 (3.993056)  acc5: 18.750000 (20.833333)  adv_loss: 47.837151 (47.119178)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.309922  data: 0.140525  max mem: 1585\n",
      "Test:  [40/79]  eta: 0:00:50  loss: 11.172411 (11.517733)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.426829)  adv_loss: 47.620934 (47.174545)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.316441  data: 0.146940  max mem: 1585\n",
      "Test:  [45/79]  eta: 0:00:44  loss: 11.421421 (11.511050)  acc1: 0.000000 (3.940217)  acc5: 18.750000 (19.565217)  adv_loss: 47.620934 (47.122925)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.312754  data: 0.142971  max mem: 1585\n",
      "Test:  [50/79]  eta: 0:00:37  loss: 10.739025 (11.458579)  acc1: 6.250000 (4.044118)  acc5: 18.750000 (20.465686)  adv_loss: 47.049187 (47.114296)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.307049  data: 0.137120  max mem: 1585\n",
      "Test:  [55/79]  eta: 0:00:31  loss: 11.313252 (11.433147)  acc1: 6.250000 (4.129464)  acc5: 12.500000 (20.200893)  adv_loss: 46.906628 (47.078872)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.321337  data: 0.151330  max mem: 1585\n",
      "Test:  [60/79]  eta: 0:00:24  loss: 11.313252 (11.424241)  acc1: 6.250000 (4.200820)  acc5: 12.500000 (20.696721)  adv_loss: 46.602757 (47.066070)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.307577  data: 0.137638  max mem: 1585\n",
      "Test:  [65/79]  eta: 0:00:18  loss: 11.313252 (11.414984)  acc1: 0.000000 (3.882576)  acc5: 18.750000 (20.454545)  adv_loss: 47.061901 (47.108966)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.307758  data: 0.137993  max mem: 1585\n",
      "Test:  [70/79]  eta: 0:00:11  loss: 11.385376 (11.439243)  acc1: 0.000000 (3.697183)  acc5: 12.500000 (19.894366)  adv_loss: 47.760284 (47.184284)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.310902  data: 0.141094  max mem: 1585\n",
      "Test:  [75/79]  eta: 0:00:05  loss: 11.385376 (11.404295)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (20.312500)  adv_loss: 47.317093 (47.060464)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.299202  data: 0.129340  max mem: 1585\n",
      "Test:  [78/79]  eta: 0:00:01  loss: 11.222515 (11.402881)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (20.240000)  adv_loss: 47.855457 (47.124577)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.254131  data: 0.125526  max mem: 1585\n",
      "Test: Total time: 0:01:42 (1.292899 s / it)\n",
      "* Acc@1 4.000 Acc@5 20.240 loss 11.403\n",
      "* adv_Acc@1 0.000 adv_Acc@5 0.000 adv_loss 47.125\n",
      ">>>>> applying attack: CW(model_name=ViTWrapper, device=cuda:0, c=10, kappa=0, steps=30, lr=0.003, attack_mode=default, return_type=float) \n",
      "Test:  [ 0/79]  eta: 0:01:30  loss: 11.795971 (11.795971)  acc1: 12.500000 (12.500000)  acc5: 25.000000 (25.000000)  adv_loss: 11.874754 (11.874754)  adv_acc1: 6.250000 (6.250000)  adv_acc5: 25.000000 (25.000000)  time: 1.150261  data: 0.113294  max mem: 1623\n",
      "Test:  [ 5/79]  eta: 0:01:28  loss: 11.018626 (11.406765)  acc1: 6.250000 (8.333333)  acc5: 18.750000 (20.833333)  adv_loss: 11.046786 (11.456637)  adv_acc1: 6.250000 (5.208333)  adv_acc5: 18.750000 (20.833333)  time: 1.190162  data: 0.116514  max mem: 1623\n",
      "Test:  [10/79]  eta: 0:01:06  loss: 11.580111 (11.467379)  acc1: 6.250000 (5.681818)  acc5: 18.750000 (20.454545)  adv_loss: 11.601521 (11.496906)  adv_acc1: 6.250000 (3.977273)  adv_acc5: 18.750000 (20.454545)  time: 0.969605  data: 0.117027  max mem: 1623\n",
      "Test:  [15/79]  eta: 0:00:52  loss: 11.580111 (11.560608)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (21.093750)  adv_loss: 11.601521 (11.580907)  adv_acc1: 0.000000 (2.734375)  adv_acc5: 18.750000 (21.093750)  time: 0.814789  data: 0.115429  max mem: 1623\n",
      "Test:  [20/79]  eta: 0:00:51  loss: 11.416915 (11.553357)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (21.428571)  adv_loss: 11.416916 (11.571762)  adv_acc1: 0.000000 (3.273810)  adv_acc5: 18.750000 (21.428571)  time: 0.866526  data: 0.122233  max mem: 1623\n",
      "Test:  [25/79]  eta: 0:00:48  loss: 11.416915 (11.531420)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (20.673077)  adv_loss: 11.416916 (11.549797)  adv_acc1: 0.000000 (3.365385)  adv_acc5: 18.750000 (20.673077)  time: 0.799740  data: 0.134005  max mem: 1623\n",
      "Test:  [30/79]  eta: 0:00:41  loss: 11.416915 (11.530670)  acc1: 0.000000 (3.830645)  acc5: 18.750000 (20.766129)  adv_loss: 11.416916 (11.546123)  adv_acc1: 0.000000 (3.024194)  adv_acc5: 18.750000 (20.766129)  time: 0.792983  data: 0.138713  max mem: 1623\n",
      "Test:  [35/79]  eta: 0:00:38  loss: 11.416915 (11.576890)  acc1: 0.000000 (3.993056)  acc5: 18.750000 (20.833333)  adv_loss: 11.416916 (11.593112)  adv_acc1: 0.000000 (3.125000)  adv_acc5: 18.750000 (20.833333)  time: 0.936669  data: 0.140462  max mem: 1623\n",
      "Test:  [40/79]  eta: 0:00:34  loss: 11.172411 (11.517733)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.426829)  adv_loss: 11.233649 (11.533234)  adv_acc1: 0.000000 (3.201220)  adv_acc5: 18.750000 (20.426829)  time: 0.896819  data: 0.145755  max mem: 1623\n",
      "Test:  [45/79]  eta: 0:00:30  loss: 11.421421 (11.511050)  acc1: 0.000000 (3.940217)  acc5: 18.750000 (19.565217)  adv_loss: 11.421423 (11.525198)  adv_acc1: 0.000000 (3.260870)  adv_acc5: 18.750000 (19.565217)  time: 0.925534  data: 0.141183  max mem: 1623\n",
      "Test:  [50/79]  eta: 0:00:26  loss: 10.739025 (11.458579)  acc1: 6.250000 (4.044118)  acc5: 18.750000 (20.465686)  adv_loss: 10.741206 (11.473037)  adv_acc1: 0.000000 (3.431373)  adv_acc5: 18.750000 (20.465686)  time: 1.009927  data: 0.135709  max mem: 1623\n",
      "Test:  [55/79]  eta: 0:00:22  loss: 11.313252 (11.433147)  acc1: 6.250000 (4.129464)  acc5: 12.500000 (20.200893)  adv_loss: 11.313251 (11.446704)  adv_acc1: 6.250000 (3.571429)  adv_acc5: 12.500000 (20.200893)  time: 1.039933  data: 0.150647  max mem: 1623\n",
      "Test:  [60/79]  eta: 0:00:17  loss: 11.313252 (11.424241)  acc1: 6.250000 (4.200820)  acc5: 12.500000 (20.696721)  adv_loss: 11.313251 (11.438542)  adv_acc1: 6.250000 (3.586066)  adv_acc5: 12.500000 (20.696721)  time: 1.038573  data: 0.138050  max mem: 1623\n",
      "Test:  [65/79]  eta: 0:00:12  loss: 11.313252 (11.414984)  acc1: 0.000000 (3.882576)  acc5: 18.750000 (20.454545)  adv_loss: 11.313251 (11.428201)  adv_acc1: 0.000000 (3.314394)  adv_acc5: 18.750000 (20.454545)  time: 0.904922  data: 0.138895  max mem: 1623\n",
      "Test:  [70/79]  eta: 0:00:07  loss: 11.385376 (11.439243)  acc1: 0.000000 (3.697183)  acc5: 12.500000 (19.894366)  adv_loss: 11.385377 (11.451590)  adv_acc1: 0.000000 (3.169014)  adv_acc5: 12.500000 (19.894366)  time: 0.806885  data: 0.141811  max mem: 1623\n",
      "Test:  [75/79]  eta: 0:00:03  loss: 11.385376 (11.404295)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (20.312500)  adv_loss: 11.385377 (11.417292)  adv_acc1: 0.000000 (3.618421)  adv_acc5: 18.750000 (20.312500)  time: 0.783898  data: 0.129807  max mem: 1623\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 11.222515 (11.402881)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (20.240000)  adv_loss: 11.222515 (11.415385)  adv_acc1: 0.000000 (3.520000)  adv_acc5: 18.750000 (20.240000)  time: 0.689111  data: 0.125694  max mem: 1623\n",
      "Test: Total time: 0:01:09 (0.877963 s / it)\n",
      "* Acc@1 4.000 Acc@5 20.240 loss 11.403\n",
      "* adv_Acc@1 3.520 adv_Acc@5 20.240 adv_loss 11.415\n",
      ">>>>> applying attack: FGSM(model_name=ViTWrapper, device=cuda:0, eps=0.03, attack_mode=default, return_type=float) \n",
      "Test:  [ 0/79]  eta: 0:00:19  loss: 11.795971 (11.795971)  acc1: 12.500000 (12.500000)  acc5: 25.000000 (25.000000)  adv_loss: 22.777174 (22.777174)  adv_acc1: 12.500000 (12.500000)  adv_acc5: 12.500000 (12.500000)  time: 0.248409  data: 0.112257  max mem: 1623\n",
      "Test:  [ 5/79]  eta: 0:00:18  loss: 11.018626 (11.406765)  acc1: 6.250000 (8.333333)  acc5: 18.750000 (20.833333)  adv_loss: 22.777174 (23.623278)  adv_acc1: 6.250000 (8.333333)  adv_acc5: 6.250000 (8.333333)  time: 0.252844  data: 0.116506  max mem: 1623\n",
      "Test:  [10/79]  eta: 0:00:17  loss: 11.580111 (11.467379)  acc1: 6.250000 (5.681818)  acc5: 18.750000 (20.454545)  adv_loss: 24.614483 (24.039088)  adv_acc1: 6.250000 (5.681818)  adv_acc5: 6.250000 (6.250000)  time: 0.253106  data: 0.116876  max mem: 1623\n",
      "Test:  [15/79]  eta: 0:00:16  loss: 11.580111 (11.560608)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (21.093750)  adv_loss: 25.020700 (24.362932)  adv_acc1: 0.000000 (3.906250)  adv_acc5: 0.000000 (4.296875)  time: 0.251363  data: 0.115062  max mem: 1623\n",
      "Test:  [20/79]  eta: 0:00:15  loss: 11.416915 (11.553357)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (21.428571)  adv_loss: 25.033527 (24.361036)  adv_acc1: 0.000000 (4.166667)  adv_acc5: 0.000000 (4.464286)  time: 0.257723  data: 0.121485  max mem: 1623\n",
      "Test:  [25/79]  eta: 0:00:14  loss: 11.416915 (11.531420)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (20.673077)  adv_loss: 25.142223 (24.311305)  adv_acc1: 0.000000 (4.326923)  adv_acc5: 0.000000 (4.567308)  time: 0.269478  data: 0.133337  max mem: 1623\n",
      "Test:  [30/79]  eta: 0:00:13  loss: 11.416915 (11.530670)  acc1: 0.000000 (3.830645)  acc5: 18.750000 (20.766129)  adv_loss: 25.033527 (24.368570)  adv_acc1: 0.000000 (3.830645)  adv_acc5: 0.000000 (4.032258)  time: 0.274144  data: 0.137995  max mem: 1623\n",
      "Test:  [35/79]  eta: 0:00:11  loss: 11.416915 (11.576890)  acc1: 0.000000 (3.993056)  acc5: 18.750000 (20.833333)  adv_loss: 24.904705 (24.326416)  adv_acc1: 0.000000 (3.993056)  adv_acc5: 0.000000 (4.166667)  time: 0.276072  data: 0.140005  max mem: 1623\n",
      "Test:  [40/79]  eta: 0:00:10  loss: 11.172411 (11.517733)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.426829)  adv_loss: 24.894190 (24.309411)  adv_acc1: 0.000000 (3.963415)  adv_acc5: 0.000000 (4.115854)  time: 0.281807  data: 0.145765  max mem: 1623\n",
      "Test:  [45/79]  eta: 0:00:09  loss: 11.421421 (11.511050)  acc1: 0.000000 (3.940217)  acc5: 18.750000 (19.565217)  adv_loss: 24.642681 (24.261414)  adv_acc1: 0.000000 (3.940217)  adv_acc5: 0.000000 (4.076087)  time: 0.277081  data: 0.141007  max mem: 1623\n",
      "Test:  [50/79]  eta: 0:00:07  loss: 10.739025 (11.458579)  acc1: 6.250000 (4.044118)  acc5: 18.750000 (20.465686)  adv_loss: 23.294849 (24.197551)  adv_acc1: 6.250000 (4.044118)  adv_acc5: 6.250000 (4.166667)  time: 0.271766  data: 0.135683  max mem: 1623\n",
      "Test:  [55/79]  eta: 0:00:06  loss: 11.313252 (11.433147)  acc1: 6.250000 (4.129464)  acc5: 12.500000 (20.200893)  adv_loss: 23.294849 (24.149234)  adv_acc1: 6.250000 (4.129464)  adv_acc5: 6.250000 (4.241071)  time: 0.286159  data: 0.150080  max mem: 1623\n",
      "Test:  [60/79]  eta: 0:00:05  loss: 11.313252 (11.424241)  acc1: 6.250000 (4.200820)  acc5: 12.500000 (20.696721)  adv_loss: 23.473209 (24.138387)  adv_acc1: 6.250000 (4.200820)  adv_acc5: 6.250000 (4.303279)  time: 0.273511  data: 0.137424  max mem: 1623\n",
      "Test:  [65/79]  eta: 0:00:03  loss: 11.313252 (11.414984)  acc1: 0.000000 (3.882576)  acc5: 18.750000 (20.454545)  adv_loss: 23.991194 (24.169324)  adv_acc1: 0.000000 (3.882576)  adv_acc5: 0.000000 (3.977273)  time: 0.274412  data: 0.138319  max mem: 1623\n",
      "Test:  [70/79]  eta: 0:00:02  loss: 11.385376 (11.439243)  acc1: 0.000000 (3.697183)  acc5: 12.500000 (19.894366)  adv_loss: 24.251625 (24.221539)  adv_acc1: 0.000000 (3.697183)  adv_acc5: 0.000000 (3.785211)  time: 0.277311  data: 0.141190  max mem: 1623\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 11.385376 (11.404295)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (20.312500)  adv_loss: 24.036297 (24.158323)  adv_acc1: 0.000000 (4.111842)  adv_acc5: 0.000000 (4.194079)  time: 0.265387  data: 0.129345  max mem: 1623\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 11.222515 (11.402881)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (20.240000)  adv_loss: 24.252882 (24.197799)  adv_acc1: 0.000000 (4.000000)  adv_acc5: 0.000000 (4.080000)  time: 0.256714  data: 0.125206  max mem: 1623\n",
      "Test: Total time: 0:00:21 (0.267834 s / it)\n",
      "* Acc@1 4.000 Acc@5 20.240 loss 11.403\n",
      "* adv_Acc@1 4.000 adv_Acc@5 4.080 adv_loss 24.198\n",
      "#####################################################################################################\n",
      "############################## evaluating adv_classifier trained on cw ##############################\n",
      "#####################################################################################################\n",
      ">>>>> applying attack: PGD(model_name=ViTWrapper, device=cuda:0, eps=0.3, alpha=0.023529411764705882, steps=15, random_start=True, attack_mode=default, return_type=float) \n",
      "Test:  [ 0/79]  eta: 0:01:40  loss: 13.525923 (13.525923)  acc1: 12.500000 (12.500000)  acc5: 18.750000 (18.750000)  adv_loss: 50.639122 (50.639122)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.278268  data: 0.112251  max mem: 1623\n",
      "Test:  [ 5/79]  eta: 0:01:36  loss: 13.525923 (14.304344)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (26.041667)  adv_loss: 50.805416 (52.072816)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.307689  data: 0.139901  max mem: 1623\n",
      "Test:  [10/79]  eta: 0:01:30  loss: 14.369545 (14.516430)  acc1: 6.250000 (5.681818)  acc5: 25.000000 (23.295455)  adv_loss: 53.288246 (52.890572)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.308894  data: 0.140568  max mem: 1623\n",
      "Test:  [15/79]  eta: 0:01:23  loss: 14.498019 (15.058972)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (19.140625)  adv_loss: 53.751293 (53.418354)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.309351  data: 0.140724  max mem: 1623\n",
      "Test:  [20/79]  eta: 0:01:17  loss: 14.728826 (15.022107)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (19.940476)  adv_loss: 53.808861 (53.382964)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.317911  data: 0.148920  max mem: 1623\n",
      "Test:  [25/79]  eta: 0:01:11  loss: 14.875450 (14.944617)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (20.192308)  adv_loss: 53.926044 (53.347604)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.329401  data: 0.160143  max mem: 1623\n",
      "Test:  [30/79]  eta: 0:01:04  loss: 15.143551 (14.882747)  acc1: 0.000000 (3.830645)  acc5: 18.750000 (19.959677)  adv_loss: 53.929691 (53.491340)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.335130  data: 0.165696  max mem: 1623\n",
      "Test:  [35/79]  eta: 0:00:58  loss: 15.143551 (14.951828)  acc1: 0.000000 (3.993056)  acc5: 18.750000 (20.833333)  adv_loss: 53.929691 (53.471167)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.336876  data: 0.167449  max mem: 1623\n",
      "Test:  [40/79]  eta: 0:00:51  loss: 14.830462 (14.869930)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.884146)  adv_loss: 53.929691 (53.560362)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.340476  data: 0.170978  max mem: 1623\n",
      "Test:  [45/79]  eta: 0:00:45  loss: 14.137597 (14.812295)  acc1: 0.000000 (3.940217)  acc5: 25.000000 (21.331522)  adv_loss: 54.402920 (53.546769)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.339628  data: 0.170076  max mem: 1623\n",
      "Test:  [50/79]  eta: 0:00:38  loss: 14.249453 (14.806942)  acc1: 6.250000 (4.044118)  acc5: 25.000000 (21.323529)  adv_loss: 53.844822 (53.540833)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.334186  data: 0.164641  max mem: 1623\n",
      "Test:  [55/79]  eta: 0:00:31  loss: 13.856272 (14.717068)  acc1: 6.250000 (4.129464)  acc5: 18.750000 (21.428571)  adv_loss: 53.596844 (53.473726)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.344016  data: 0.174376  max mem: 1623\n",
      "Test:  [60/79]  eta: 0:00:25  loss: 13.856272 (14.738635)  acc1: 6.250000 (4.200820)  acc5: 18.750000 (21.004098)  adv_loss: 52.980541 (53.455657)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.327369  data: 0.157620  max mem: 1623\n",
      "Test:  [65/79]  eta: 0:00:18  loss: 14.457070 (14.790233)  acc1: 0.000000 (3.882576)  acc5: 18.750000 (20.928030)  adv_loss: 53.596844 (53.533768)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.317731  data: 0.147838  max mem: 1623\n",
      "Test:  [70/79]  eta: 0:00:11  loss: 14.620044 (14.808658)  acc1: 0.000000 (3.697183)  acc5: 18.750000 (21.302817)  adv_loss: 53.904270 (53.589187)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.313812  data: 0.143921  max mem: 1623\n",
      "Test:  [75/79]  eta: 0:00:05  loss: 14.662786 (14.750938)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (21.299342)  adv_loss: 53.569641 (53.430813)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.299939  data: 0.130047  max mem: 1623\n",
      "Test:  [78/79]  eta: 0:00:01  loss: 14.662786 (14.743645)  acc1: 0.000000 (4.000000)  acc5: 25.000000 (21.200000)  adv_loss: 53.904270 (53.519333)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.254992  data: 0.125521  max mem: 1623\n",
      "Test: Total time: 0:01:43 (1.310607 s / it)\n",
      "* Acc@1 4.000 Acc@5 21.200 loss 14.744\n",
      "* adv_Acc@1 0.000 adv_Acc@5 0.000 adv_loss 53.519\n",
      ">>>>> applying attack: CW(model_name=ViTWrapper, device=cuda:0, c=10, kappa=0, steps=30, lr=0.003, attack_mode=default, return_type=float) \n",
      "Test:  [ 0/79]  eta: 0:01:53  loss: 13.525923 (13.525923)  acc1: 12.500000 (12.500000)  acc5: 18.750000 (18.750000)  adv_loss: 13.641943 (13.641943)  adv_acc1: 6.250000 (6.250000)  adv_acc5: 18.750000 (18.750000)  time: 1.430385  data: 0.167160  max mem: 1623\n",
      "Test:  [ 5/79]  eta: 0:01:32  loss: 13.525923 (14.304344)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (26.041667)  adv_loss: 13.641943 (14.344891)  adv_acc1: 6.250000 (7.291667)  adv_acc5: 25.000000 (26.041667)  time: 1.252819  data: 0.140439  max mem: 1623\n",
      "Test:  [10/79]  eta: 0:01:14  loss: 14.369545 (14.516430)  acc1: 6.250000 (5.681818)  acc5: 25.000000 (23.295455)  adv_loss: 14.369547 (14.541960)  adv_acc1: 6.250000 (5.113636)  adv_acc5: 25.000000 (23.295455)  time: 1.072509  data: 0.137466  max mem: 1623\n",
      "Test:  [15/79]  eta: 0:00:56  loss: 14.498019 (15.058972)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (19.140625)  adv_loss: 14.499352 (15.076524)  adv_acc1: 0.000000 (3.515625)  adv_acc5: 18.750000 (19.140625)  time: 0.889267  data: 0.133307  max mem: 1623\n",
      "Test:  [20/79]  eta: 0:00:55  loss: 14.728826 (15.022107)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (19.940476)  adv_loss: 14.736960 (15.037877)  adv_acc1: 0.000000 (3.869048)  adv_acc5: 18.750000 (19.940476)  time: 0.922925  data: 0.137177  max mem: 1623\n",
      "Test:  [25/79]  eta: 0:00:51  loss: 14.875450 (14.944617)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (20.192308)  adv_loss: 14.875453 (14.962412)  adv_acc1: 0.000000 (3.846154)  adv_acc5: 18.750000 (20.192308)  time: 0.865944  data: 0.147917  max mem: 1623\n",
      "Test:  [30/79]  eta: 0:00:44  loss: 15.143551 (14.882747)  acc1: 0.000000 (3.830645)  acc5: 18.750000 (19.959677)  adv_loss: 15.143552 (14.897674)  adv_acc1: 0.000000 (3.427419)  adv_acc5: 18.750000 (19.959677)  time: 0.824471  data: 0.151632  max mem: 1623\n",
      "Test:  [35/79]  eta: 0:00:40  loss: 15.143551 (14.951828)  acc1: 0.000000 (3.993056)  acc5: 18.750000 (20.833333)  adv_loss: 15.143552 (14.967849)  adv_acc1: 0.000000 (3.472222)  adv_acc5: 18.750000 (20.833333)  time: 0.948928  data: 0.152531  max mem: 1623\n",
      "Test:  [40/79]  eta: 0:00:36  loss: 14.830462 (14.869930)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.884146)  adv_loss: 14.830461 (14.887792)  adv_acc1: 0.000000 (3.353659)  adv_acc5: 18.750000 (20.884146)  time: 0.944668  data: 0.156296  max mem: 1623\n",
      "Test:  [45/79]  eta: 0:00:32  loss: 14.137597 (14.812295)  acc1: 0.000000 (3.940217)  acc5: 25.000000 (21.331522)  adv_loss: 14.137597 (14.828405)  adv_acc1: 0.000000 (3.396739)  adv_acc5: 25.000000 (21.331522)  time: 0.971047  data: 0.149255  max mem: 1623\n",
      "Test:  [50/79]  eta: 0:00:28  loss: 14.249453 (14.806942)  acc1: 6.250000 (4.044118)  acc5: 25.000000 (21.323529)  adv_loss: 14.293361 (14.823601)  adv_acc1: 0.000000 (3.553922)  adv_acc5: 25.000000 (21.323529)  time: 1.052433  data: 0.140749  max mem: 1623\n",
      "Test:  [55/79]  eta: 0:00:23  loss: 13.856272 (14.717068)  acc1: 6.250000 (4.129464)  acc5: 18.750000 (21.428571)  adv_loss: 13.863402 (14.732454)  adv_acc1: 6.250000 (3.683036)  adv_acc5: 18.750000 (21.428571)  time: 1.098487  data: 0.153390  max mem: 1623\n",
      "Test:  [60/79]  eta: 0:00:18  loss: 13.856272 (14.738635)  acc1: 6.250000 (4.200820)  acc5: 18.750000 (21.004098)  adv_loss: 13.863402 (14.754136)  adv_acc1: 6.250000 (3.688525)  adv_acc5: 18.750000 (21.004098)  time: 1.050454  data: 0.138874  max mem: 1623\n",
      "Test:  [65/79]  eta: 0:00:13  loss: 14.457070 (14.790233)  acc1: 0.000000 (3.882576)  acc5: 18.750000 (20.928030)  adv_loss: 14.472968 (14.804561)  adv_acc1: 0.000000 (3.409091)  adv_acc5: 18.750000 (20.928030)  time: 0.904310  data: 0.138393  max mem: 1623\n",
      "Test:  [70/79]  eta: 0:00:08  loss: 14.620044 (14.808658)  acc1: 0.000000 (3.697183)  acc5: 18.750000 (21.302817)  adv_loss: 14.620800 (14.822040)  adv_acc1: 0.000000 (3.257042)  adv_acc5: 18.750000 (21.302817)  time: 0.817182  data: 0.141251  max mem: 1623\n",
      "Test:  [75/79]  eta: 0:00:03  loss: 14.662786 (14.750938)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (21.299342)  adv_loss: 14.662786 (14.765029)  adv_acc1: 0.000000 (3.700658)  adv_acc5: 18.750000 (21.299342)  time: 0.816371  data: 0.129128  max mem: 1623\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 14.662786 (14.743645)  acc1: 0.000000 (4.000000)  acc5: 25.000000 (21.200000)  adv_loss: 14.662786 (14.757200)  adv_acc1: 0.000000 (3.600000)  adv_acc5: 25.000000 (21.200000)  time: 0.721652  data: 0.125113  max mem: 1623\n",
      "Test: Total time: 0:01:12 (0.919168 s / it)\n",
      "* Acc@1 4.000 Acc@5 21.200 loss 14.744\n",
      "* adv_Acc@1 3.600 adv_Acc@5 21.200 adv_loss 14.757\n",
      ">>>>> applying attack: FGSM(model_name=ViTWrapper, device=cuda:0, eps=0.03, attack_mode=default, return_type=float) \n",
      "Test:  [ 0/79]  eta: 0:00:19  loss: 13.525923 (13.525923)  acc1: 12.500000 (12.500000)  acc5: 18.750000 (18.750000)  adv_loss: 22.692768 (22.692768)  adv_acc1: 6.250000 (6.250000)  adv_acc5: 6.250000 (6.250000)  time: 0.249699  data: 0.113621  max mem: 1623\n",
      "Test:  [ 5/79]  eta: 0:00:18  loss: 13.525923 (14.304344)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (26.041667)  adv_loss: 23.347969 (24.085189)  adv_acc1: 6.250000 (5.208333)  adv_acc5: 6.250000 (8.333333)  time: 0.253287  data: 0.117344  max mem: 1623\n",
      "Test:  [10/79]  eta: 0:00:17  loss: 14.369545 (14.516430)  acc1: 6.250000 (5.681818)  acc5: 25.000000 (23.295455)  adv_loss: 25.136618 (24.782492)  adv_acc1: 6.250000 (3.977273)  adv_acc5: 6.250000 (5.681818)  time: 0.253501  data: 0.117465  max mem: 1623\n",
      "Test:  [15/79]  eta: 0:00:16  loss: 14.498019 (15.058972)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (19.140625)  adv_loss: 25.681362 (25.293408)  adv_acc1: 0.000000 (2.734375)  adv_acc5: 0.000000 (4.296875)  time: 0.251580  data: 0.115546  max mem: 1623\n",
      "Test:  [20/79]  eta: 0:00:15  loss: 14.728826 (15.022107)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (19.940476)  adv_loss: 25.877436 (25.182171)  adv_acc1: 0.000000 (3.273810)  adv_acc5: 0.000000 (4.761905)  time: 0.258028  data: 0.121933  max mem: 1623\n",
      "Test:  [25/79]  eta: 0:00:14  loss: 14.875450 (14.944617)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (20.192308)  adv_loss: 26.078945 (25.187627)  adv_acc1: 0.000000 (3.365385)  adv_acc5: 0.000000 (5.048077)  time: 0.269563  data: 0.133336  max mem: 1623\n",
      "Test:  [30/79]  eta: 0:00:13  loss: 15.143551 (14.882747)  acc1: 0.000000 (3.830645)  acc5: 18.750000 (19.959677)  adv_loss: 25.877436 (25.213908)  adv_acc1: 0.000000 (3.024194)  adv_acc5: 0.000000 (4.435484)  time: 0.274297  data: 0.138050  max mem: 1623\n",
      "Test:  [35/79]  eta: 0:00:11  loss: 15.143551 (14.951828)  acc1: 0.000000 (3.993056)  acc5: 18.750000 (20.833333)  adv_loss: 25.593790 (25.211769)  adv_acc1: 0.000000 (3.125000)  adv_acc5: 0.000000 (4.687500)  time: 0.276232  data: 0.139972  max mem: 1623\n",
      "Test:  [40/79]  eta: 0:00:10  loss: 14.830462 (14.869930)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.884146)  adv_loss: 25.132448 (25.141571)  adv_acc1: 0.000000 (3.201220)  adv_acc5: 0.000000 (4.725610)  time: 0.282311  data: 0.146036  max mem: 1623\n",
      "Test:  [45/79]  eta: 0:00:09  loss: 14.137597 (14.812295)  acc1: 0.000000 (3.940217)  acc5: 25.000000 (21.331522)  adv_loss: 25.024395 (25.019488)  adv_acc1: 0.000000 (3.260870)  adv_acc5: 0.000000 (4.755435)  time: 0.277677  data: 0.141372  max mem: 1623\n",
      "Test:  [50/79]  eta: 0:00:07  loss: 14.249453 (14.806942)  acc1: 6.250000 (4.044118)  acc5: 25.000000 (21.323529)  adv_loss: 24.431599 (24.987056)  adv_acc1: 6.250000 (3.431373)  adv_acc5: 6.250000 (4.901961)  time: 0.272372  data: 0.136110  max mem: 1623\n",
      "Test:  [55/79]  eta: 0:00:06  loss: 13.856272 (14.717068)  acc1: 6.250000 (4.129464)  acc5: 18.750000 (21.428571)  adv_loss: 23.636908 (24.874618)  adv_acc1: 6.250000 (3.571429)  adv_acc5: 6.250000 (4.910714)  time: 0.286888  data: 0.150619  max mem: 1623\n",
      "Test:  [60/79]  eta: 0:00:05  loss: 13.856272 (14.738635)  acc1: 6.250000 (4.200820)  acc5: 18.750000 (21.004098)  adv_loss: 23.668444 (24.880389)  adv_acc1: 6.250000 (3.586066)  adv_acc5: 6.250000 (4.918033)  time: 0.273549  data: 0.137354  max mem: 1623\n",
      "Test:  [65/79]  eta: 0:00:03  loss: 14.457070 (14.790233)  acc1: 0.000000 (3.882576)  acc5: 18.750000 (20.928030)  adv_loss: 24.463268 (24.915212)  adv_acc1: 0.000000 (3.314394)  adv_acc5: 0.000000 (4.545455)  time: 0.274136  data: 0.138034  max mem: 1623\n",
      "Test:  [70/79]  eta: 0:00:02  loss: 14.620044 (14.808658)  acc1: 0.000000 (3.697183)  acc5: 18.750000 (21.302817)  adv_loss: 24.862961 (24.970282)  adv_acc1: 0.000000 (3.169014)  adv_acc5: 0.000000 (4.313380)  time: 0.277010  data: 0.140841  max mem: 1623\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 14.662786 (14.750938)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (21.299342)  adv_loss: 24.862961 (24.914031)  adv_acc1: 0.000000 (3.618421)  adv_acc5: 0.000000 (4.687500)  time: 0.265315  data: 0.129117  max mem: 1623\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 14.662786 (14.743645)  acc1: 0.000000 (4.000000)  acc5: 25.000000 (21.200000)  adv_loss: 25.176540 (24.971769)  adv_acc1: 0.000000 (3.520000)  adv_acc5: 0.000000 (4.560000)  time: 0.256753  data: 0.125264  max mem: 1623\n",
      "Test: Total time: 0:00:21 (0.268066 s / it)\n",
      "* Acc@1 4.000 Acc@5 21.200 loss 14.744\n",
      "* adv_Acc@1 3.520 adv_Acc@5 4.560 adv_loss 24.972\n",
      "#######################################################################################################\n",
      "############################## evaluating adv_classifier trained on fgsm ##############################\n",
      "#######################################################################################################\n",
      ">>>>> applying attack: PGD(model_name=ViTWrapper, device=cuda:0, eps=0.3, alpha=0.023529411764705882, steps=15, random_start=True, attack_mode=default, return_type=float) \n",
      "Test:  [ 0/79]  eta: 0:01:41  loss: 11.006817 (11.006817)  acc1: 12.500000 (12.500000)  acc5: 37.500000 (37.500000)  adv_loss: 48.567707 (48.567707)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.280511  data: 0.111878  max mem: 1623\n",
      "Test:  [ 5/79]  eta: 0:01:35  loss: 11.006817 (11.411271)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (25.000000)  adv_loss: 49.445808 (50.664766)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.285125  data: 0.116613  max mem: 1623\n",
      "Test:  [10/79]  eta: 0:01:28  loss: 11.917227 (11.825965)  acc1: 6.250000 (5.681818)  acc5: 25.000000 (24.431818)  adv_loss: 51.929440 (51.602056)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.286120  data: 0.117485  max mem: 1623\n",
      "Test:  [15/79]  eta: 0:01:22  loss: 12.243925 (12.332522)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (22.265625)  adv_loss: 53.185410 (52.329397)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.284916  data: 0.116132  max mem: 1623\n",
      "Test:  [20/79]  eta: 0:01:16  loss: 12.243925 (12.178569)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (22.916667)  adv_loss: 53.194023 (52.209946)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.291472  data: 0.122477  max mem: 1623\n",
      "Test:  [25/79]  eta: 0:01:10  loss: 12.243925 (12.168630)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (23.317308)  adv_loss: 53.194023 (52.059010)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.303142  data: 0.133834  max mem: 1623\n",
      "Test:  [30/79]  eta: 0:01:03  loss: 12.282470 (12.195343)  acc1: 0.000000 (3.830645)  acc5: 25.000000 (24.395161)  adv_loss: 53.194023 (52.144768)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.307695  data: 0.138291  max mem: 1623\n",
      "Test:  [35/79]  eta: 0:00:57  loss: 12.161050 (12.265634)  acc1: 0.000000 (3.993056)  acc5: 25.000000 (23.784722)  adv_loss: 52.458481 (52.122425)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.309424  data: 0.139871  max mem: 1623\n",
      "Test:  [40/79]  eta: 0:00:50  loss: 12.146588 (12.163893)  acc1: 0.000000 (3.963415)  acc5: 25.000000 (23.628049)  adv_loss: 52.390278 (52.137012)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.315051  data: 0.145410  max mem: 1623\n",
      "Test:  [45/79]  eta: 0:00:44  loss: 12.004870 (12.097364)  acc1: 0.000000 (3.940217)  acc5: 25.000000 (23.913043)  adv_loss: 52.458481 (52.083857)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.310531  data: 0.140944  max mem: 1623\n",
      "Test:  [50/79]  eta: 0:00:37  loss: 11.792065 (12.083799)  acc1: 6.250000 (4.044118)  acc5: 25.000000 (24.877451)  adv_loss: 51.631950 (52.056138)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.305291  data: 0.135543  max mem: 1623\n",
      "Test:  [55/79]  eta: 0:00:31  loss: 11.646162 (12.092948)  acc1: 6.250000 (4.129464)  acc5: 25.000000 (24.218750)  adv_loss: 51.361904 (52.028860)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.319940  data: 0.150167  max mem: 1623\n",
      "Test:  [60/79]  eta: 0:00:24  loss: 11.950278 (12.096414)  acc1: 6.250000 (4.200820)  acc5: 25.000000 (24.077869)  adv_loss: 51.361904 (52.020970)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.307283  data: 0.137580  max mem: 1623\n",
      "Test:  [65/79]  eta: 0:00:18  loss: 12.443351 (12.123376)  acc1: 0.000000 (3.882576)  acc5: 25.000000 (23.674242)  adv_loss: 52.363365 (52.109877)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.308448  data: 0.138664  max mem: 1623\n",
      "Test:  [70/79]  eta: 0:00:11  loss: 12.285851 (12.124089)  acc1: 0.000000 (3.697183)  acc5: 18.750000 (23.767606)  adv_loss: 52.895382 (52.175857)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.311086  data: 0.141438  max mem: 1623\n",
      "Test:  [75/79]  eta: 0:00:05  loss: 12.285851 (12.092785)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (24.177632)  adv_loss: 52.863346 (52.042011)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.299371  data: 0.129838  max mem: 1623\n",
      "Test:  [78/79]  eta: 0:00:01  loss: 12.237529 (12.061623)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (24.240000)  adv_loss: 52.895382 (52.096024)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.255013  data: 0.125932  max mem: 1623\n",
      "Test: Total time: 0:01:42 (1.292472 s / it)\n",
      "* Acc@1 4.000 Acc@5 24.240 loss 12.062\n",
      "* adv_Acc@1 0.000 adv_Acc@5 0.000 adv_loss 52.096\n",
      ">>>>> applying attack: CW(model_name=ViTWrapper, device=cuda:0, c=10, kappa=0, steps=30, lr=0.003, attack_mode=default, return_type=float) \n",
      "Test:  [ 0/79]  eta: 0:01:48  loss: 11.006817 (11.006817)  acc1: 12.500000 (12.500000)  acc5: 37.500000 (37.500000)  adv_loss: 11.043774 (11.043774)  adv_acc1: 12.500000 (12.500000)  adv_acc5: 37.500000 (37.500000)  time: 1.373513  data: 0.113910  max mem: 1623\n",
      "Test:  [ 5/79]  eta: 0:01:36  loss: 11.006817 (11.411271)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (25.000000)  adv_loss: 11.043774 (11.436049)  adv_acc1: 6.250000 (8.333333)  adv_acc5: 25.000000 (25.000000)  time: 1.302811  data: 0.117682  max mem: 1623\n",
      "Test:  [10/79]  eta: 0:01:13  loss: 11.917227 (11.825965)  acc1: 6.250000 (5.681818)  acc5: 25.000000 (24.431818)  adv_loss: 11.919601 (11.842247)  adv_acc1: 6.250000 (5.681818)  adv_acc5: 25.000000 (24.431818)  time: 1.070706  data: 0.117284  max mem: 1623\n",
      "Test:  [15/79]  eta: 0:00:56  loss: 12.243925 (12.332522)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (22.265625)  adv_loss: 12.271987 (12.343716)  adv_acc1: 0.000000 (3.906250)  adv_acc5: 18.750000 (22.265625)  time: 0.884231  data: 0.115601  max mem: 1623\n",
      "Test:  [20/79]  eta: 0:00:56  loss: 12.243925 (12.178569)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (22.916667)  adv_loss: 12.271987 (12.188224)  adv_acc1: 0.000000 (4.166667)  adv_acc5: 18.750000 (22.916667)  time: 0.929075  data: 0.121929  max mem: 1623\n",
      "Test:  [25/79]  eta: 0:00:51  loss: 12.243925 (12.168630)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (23.317308)  adv_loss: 12.271987 (12.180836)  adv_acc1: 0.000000 (4.086538)  adv_acc5: 18.750000 (23.317308)  time: 0.839575  data: 0.133288  max mem: 1623\n",
      "Test:  [30/79]  eta: 0:00:44  loss: 12.282470 (12.195343)  acc1: 0.000000 (3.830645)  acc5: 25.000000 (24.395161)  adv_loss: 12.282468 (12.205583)  adv_acc1: 0.000000 (3.629032)  adv_acc5: 25.000000 (24.395161)  time: 0.822597  data: 0.138467  max mem: 1623\n",
      "Test:  [35/79]  eta: 0:00:39  loss: 12.161050 (12.265634)  acc1: 0.000000 (3.993056)  acc5: 25.000000 (23.784722)  adv_loss: 12.161049 (12.276049)  adv_acc1: 0.000000 (3.819444)  adv_acc5: 25.000000 (23.784722)  time: 0.914574  data: 0.140670  max mem: 1623\n",
      "Test:  [40/79]  eta: 0:00:35  loss: 12.146588 (12.163893)  acc1: 0.000000 (3.963415)  acc5: 25.000000 (23.628049)  adv_loss: 12.149730 (12.174298)  adv_acc1: 0.000000 (3.810976)  adv_acc5: 25.000000 (23.628049)  time: 0.879592  data: 0.147032  max mem: 1623\n",
      "Test:  [45/79]  eta: 0:00:32  loss: 12.004870 (12.097364)  acc1: 0.000000 (3.940217)  acc5: 25.000000 (23.913043)  adv_loss: 12.004873 (12.107084)  adv_acc1: 0.000000 (3.804348)  adv_acc5: 25.000000 (23.913043)  time: 0.964749  data: 0.142732  max mem: 1623\n",
      "Test:  [50/79]  eta: 0:00:27  loss: 11.792065 (12.083799)  acc1: 6.250000 (4.044118)  acc5: 25.000000 (24.877451)  adv_loss: 11.803447 (12.093422)  adv_acc1: 6.250000 (3.921569)  adv_acc5: 25.000000 (24.877451)  time: 1.037908  data: 0.137298  max mem: 1623\n",
      "Test:  [55/79]  eta: 0:00:23  loss: 11.646162 (12.092948)  acc1: 6.250000 (4.129464)  acc5: 25.000000 (24.218750)  adv_loss: 11.666259 (12.101838)  adv_acc1: 6.250000 (4.017857)  adv_acc5: 25.000000 (24.218750)  time: 1.119309  data: 0.151334  max mem: 1623\n",
      "Test:  [60/79]  eta: 0:00:18  loss: 11.950278 (12.096414)  acc1: 6.250000 (4.200820)  acc5: 25.000000 (24.077869)  adv_loss: 11.965483 (12.105202)  adv_acc1: 6.250000 (4.098361)  adv_acc5: 25.000000 (24.077869)  time: 1.094656  data: 0.137871  max mem: 1623\n",
      "Test:  [65/79]  eta: 0:00:13  loss: 12.443351 (12.123376)  acc1: 0.000000 (3.882576)  acc5: 25.000000 (23.674242)  adv_loss: 12.443352 (12.131498)  adv_acc1: 0.000000 (3.787879)  adv_acc5: 25.000000 (23.674242)  time: 0.904297  data: 0.138146  max mem: 1623\n",
      "Test:  [70/79]  eta: 0:00:08  loss: 12.285851 (12.124089)  acc1: 0.000000 (3.697183)  acc5: 18.750000 (23.767606)  adv_loss: 12.285853 (12.131666)  adv_acc1: 0.000000 (3.609155)  adv_acc5: 18.750000 (23.767606)  time: 0.816841  data: 0.140670  max mem: 1623\n",
      "Test:  [75/79]  eta: 0:00:03  loss: 12.285851 (12.092785)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (24.177632)  adv_loss: 12.285853 (12.101880)  adv_acc1: 0.000000 (3.947368)  adv_acc5: 18.750000 (24.177632)  time: 0.805413  data: 0.129107  max mem: 1623\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 12.237529 (12.061623)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (24.240000)  adv_loss: 12.237530 (12.070373)  adv_acc1: 0.000000 (3.840000)  adv_acc5: 18.750000 (24.240000)  time: 0.699549  data: 0.125078  max mem: 1623\n",
      "Test: Total time: 0:01:12 (0.911998 s / it)\n",
      "* Acc@1 4.000 Acc@5 24.240 loss 12.062\n",
      "* adv_Acc@1 3.840 adv_Acc@5 24.240 adv_loss 12.070\n",
      ">>>>> applying attack: FGSM(model_name=ViTWrapper, device=cuda:0, eps=0.03, attack_mode=default, return_type=float) \n",
      "Test:  [ 0/79]  eta: 0:00:19  loss: 11.006817 (11.006817)  acc1: 12.500000 (12.500000)  acc5: 37.500000 (37.500000)  adv_loss: 24.085732 (24.085732)  adv_acc1: 12.500000 (12.500000)  adv_acc5: 12.500000 (12.500000)  time: 0.248875  data: 0.112116  max mem: 1623\n",
      "Test:  [ 5/79]  eta: 0:00:18  loss: 11.006817 (11.411271)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (25.000000)  adv_loss: 24.375746 (25.909140)  adv_acc1: 6.250000 (8.333333)  adv_acc5: 12.500000 (11.458333)  time: 0.251582  data: 0.115403  max mem: 1623\n",
      "Test:  [10/79]  eta: 0:00:17  loss: 11.917227 (11.825965)  acc1: 6.250000 (5.681818)  acc5: 25.000000 (24.431818)  adv_loss: 26.921518 (26.777751)  adv_acc1: 6.250000 (5.681818)  adv_acc5: 6.250000 (7.386364)  time: 0.252365  data: 0.116200  max mem: 1623\n",
      "Test:  [15/79]  eta: 0:00:16  loss: 12.243925 (12.332522)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (22.265625)  adv_loss: 28.157070 (27.423082)  adv_acc1: 0.000000 (3.906250)  adv_acc5: 0.000000 (5.078125)  time: 0.250714  data: 0.114545  max mem: 1623\n",
      "Test:  [20/79]  eta: 0:00:15  loss: 12.243925 (12.178569)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (22.916667)  adv_loss: 28.157070 (27.304694)  adv_acc1: 0.000000 (4.166667)  adv_acc5: 0.000000 (5.059524)  time: 0.257260  data: 0.121106  max mem: 1623\n",
      "Test:  [25/79]  eta: 0:00:14  loss: 12.243925 (12.168630)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (23.317308)  adv_loss: 28.016113 (27.306721)  adv_acc1: 0.000000 (4.326923)  adv_acc5: 0.000000 (5.048077)  time: 0.268956  data: 0.132863  max mem: 1623\n",
      "Test:  [30/79]  eta: 0:00:13  loss: 12.282470 (12.195343)  acc1: 0.000000 (3.830645)  acc5: 25.000000 (24.395161)  adv_loss: 27.497435 (27.343973)  adv_acc1: 0.000000 (3.830645)  adv_acc5: 0.000000 (4.435484)  time: 0.273617  data: 0.137590  max mem: 1623\n",
      "Test:  [35/79]  eta: 0:00:11  loss: 12.161050 (12.265634)  acc1: 0.000000 (3.993056)  acc5: 25.000000 (23.784722)  adv_loss: 27.445847 (27.356716)  adv_acc1: 0.000000 (3.993056)  adv_acc5: 0.000000 (4.513889)  time: 0.275722  data: 0.139625  max mem: 1623\n",
      "Test:  [40/79]  eta: 0:00:10  loss: 12.146588 (12.163893)  acc1: 0.000000 (3.963415)  acc5: 25.000000 (23.628049)  adv_loss: 27.329895 (27.287895)  adv_acc1: 0.000000 (3.963415)  adv_acc5: 0.000000 (4.725610)  time: 0.281643  data: 0.145616  max mem: 1623\n",
      "Test:  [45/79]  eta: 0:00:09  loss: 12.004870 (12.097364)  acc1: 0.000000 (3.940217)  acc5: 25.000000 (23.913043)  adv_loss: 27.294113 (27.228786)  adv_acc1: 0.000000 (3.940217)  adv_acc5: 0.000000 (4.755435)  time: 0.277798  data: 0.141687  max mem: 1623\n",
      "Test:  [50/79]  eta: 0:00:07  loss: 11.792065 (12.083799)  acc1: 6.250000 (4.044118)  acc5: 25.000000 (24.877451)  adv_loss: 26.652597 (27.204371)  adv_acc1: 6.250000 (4.044118)  adv_acc5: 6.250000 (4.779412)  time: 0.272621  data: 0.136425  max mem: 1623\n",
      "Test:  [55/79]  eta: 0:00:06  loss: 11.646162 (12.092948)  acc1: 6.250000 (4.129464)  acc5: 25.000000 (24.218750)  adv_loss: 26.135328 (27.159433)  adv_acc1: 6.250000 (4.129464)  adv_acc5: 6.250000 (4.799107)  time: 0.287254  data: 0.151169  max mem: 1623\n",
      "Test:  [60/79]  eta: 0:00:05  loss: 11.950278 (12.096414)  acc1: 6.250000 (4.200820)  acc5: 25.000000 (24.077869)  adv_loss: 26.652597 (27.150171)  adv_acc1: 6.250000 (4.200820)  adv_acc5: 6.250000 (4.918033)  time: 0.274203  data: 0.138073  max mem: 1623\n",
      "Test:  [65/79]  eta: 0:00:03  loss: 12.443351 (12.123376)  acc1: 0.000000 (3.882576)  acc5: 25.000000 (23.674242)  adv_loss: 27.838800 (27.223770)  adv_acc1: 0.000000 (3.882576)  adv_acc5: 0.000000 (4.640152)  time: 0.274314  data: 0.138208  max mem: 1623\n",
      "Test:  [70/79]  eta: 0:00:02  loss: 12.285851 (12.124089)  acc1: 0.000000 (3.697183)  acc5: 18.750000 (23.767606)  adv_loss: 28.015530 (27.263280)  adv_acc1: 0.000000 (3.697183)  adv_acc5: 0.000000 (4.577465)  time: 0.277042  data: 0.140895  max mem: 1623\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 12.285851 (12.092785)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (24.177632)  adv_loss: 27.838800 (27.199038)  adv_acc1: 0.000000 (4.111842)  adv_acc5: 6.250000 (5.016447)  time: 0.265343  data: 0.129154  max mem: 1623\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 12.237529 (12.061623)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (24.240000)  adv_loss: 27.796087 (27.169391)  adv_acc1: 0.000000 (4.000000)  adv_acc5: 6.250000 (4.960000)  time: 0.256788  data: 0.125226  max mem: 1623\n",
      "Test: Total time: 0:00:21 (0.267887 s / it)\n",
      "* Acc@1 4.000 Acc@5 24.240 loss 12.062\n",
      "* adv_Acc@1 4.000 adv_Acc@5 4.960 adv_loss 27.169\n",
      "########################################################################################################\n",
      "############################## evaluating adv_classifier trained on clean ##############################\n",
      "########################################################################################################\n",
      ">>>>> applying attack: PGD(model_name=ViTWrapper, device=cuda:0, eps=0.3, alpha=0.023529411764705882, steps=15, random_start=True, attack_mode=default, return_type=float) \n",
      "Test:  [ 0/79]  eta: 0:01:41  loss: 0.003950 (0.003950)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  adv_loss: 35.077042 (35.077042)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.281594  data: 0.114054  max mem: 1623\n",
      "Test:  [ 5/79]  eta: 0:01:35  loss: 0.050309 (0.075488)  acc1: 93.750000 (96.875000)  acc5: 100.000000 (100.000000)  adv_loss: 35.637630 (35.824354)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.285231  data: 0.116816  max mem: 1623\n",
      "Test:  [10/79]  eta: 0:01:28  loss: 0.050309 (0.083421)  acc1: 100.000000 (97.159091)  acc5: 100.000000 (100.000000)  adv_loss: 35.412766 (35.677356)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.285498  data: 0.116650  max mem: 1623\n",
      "Test:  [15/79]  eta: 0:01:22  loss: 0.050309 (0.093181)  acc1: 93.750000 (96.875000)  acc5: 100.000000 (100.000000)  adv_loss: 35.335697 (35.600500)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.283978  data: 0.114981  max mem: 1623\n",
      "Test:  [20/79]  eta: 0:01:16  loss: 0.060876 (0.102474)  acc1: 93.750000 (96.726190)  acc5: 100.000000 (100.000000)  adv_loss: 35.335697 (35.447615)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.290548  data: 0.121298  max mem: 1623\n",
      "Test:  [25/79]  eta: 0:01:10  loss: 0.038588 (0.089864)  acc1: 100.000000 (97.115385)  acc5: 100.000000 (100.000000)  adv_loss: 35.146343 (35.424622)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.302000  data: 0.132581  max mem: 1623\n",
      "Test:  [30/79]  eta: 0:01:03  loss: 0.038588 (0.084775)  acc1: 100.000000 (97.379032)  acc5: 100.000000 (100.000000)  adv_loss: 35.146343 (35.390461)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.307252  data: 0.137735  max mem: 1623\n",
      "Test:  [35/79]  eta: 0:00:57  loss: 0.029115 (0.080989)  acc1: 100.000000 (97.569444)  acc5: 100.000000 (100.000000)  adv_loss: 35.291107 (35.404906)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.309105  data: 0.139566  max mem: 1623\n",
      "Test:  [40/79]  eta: 0:00:50  loss: 0.038588 (0.078956)  acc1: 100.000000 (97.713415)  acc5: 100.000000 (100.000000)  adv_loss: 35.470242 (35.460470)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.315081  data: 0.145637  max mem: 1623\n",
      "Test:  [45/79]  eta: 0:00:44  loss: 0.056058 (0.083779)  acc1: 100.000000 (97.690217)  acc5: 100.000000 (100.000000)  adv_loss: 35.625515 (35.481514)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.311167  data: 0.141671  max mem: 1623\n",
      "Test:  [50/79]  eta: 0:00:37  loss: 0.048692 (0.080641)  acc1: 100.000000 (97.794118)  acc5: 100.000000 (100.000000)  adv_loss: 35.693924 (35.477941)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.305585  data: 0.136130  max mem: 1623\n",
      "Test:  [55/79]  eta: 0:00:31  loss: 0.059690 (0.082373)  acc1: 100.000000 (97.656250)  acc5: 100.000000 (100.000000)  adv_loss: 35.773151 (35.503673)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.320591  data: 0.151048  max mem: 1623\n",
      "Test:  [60/79]  eta: 0:00:24  loss: 0.061207 (0.084029)  acc1: 100.000000 (97.540984)  acc5: 100.000000 (100.000000)  adv_loss: 35.625515 (35.516493)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.307482  data: 0.137947  max mem: 1623\n",
      "Test:  [65/79]  eta: 0:00:18  loss: 0.021203 (0.079007)  acc1: 100.000000 (97.727273)  acc5: 100.000000 (100.000000)  adv_loss: 35.418137 (35.468930)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.308063  data: 0.138478  max mem: 1623\n",
      "Test:  [70/79]  eta: 0:00:11  loss: 0.021203 (0.075005)  acc1: 100.000000 (97.887324)  acc5: 100.000000 (100.000000)  adv_loss: 35.380188 (35.440785)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.311216  data: 0.141585  max mem: 1623\n",
      "Test:  [75/79]  eta: 0:00:05  loss: 0.016655 (0.071368)  acc1: 100.000000 (98.026316)  acc5: 100.000000 (100.000000)  adv_loss: 35.216679 (35.437517)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.299159  data: 0.129457  max mem: 1623\n",
      "Test:  [78/79]  eta: 0:00:01  loss: 0.015308 (0.080008)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (100.000000)  adv_loss: 35.198669 (35.418482)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.254290  data: 0.125521  max mem: 1623\n",
      "Test: Total time: 0:01:42 (1.292155 s / it)\n",
      "* Acc@1 98.000 Acc@5 100.000 loss 0.080\n",
      "* adv_Acc@1 0.000 adv_Acc@5 0.000 adv_loss 35.418\n",
      ">>>>> applying attack: CW(model_name=ViTWrapper, device=cuda:0, c=10, kappa=0, steps=30, lr=0.003, attack_mode=default, return_type=float) \n",
      "Test:  [ 0/79]  eta: 0:01:13  loss: 0.003950 (0.003950)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  adv_loss: 0.570802 (0.570802)  adv_acc1: 81.250000 (81.250000)  adv_acc5: 100.000000 (100.000000)  time: 0.930035  data: 0.113205  max mem: 1623\n",
      "Test:  [ 5/79]  eta: 0:01:38  loss: 0.050309 (0.075488)  acc1: 93.750000 (96.875000)  acc5: 100.000000 (100.000000)  adv_loss: 0.496168 (0.608517)  adv_acc1: 81.250000 (77.083333)  adv_acc5: 100.000000 (100.000000)  time: 1.331318  data: 0.116173  max mem: 1623\n",
      "Test:  [10/79]  eta: 0:01:35  loss: 0.050309 (0.083421)  acc1: 100.000000 (97.159091)  acc5: 100.000000 (100.000000)  adv_loss: 0.496168 (0.568921)  adv_acc1: 81.250000 (77.272727)  adv_acc5: 100.000000 (99.431818)  time: 1.390165  data: 0.116969  max mem: 1623\n",
      "Test:  [15/79]  eta: 0:01:28  loss: 0.050309 (0.093181)  acc1: 93.750000 (96.875000)  acc5: 100.000000 (100.000000)  adv_loss: 0.496168 (0.570349)  adv_acc1: 75.000000 (76.953125)  adv_acc5: 100.000000 (99.609375)  time: 1.381804  data: 0.115083  max mem: 1623\n",
      "Test:  [20/79]  eta: 0:01:18  loss: 0.060876 (0.102474)  acc1: 93.750000 (96.726190)  acc5: 100.000000 (100.000000)  adv_loss: 0.516826 (0.585112)  adv_acc1: 81.250000 (76.785714)  adv_acc5: 100.000000 (99.404762)  time: 1.343645  data: 0.121946  max mem: 1623\n",
      "Test:  [25/79]  eta: 0:01:13  loss: 0.038588 (0.089864)  acc1: 100.000000 (97.115385)  acc5: 100.000000 (100.000000)  adv_loss: 0.482350 (0.536915)  adv_acc1: 81.250000 (78.605769)  adv_acc5: 100.000000 (99.519231)  time: 1.366888  data: 0.133627  max mem: 1623\n",
      "Test:  [30/79]  eta: 0:01:06  loss: 0.038588 (0.084775)  acc1: 100.000000 (97.379032)  acc5: 100.000000 (100.000000)  adv_loss: 0.463294 (0.516052)  adv_acc1: 81.250000 (79.637097)  adv_acc5: 100.000000 (99.596774)  time: 1.330577  data: 0.138524  max mem: 1623\n",
      "Test:  [35/79]  eta: 0:00:58  loss: 0.029115 (0.080989)  acc1: 100.000000 (97.569444)  acc5: 100.000000 (100.000000)  adv_loss: 0.463294 (0.516544)  adv_acc1: 81.250000 (79.513889)  adv_acc5: 100.000000 (99.652778)  time: 1.269847  data: 0.141078  max mem: 1623\n",
      "Test:  [40/79]  eta: 0:00:51  loss: 0.038588 (0.078956)  acc1: 100.000000 (97.713415)  acc5: 100.000000 (100.000000)  adv_loss: 0.444854 (0.509524)  adv_acc1: 81.250000 (80.030488)  adv_acc5: 100.000000 (99.695122)  time: 1.309354  data: 0.146603  max mem: 1623\n",
      "Test:  [45/79]  eta: 0:00:44  loss: 0.056058 (0.083779)  acc1: 100.000000 (97.690217)  acc5: 100.000000 (100.000000)  adv_loss: 0.480349 (0.509340)  adv_acc1: 81.250000 (80.298913)  adv_acc5: 100.000000 (99.728261)  time: 1.229876  data: 0.142035  max mem: 1623\n",
      "Test:  [50/79]  eta: 0:00:37  loss: 0.048692 (0.080641)  acc1: 100.000000 (97.794118)  acc5: 100.000000 (100.000000)  adv_loss: 0.480349 (0.497459)  adv_acc1: 81.250000 (80.882353)  adv_acc5: 100.000000 (99.754902)  time: 1.179437  data: 0.136399  max mem: 1623\n",
      "Test:  [55/79]  eta: 0:00:30  loss: 0.059690 (0.082373)  acc1: 100.000000 (97.656250)  acc5: 100.000000 (100.000000)  adv_loss: 0.498418 (0.500959)  adv_acc1: 81.250000 (80.691964)  adv_acc5: 100.000000 (99.776786)  time: 1.227119  data: 0.150863  max mem: 1623\n",
      "Test:  [60/79]  eta: 0:00:24  loss: 0.061207 (0.084029)  acc1: 100.000000 (97.540984)  acc5: 100.000000 (100.000000)  adv_loss: 0.521573 (0.506258)  adv_acc1: 81.250000 (80.327869)  adv_acc5: 100.000000 (99.795082)  time: 1.191521  data: 0.137832  max mem: 1623\n",
      "Test:  [65/79]  eta: 0:00:17  loss: 0.021203 (0.079007)  acc1: 100.000000 (97.727273)  acc5: 100.000000 (100.000000)  adv_loss: 0.420723 (0.494893)  adv_acc1: 81.250000 (80.871212)  adv_acc5: 100.000000 (99.810606)  time: 1.226212  data: 0.138727  max mem: 1623\n",
      "Test:  [70/79]  eta: 0:00:11  loss: 0.021203 (0.075005)  acc1: 100.000000 (97.887324)  acc5: 100.000000 (100.000000)  adv_loss: 0.521573 (0.502737)  adv_acc1: 81.250000 (80.809859)  adv_acc5: 100.000000 (99.823944)  time: 1.303045  data: 0.141199  max mem: 1623\n",
      "Test:  [75/79]  eta: 0:00:05  loss: 0.016655 (0.071368)  acc1: 100.000000 (98.026316)  acc5: 100.000000 (100.000000)  adv_loss: 0.439462 (0.496783)  adv_acc1: 81.250000 (80.921053)  adv_acc5: 100.000000 (99.835526)  time: 1.279353  data: 0.128847  max mem: 1623\n",
      "Test:  [78/79]  eta: 0:00:01  loss: 0.015308 (0.080008)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (100.000000)  adv_loss: 0.482014 (0.512122)  adv_acc1: 75.000000 (80.640000)  adv_acc5: 100.000000 (99.840000)  time: 1.289129  data: 0.124928  max mem: 1623\n",
      "Test: Total time: 0:01:41 (1.285205 s / it)\n",
      "* Acc@1 98.000 Acc@5 100.000 loss 0.080\n",
      "* adv_Acc@1 80.640 adv_Acc@5 99.840 adv_loss 0.512\n",
      ">>>>> applying attack: FGSM(model_name=ViTWrapper, device=cuda:0, eps=0.03, attack_mode=default, return_type=float) \n",
      "Test:  [ 0/79]  eta: 0:00:19  loss: 0.003950 (0.003950)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  adv_loss: 4.984804 (4.984804)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 62.500000 (62.500000)  time: 0.248143  data: 0.111757  max mem: 1623\n",
      "Test:  [ 5/79]  eta: 0:00:18  loss: 0.050309 (0.075488)  acc1: 93.750000 (96.875000)  acc5: 100.000000 (100.000000)  adv_loss: 5.339688 (5.962030)  adv_acc1: 6.250000 (7.291667)  adv_acc5: 50.000000 (52.083333)  time: 0.252279  data: 0.116057  max mem: 1623\n",
      "Test:  [10/79]  eta: 0:00:17  loss: 0.050309 (0.083421)  acc1: 100.000000 (97.159091)  acc5: 100.000000 (100.000000)  adv_loss: 5.775691 (5.701489)  adv_acc1: 12.500000 (14.772727)  adv_acc5: 56.250000 (55.113636)  time: 0.252783  data: 0.116567  max mem: 1623\n",
      "Test:  [15/79]  eta: 0:00:16  loss: 0.050309 (0.093181)  acc1: 93.750000 (96.875000)  acc5: 100.000000 (100.000000)  adv_loss: 5.339688 (5.588511)  adv_acc1: 12.500000 (17.968750)  adv_acc5: 56.250000 (57.031250)  time: 0.250913  data: 0.114647  max mem: 1623\n",
      "Test:  [20/79]  eta: 0:00:15  loss: 0.060876 (0.102474)  acc1: 93.750000 (96.726190)  acc5: 100.000000 (100.000000)  adv_loss: 5.499987 (5.718628)  adv_acc1: 12.500000 (16.666667)  adv_acc5: 56.250000 (56.250000)  time: 0.258767  data: 0.122553  max mem: 1623\n",
      "Test:  [25/79]  eta: 0:00:14  loss: 0.038588 (0.089864)  acc1: 100.000000 (97.115385)  acc5: 100.000000 (100.000000)  adv_loss: 5.411205 (5.654711)  adv_acc1: 18.750000 (16.826923)  adv_acc5: 56.250000 (55.048077)  time: 0.271418  data: 0.135188  max mem: 1623\n",
      "Test:  [30/79]  eta: 0:00:13  loss: 0.038588 (0.084775)  acc1: 100.000000 (97.379032)  acc5: 100.000000 (100.000000)  adv_loss: 5.083531 (5.552180)  adv_acc1: 18.750000 (17.943548)  adv_acc5: 56.250000 (56.048387)  time: 0.275827  data: 0.139659  max mem: 1623\n",
      "Test:  [35/79]  eta: 0:00:11  loss: 0.029115 (0.080989)  acc1: 100.000000 (97.569444)  acc5: 100.000000 (100.000000)  adv_loss: 5.281916 (5.511020)  adv_acc1: 18.750000 (18.229167)  adv_acc5: 56.250000 (55.729167)  time: 0.277730  data: 0.141635  max mem: 1623\n",
      "Test:  [40/79]  eta: 0:00:10  loss: 0.038588 (0.078956)  acc1: 100.000000 (97.713415)  acc5: 100.000000 (100.000000)  adv_loss: 5.082420 (5.513557)  adv_acc1: 18.750000 (17.682927)  adv_acc5: 56.250000 (56.859756)  time: 0.283347  data: 0.147196  max mem: 1623\n",
      "Test:  [45/79]  eta: 0:00:09  loss: 0.056058 (0.083779)  acc1: 100.000000 (97.690217)  acc5: 100.000000 (100.000000)  adv_loss: 5.286407 (5.530273)  adv_acc1: 18.750000 (17.527174)  adv_acc5: 56.250000 (56.250000)  time: 0.278379  data: 0.142302  max mem: 1623\n",
      "Test:  [50/79]  eta: 0:00:07  loss: 0.048692 (0.080641)  acc1: 100.000000 (97.794118)  acc5: 100.000000 (100.000000)  adv_loss: 5.384952 (5.500823)  adv_acc1: 12.500000 (17.401961)  adv_acc5: 56.250000 (55.882353)  time: 0.274534  data: 0.138403  max mem: 1623\n",
      "Test:  [55/79]  eta: 0:00:06  loss: 0.059690 (0.082373)  acc1: 100.000000 (97.656250)  acc5: 100.000000 (100.000000)  adv_loss: 5.384952 (5.481617)  adv_acc1: 12.500000 (17.522321)  adv_acc5: 50.000000 (56.026786)  time: 0.290984  data: 0.154846  max mem: 1623\n",
      "Test:  [60/79]  eta: 0:00:05  loss: 0.061207 (0.084029)  acc1: 100.000000 (97.540984)  acc5: 100.000000 (100.000000)  adv_loss: 5.486139 (5.504612)  adv_acc1: 12.500000 (17.008197)  adv_acc5: 50.000000 (55.327869)  time: 0.279341  data: 0.143195  max mem: 1623\n",
      "Test:  [65/79]  eta: 0:00:03  loss: 0.021203 (0.079007)  acc1: 100.000000 (97.727273)  acc5: 100.000000 (100.000000)  adv_loss: 5.292174 (5.470165)  adv_acc1: 12.500000 (16.950758)  adv_acc5: 50.000000 (55.208333)  time: 0.280822  data: 0.144684  max mem: 1623\n",
      "Test:  [70/79]  eta: 0:00:02  loss: 0.021203 (0.075005)  acc1: 100.000000 (97.887324)  acc5: 100.000000 (100.000000)  adv_loss: 5.292174 (5.458290)  adv_acc1: 12.500000 (16.461268)  adv_acc5: 50.000000 (55.369718)  time: 0.284496  data: 0.148281  max mem: 1623\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 0.016655 (0.071368)  acc1: 100.000000 (98.026316)  acc5: 100.000000 (100.000000)  adv_loss: 5.391455 (5.476925)  adv_acc1: 12.500000 (16.611842)  adv_acc5: 50.000000 (55.098684)  time: 0.272507  data: 0.136288  max mem: 1623\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 0.015308 (0.080008)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (100.000000)  adv_loss: 5.505926 (5.520856)  adv_acc1: 12.500000 (16.480000)  adv_acc5: 50.000000 (55.200000)  time: 0.263519  data: 0.131842  max mem: 1623\n",
      "Test: Total time: 0:00:21 (0.271243 s / it)\n",

      "* Acc@1 98.000 Acc@5 100.000 loss 0.080\n",
      "* adv_Acc@1 16.480 adv_Acc@5 55.200 adv_loss 5.521\n"
     ]
    }
   ],
   "source": [
    "attacks = [x for x in loader_dict.keys()]\n",
    "for attack in attacks:\n",
    "    \n",
    "    pstr = \"#\"*30 + f''' evaluating adv_classifier trained on {attack} ''' + \"#\"*30\n",
    "    print(len(pstr)*\"#\")\n",
    "    print(pstr)\n",
    "    print(len(pstr)*\"#\")\n",
    "    \n",
    "    adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                             num_labels=len(CLASS_SUBSET))\n",
    "    adv_classifier.to(DEVICE)\n",
    "    \n",
    "    save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version)\n",
    "    save_file = f\"{attack}.pt\"\n",
    "    adv_classifier.load_state_dict(torch.load(str(save_path) + \"/\" + save_file))\n",
    "    \n",
    "    vits = ViTWrapper(model, adv_classifier, transform=None)\n",
    "\n",
    "    for applied_attack in attacks:\n",
    "        \n",
    "        if applied_attack == \"pgd\":\n",
    "            ev_attack = PGD(vits, eps=0.3, alpha=6/255, steps=15)\n",
    "        elif applied_attack == \"cw\":\n",
    "            ev_attack = CW(vits, c=10, lr=0.003, steps=30)\n",
    "        elif applied_attack == \"fgsm\":\n",
    "            ev_attack = FGSM(vits, eps=0.03)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        model.train()\n",
    "        print(\">\"*5 + f''' applying attack: {ev_attack} ''')\n",
    "        logger_dict, logger = validate_network(model, \n",
    "                                               adv_classifier, \n",
    "                                               loader_dict[\"clean\"][\"validation\"],\n",
    "                                               criterion=nn.CrossEntropyLoss(),\n",
    "                                               tensor_dir=None, \n",
    "                                               adversarial_attack=ev_attack,\n",
    "                                               n=4, \n",
    "                                               avgpool_patchtokens=False, \n",
    "                                               path_predictions=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on full pipeline with post-hoc as multiplexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load clean_classifier\n",
    "name=\"clean\"\n",
    "clean_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                    num_labels=len(CLASS_SUBSET))\n",
    "clean_classifier.to(DEVICE)\n",
    "clean_classifier.load_state_dict(torch.load(f\"/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/{version}/{name}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## Validating Posthoc: cw and adv_classifier: cw on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 1.307345 (1.307345)  acc1: 75.000000 (75.000000)  acc5: 87.500000 (87.500000)  time: 0.214432  data: 0.173989  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 1.143139 (1.133734)  acc1: 75.000000 (78.000000)  acc5: 87.500000 (86.000000)  time: 0.219044  data: 0.185667  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.219371 s / it)\n",
      "* Acc@1 78.000 Acc@5 86.000 loss 1.134\n",
      "################################################## Validating Posthoc: cw and adv_classifier: cw on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:02  loss: 5.649935 (5.649935)  acc1: 50.000000 (50.000000)  acc5: 62.500000 (62.500000)  time: 0.534863  data: 0.494440  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 6.480145 (6.743094)  acc1: 31.250000 (38.000000)  acc5: 62.500000 (66.000000)  time: 0.324902  data: 0.291575  max mem: 210\n",
      "Test: Total time: 0:00:01 (0.325233 s / it)\n",
      "* Acc@1 38.000 Acc@5 66.000 loss 6.743\n",
      "################################################## Validating Posthoc: cw and adv_classifier: cw on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:02  loss: 2.073831 (2.073831)  acc1: 75.000000 (75.000000)  acc5: 75.000000 (75.000000)  time: 0.567346  data: 0.527024  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.953828 (1.435039)  acc1: 75.000000 (78.000000)  acc5: 75.000000 (80.000000)  time: 0.345518  data: 0.312010  max mem: 210\n",
      "Test: Total time: 0:00:01 (0.345850 s / it)\n",
      "* Acc@1 78.000 Acc@5 80.000 loss 1.435\n",
      "################################################## Validating Posthoc: cw and adv_classifier: cw on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:42  loss: 13.525923 (13.525923)  acc1: 12.500000 (12.500000)  acc5: 18.750000 (18.750000)  time: 0.539719  data: 0.499344  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:41  loss: 13.525923 (14.304344)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (26.041667)  time: 0.558826  data: 0.518431  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:38  loss: 14.199209 (14.420988)  acc1: 6.250000 (6.250000)  acc5: 25.000000 (23.863636)  time: 0.559768  data: 0.519416  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:35  loss: 14.369546 (14.993355)  acc1: 0.000000 (4.296875)  acc5: 18.750000 (19.531250)  time: 0.560960  data: 0.520625  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:33  loss: 14.728824 (14.972114)  acc1: 0.000000 (4.464286)  acc5: 18.750000 (20.238095)  time: 0.572157  data: 0.531811  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:32  loss: 14.875451 (14.904237)  acc1: 0.000000 (4.567308)  acc5: 18.750000 (20.432692)  time: 0.608295  data: 0.568020  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:29  loss: 15.143551 (14.824061)  acc1: 0.000000 (4.032258)  acc5: 18.750000 (20.362903)  time: 0.612796  data: 0.572567  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:25  loss: 15.143551 (14.901293)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (21.180556)  time: 0.597851  data: 0.557663  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:22  loss: 14.830462 (14.798985)  acc1: 0.000000 (4.268293)  acc5: 18.750000 (21.341463)  time: 0.597941  data: 0.557831  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:19  loss: 14.137599 (14.686963)  acc1: 0.000000 (4.619565)  acc5: 25.000000 (22.010870)  time: 0.551667  data: 0.511560  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:16  loss: 14.249454 (14.673475)  acc1: 6.250000 (4.779412)  acc5: 25.000000 (22.058824)  time: 0.571363  data: 0.531251  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:14  loss: 13.856274 (14.595517)  acc1: 6.250000 (4.799107)  acc5: 18.750000 (22.098214)  time: 0.661753  data: 0.621652  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:11  loss: 13.760896 (14.611113)  acc1: 6.250000 (4.918033)  acc5: 18.750000 (21.721311)  time: 0.658904  data: 0.618726  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:08  loss: 14.420955 (14.672373)  acc1: 0.000000 (4.545455)  acc5: 18.750000 (21.590909)  time: 0.668818  data: 0.628551  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:05  loss: 14.603889 (14.681700)  acc1: 0.000000 (4.401408)  acc5: 18.750000 (22.007042)  time: 0.630305  data: 0.589927  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:02  loss: 14.662786 (14.632332)  acc1: 0.000000 (4.769737)  acc5: 18.750000 (21.957237)  time: 0.564207  data: 0.523740  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 14.662786 (14.629543)  acc1: 0.000000 (4.640000)  acc5: 25.000000 (21.840000)  time: 0.517979  data: 0.478868  max mem: 210\n",
      "Test: Total time: 0:00:46 (0.588676 s / it)\n",
      "* Acc@1 4.640 Acc@5 21.840 loss 14.630\n",
      "################################################## Validating Posthoc: cw and adv_classifier: fgsm on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 1.307345 (1.307345)  acc1: 75.000000 (75.000000)  acc5: 87.500000 (87.500000)  time: 0.192384  data: 0.151429  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 1.143139 (1.133734)  acc1: 75.000000 (78.000000)  acc5: 87.500000 (86.000000)  time: 0.151673  data: 0.117963  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.152015 s / it)\n",
      "* Acc@1 78.000 Acc@5 86.000 loss 1.134\n",
      "################################################## Validating Posthoc: cw and adv_classifier: fgsm on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 5.649935 (5.649935)  acc1: 50.000000 (50.000000)  acc5: 62.500000 (62.500000)  time: 0.189451  data: 0.148428  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 6.480145 (6.743094)  acc1: 31.250000 (38.000000)  acc5: 62.500000 (66.000000)  time: 0.151013  data: 0.117366  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.151339 s / it)\n",
      "* Acc@1 38.000 Acc@5 66.000 loss 6.743\n",
      "################################################## Validating Posthoc: cw and adv_classifier: fgsm on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 2.073831 (2.073831)  acc1: 75.000000 (75.000000)  acc5: 75.000000 (75.000000)  time: 0.190943  data: 0.150450  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.953828 (1.435039)  acc1: 75.000000 (78.000000)  acc5: 75.000000 (80.000000)  time: 0.150274  data: 0.116782  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.150606 s / it)\n",
      "* Acc@1 78.000 Acc@5 80.000 loss 1.435\n",
      "################################################## Validating Posthoc: cw and adv_classifier: fgsm on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:24  loss: 11.006817 (11.006817)  acc1: 12.500000 (12.500000)  acc5: 37.500000 (37.500000)  time: 0.311788  data: 0.271095  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:23  loss: 11.006817 (11.411270)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (25.000000)  time: 0.314115  data: 0.273722  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:21  loss: 11.137864 (11.755114)  acc1: 6.250000 (6.250000)  acc5: 25.000000 (25.000000)  time: 0.314514  data: 0.274151  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:20  loss: 12.243926 (12.283812)  acc1: 0.000000 (4.296875)  acc5: 18.750000 (22.656250)  time: 0.314371  data: 0.274005  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:18  loss: 12.243926 (12.141456)  acc1: 0.000000 (4.464286)  acc5: 18.750000 (23.214286)  time: 0.320954  data: 0.280578  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:17  loss: 12.243926 (12.138654)  acc1: 0.000000 (4.567308)  acc5: 18.750000 (23.557692)  time: 0.332854  data: 0.292445  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:16  loss: 12.282470 (12.147343)  acc1: 0.000000 (4.032258)  acc5: 25.000000 (24.798387)  time: 0.337315  data: 0.296883  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:14  loss: 12.161049 (12.224302)  acc1: 0.000000 (4.166667)  acc5: 25.000000 (24.131944)  time: 0.337769  data: 0.297348  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:12  loss: 12.146587 (12.111520)  acc1: 0.000000 (4.268293)  acc5: 25.000000 (24.085366)  time: 0.345369  data: 0.304869  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:11  loss: 11.736071 (11.998845)  acc1: 0.000000 (4.619565)  acc5: 25.000000 (24.456522)  time: 0.341295  data: 0.300820  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:09  loss: 11.736071 (11.975646)  acc1: 6.250000 (4.779412)  acc5: 25.000000 (25.490196)  time: 0.336239  data: 0.295772  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:08  loss: 11.204629 (11.994452)  acc1: 6.250000 (4.799107)  acc5: 25.000000 (24.776786)  time: 0.351464  data: 0.310976  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:06  loss: 11.736071 (11.987443)  acc1: 6.250000 (4.918033)  acc5: 25.000000 (24.692623)  time: 0.336245  data: 0.295879  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:04  loss: 12.443351 (12.022660)  acc1: 0.000000 (4.545455)  acc5: 25.000000 (24.242424)  time: 0.335786  data: 0.295425  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:03  loss: 12.285851 (12.016546)  acc1: 0.000000 (4.401408)  acc5: 18.750000 (24.295775)  time: 0.342678  data: 0.302320  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 12.285851 (11.992317)  acc1: 0.000000 (4.769737)  acc5: 18.750000 (24.671053)  time: 0.346496  data: 0.306152  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 12.139828 (11.964970)  acc1: 0.000000 (4.640000)  acc5: 18.750000 (24.720000)  time: 0.344171  data: 0.305228  max mem: 210\n",
      "Test: Total time: 0:00:26 (0.337125 s / it)\n",
      "* Acc@1 4.640 Acc@5 24.720 loss 11.965\n",
      "################################################## Validating Posthoc: cw and adv_classifier: pgd on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 1.307345 (1.307345)  acc1: 75.000000 (75.000000)  acc5: 87.500000 (87.500000)  time: 0.186048  data: 0.145842  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 1.143139 (1.133734)  acc1: 75.000000 (78.000000)  acc5: 87.500000 (86.000000)  time: 0.146997  data: 0.113706  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.147374 s / it)\n",
      "* Acc@1 78.000 Acc@5 86.000 loss 1.134\n",
      "################################################## Validating Posthoc: cw and adv_classifier: pgd on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 5.649935 (5.649935)  acc1: 50.000000 (50.000000)  acc5: 62.500000 (62.500000)  time: 0.185681  data: 0.145531  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 6.480145 (6.743094)  acc1: 31.250000 (38.000000)  acc5: 62.500000 (66.000000)  time: 0.146900  data: 0.113581  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.147223 s / it)\n",
      "* Acc@1 38.000 Acc@5 66.000 loss 6.743\n",
      "################################################## Validating Posthoc: cw and adv_classifier: pgd on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:04  loss: 2.073831 (2.073831)  acc1: 75.000000 (75.000000)  acc5: 75.000000 (75.000000)  time: 1.073149  data: 1.032895  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.953828 (1.435039)  acc1: 75.000000 (78.000000)  acc5: 75.000000 (80.000000)  time: 0.378200  data: 0.344919  max mem: 210\n",
      "Test: Total time: 0:00:01 (0.378526 s / it)\n",
      "* Acc@1 78.000 Acc@5 80.000 loss 1.435\n",
      "################################################## Validating Posthoc: cw and adv_classifier: pgd on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:24  loss: 11.795970 (11.795970)  acc1: 12.500000 (12.500000)  acc5: 25.000000 (25.000000)  time: 0.308286  data: 0.268024  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:26  loss: 11.018625 (11.406765)  acc1: 6.250000 (8.333333)  acc5: 18.750000 (20.833333)  time: 0.362544  data: 0.322253  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:27  loss: 11.580111 (11.400425)  acc1: 6.250000 (6.250000)  acc5: 18.750000 (21.022727)  time: 0.399003  data: 0.358692  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:26  loss: 11.580111 (11.514577)  acc1: 0.000000 (4.296875)  acc5: 18.750000 (21.484375)  time: 0.416612  data: 0.376307  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:23  loss: 11.416915 (11.518286)  acc1: 0.000000 (4.464286)  acc5: 18.750000 (21.726190)  time: 0.408505  data: 0.368201  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:22  loss: 11.416915 (11.503093)  acc1: 0.000000 (4.567308)  acc5: 18.750000 (20.913462)  time: 0.443936  data: 0.403623  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:20  loss: 11.222027 (11.483163)  acc1: 0.000000 (4.032258)  acc5: 18.750000 (21.169355)  time: 0.417633  data: 0.377315  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:17  loss: 11.172412 (11.535981)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (21.180556)  time: 0.383787  data: 0.343462  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:15  loss: 11.107396 (11.460660)  acc1: 0.000000 (4.268293)  acc5: 18.750000 (20.884146)  time: 0.386435  data: 0.346073  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:13  loss: 11.096105 (11.410004)  acc1: 0.000000 (4.619565)  acc5: 18.750000 (20.108696)  time: 0.345116  data: 0.304751  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:11  loss: 10.488884 (11.350741)  acc1: 6.250000 (4.779412)  acc5: 18.750000 (21.078431)  time: 0.339343  data: 0.298986  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:09  loss: 11.313253 (11.334938)  acc1: 6.250000 (4.799107)  acc5: 12.500000 (20.758929)  time: 0.354360  data: 0.313987  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:07  loss: 11.025122 (11.322228)  acc1: 6.250000 (4.918033)  acc5: 12.500000 (21.209016)  time: 0.344075  data: 0.303726  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:05  loss: 11.194990 (11.320700)  acc1: 0.000000 (4.545455)  acc5: 18.750000 (20.928030)  time: 0.346763  data: 0.306413  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:03  loss: 11.313253 (11.340247)  acc1: 0.000000 (4.401408)  acc5: 12.500000 (20.422535)  time: 0.365034  data: 0.324678  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 11.194990 (11.311812)  acc1: 0.000000 (4.769737)  acc5: 18.750000 (20.805921)  time: 0.357686  data: 0.317337  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 11.222515 (11.313910)  acc1: 0.000000 (4.640000)  acc5: 18.750000 (20.720000)  time: 0.350100  data: 0.311153  max mem: 210\n",
      "Test: Total time: 0:00:29 (0.371983 s / it)\n",
      "* Acc@1 4.640 Acc@5 20.720 loss 11.314\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: cw on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 5.488269 (5.488269)  acc1: 18.750000 (18.750000)  acc5: 56.250000 (56.250000)  time: 0.186534  data: 0.146108  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 5.259805 (4.969323)  acc1: 18.750000 (16.000000)  acc5: 56.250000 (58.000000)  time: 0.147838  data: 0.114561  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.148223 s / it)\n",
      "* Acc@1 16.000 Acc@5 58.000 loss 4.969\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: cw on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.298653 (0.298653)  acc1: 93.750000 (93.750000)  acc5: 93.750000 (93.750000)  time: 0.184226  data: 0.143873  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.074663)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (98.000000)  time: 0.147216  data: 0.113967  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.147603 s / it)\n",
      "* Acc@1 98.000 Acc@5 98.000 loss 0.075\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: cw on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 7.155119 (7.155119)  acc1: 6.250000 (6.250000)  acc5: 37.500000 (37.500000)  time: 0.191207  data: 0.150940  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 6.094499 (6.365180)  acc1: 0.000000 (4.000000)  acc5: 37.500000 (46.000000)  time: 0.150229  data: 0.116721  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.150618 s / it)\n",
      "* Acc@1 4.000 Acc@5 46.000 loss 6.365\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: cw on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:27  loss: 13.525923 (13.525923)  acc1: 12.500000 (12.500000)  acc5: 18.750000 (18.750000)  time: 0.345704  data: 0.305245  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:28  loss: 13.525923 (14.304344)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (26.041667)  time: 0.384810  data: 0.344460  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:29  loss: 14.369546 (14.516430)  acc1: 6.250000 (5.681818)  acc5: 25.000000 (23.295455)  time: 0.432829  data: 0.392446  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:26  loss: 14.498019 (15.058971)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (19.140625)  time: 0.413024  data: 0.372660  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:25  loss: 14.728824 (15.022107)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (19.940476)  time: 0.428533  data: 0.388172  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:22  loss: 14.875451 (14.944617)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (20.192308)  time: 0.434050  data: 0.393704  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:20  loss: 15.143551 (14.882747)  acc1: 0.000000 (3.830645)  acc5: 18.750000 (19.959677)  time: 0.394706  data: 0.354357  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:17  loss: 15.143551 (14.951828)  acc1: 0.000000 (3.993056)  acc5: 18.750000 (20.833333)  time: 0.380903  data: 0.340567  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:15  loss: 14.830462 (14.869930)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.884146)  time: 0.355268  data: 0.314951  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:13  loss: 14.137599 (14.812295)  acc1: 0.000000 (3.940217)  acc5: 25.000000 (21.331522)  time: 0.339276  data: 0.298921  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:10  loss: 14.249454 (14.806942)  acc1: 6.250000 (4.044118)  acc5: 25.000000 (21.323529)  time: 0.333881  data: 0.293553  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:09  loss: 13.856274 (14.717068)  acc1: 6.250000 (4.129464)  acc5: 18.750000 (21.428571)  time: 0.349810  data: 0.309450  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:07  loss: 13.856274 (14.738635)  acc1: 6.250000 (4.200820)  acc5: 18.750000 (21.004098)  time: 0.337313  data: 0.296958  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:05  loss: 14.457070 (14.790233)  acc1: 0.000000 (3.882576)  acc5: 18.750000 (20.928030)  time: 0.336253  data: 0.295912  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:03  loss: 14.620043 (14.808658)  acc1: 0.000000 (3.697183)  acc5: 18.750000 (21.302817)  time: 0.339340  data: 0.298993  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 14.662786 (14.750938)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (21.299342)  time: 0.326516  data: 0.286191  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 14.662786 (14.743645)  acc1: 0.000000 (4.000000)  acc5: 25.000000 (21.200000)  time: 0.313945  data: 0.275016  max mem: 210\n",
      "Test: Total time: 0:00:28 (0.360045 s / it)\n",
      "* Acc@1 4.000 Acc@5 21.200 loss 14.744\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: fgsm on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 5.488269 (5.488269)  acc1: 18.750000 (18.750000)  acc5: 56.250000 (56.250000)  time: 0.186707  data: 0.146438  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 5.259805 (4.969323)  acc1: 18.750000 (16.000000)  acc5: 56.250000 (58.000000)  time: 0.148268  data: 0.115083  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.148579 s / it)\n",
      "* Acc@1 16.000 Acc@5 58.000 loss 4.969\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: fgsm on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.298653 (0.298653)  acc1: 93.750000 (93.750000)  acc5: 93.750000 (93.750000)  time: 0.187712  data: 0.147385  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.074663)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (98.000000)  time: 0.147314  data: 0.114074  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.147628 s / it)\n",
      "* Acc@1 98.000 Acc@5 98.000 loss 0.075\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: fgsm on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 7.155119 (7.155119)  acc1: 6.250000 (6.250000)  acc5: 37.500000 (37.500000)  time: 0.187254  data: 0.146930  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 6.094499 (6.365180)  acc1: 0.000000 (4.000000)  acc5: 37.500000 (46.000000)  time: 0.148164  data: 0.114859  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.148488 s / it)\n",
      "* Acc@1 4.000 Acc@5 46.000 loss 6.365\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: fgsm on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:24  loss: 11.006817 (11.006817)  acc1: 12.500000 (12.500000)  acc5: 37.500000 (37.500000)  time: 0.307253  data: 0.267024  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:23  loss: 11.006817 (11.411270)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (25.000000)  time: 0.313272  data: 0.273003  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:21  loss: 11.917228 (11.825965)  acc1: 6.250000 (5.681818)  acc5: 25.000000 (24.431818)  time: 0.313814  data: 0.273477  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:19  loss: 12.243926 (12.332522)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (22.265625)  time: 0.312060  data: 0.271722  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:18  loss: 12.243926 (12.178569)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (22.916667)  time: 0.318922  data: 0.278553  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:17  loss: 12.243926 (12.168630)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (23.317308)  time: 0.329988  data: 0.289584  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:16  loss: 12.282470 (12.195342)  acc1: 0.000000 (3.830645)  acc5: 25.000000 (24.395161)  time: 0.335766  data: 0.295361  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:14  loss: 12.161049 (12.265634)  acc1: 0.000000 (3.993056)  acc5: 25.000000 (23.784722)  time: 0.338926  data: 0.298530  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:12  loss: 12.146587 (12.163893)  acc1: 0.000000 (3.963415)  acc5: 25.000000 (23.628049)  time: 0.345224  data: 0.304846  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:11  loss: 12.004870 (12.097364)  acc1: 0.000000 (3.940217)  acc5: 25.000000 (23.913043)  time: 0.341938  data: 0.301578  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:09  loss: 11.792064 (12.083798)  acc1: 6.250000 (4.044118)  acc5: 25.000000 (24.877451)  time: 0.336466  data: 0.296084  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:08  loss: 11.646162 (12.092948)  acc1: 6.250000 (4.129464)  acc5: 25.000000 (24.218750)  time: 0.350241  data: 0.309852  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:06  loss: 11.950278 (12.096414)  acc1: 6.250000 (4.200820)  acc5: 25.000000 (24.077869)  time: 0.337094  data: 0.296706  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:04  loss: 12.443351 (12.123376)  acc1: 0.000000 (3.882576)  acc5: 25.000000 (23.674242)  time: 0.336451  data: 0.296056  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:03  loss: 12.285851 (12.124089)  acc1: 0.000000 (3.697183)  acc5: 18.750000 (23.767606)  time: 0.339456  data: 0.299093  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 12.285851 (12.092785)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (24.177632)  time: 0.327055  data: 0.286693  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 12.237530 (12.061623)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (24.240000)  time: 0.314591  data: 0.275674  max mem: 210\n",
      "Test: Total time: 0:00:26 (0.329303 s / it)\n",
      "* Acc@1 4.000 Acc@5 24.240 loss 12.062\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: pgd on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 5.488269 (5.488269)  acc1: 18.750000 (18.750000)  acc5: 56.250000 (56.250000)  time: 0.191830  data: 0.151345  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 5.259805 (4.969323)  acc1: 18.750000 (16.000000)  acc5: 56.250000 (58.000000)  time: 0.149240  data: 0.115867  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.149649 s / it)\n",
      "* Acc@1 16.000 Acc@5 58.000 loss 4.969\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: pgd on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.298653 (0.298653)  acc1: 93.750000 (93.750000)  acc5: 93.750000 (93.750000)  time: 0.193476  data: 0.153124  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.074664)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (98.000000)  time: 0.148913  data: 0.115489  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.149242 s / it)\n",
      "* Acc@1 98.000 Acc@5 98.000 loss 0.075\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: pgd on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 7.155119 (7.155119)  acc1: 6.250000 (6.250000)  acc5: 37.500000 (37.500000)  time: 0.188547  data: 0.148005  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 6.094499 (6.365180)  acc1: 0.000000 (4.000000)  acc5: 37.500000 (46.000000)  time: 0.149817  data: 0.116219  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.150212 s / it)\n",
      "* Acc@1 4.000 Acc@5 46.000 loss 6.365\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: pgd on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:24  loss: 11.795970 (11.795970)  acc1: 12.500000 (12.500000)  acc5: 25.000000 (25.000000)  time: 0.311119  data: 0.270719  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:23  loss: 11.018625 (11.406765)  acc1: 6.250000 (8.333333)  acc5: 18.750000 (20.833333)  time: 0.312625  data: 0.272291  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:21  loss: 11.580111 (11.467379)  acc1: 6.250000 (5.681818)  acc5: 18.750000 (20.454545)  time: 0.313619  data: 0.273288  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:19  loss: 11.580111 (11.560608)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (21.093750)  time: 0.312035  data: 0.271713  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:19  loss: 11.416915 (11.553357)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (21.428571)  time: 0.329029  data: 0.288717  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:18  loss: 11.416915 (11.531420)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (20.673077)  time: 0.341365  data: 0.301044  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:16  loss: 11.416915 (11.530670)  acc1: 0.000000 (3.830645)  acc5: 18.750000 (20.766129)  time: 0.345998  data: 0.305614  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:14  loss: 11.416915 (11.576890)  acc1: 0.000000 (3.993056)  acc5: 18.750000 (20.833333)  time: 0.348666  data: 0.308274  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:13  loss: 11.172412 (11.517733)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.426829)  time: 0.344823  data: 0.304416  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:11  loss: 11.421421 (11.511051)  acc1: 0.000000 (3.940217)  acc5: 18.750000 (19.565217)  time: 0.360152  data: 0.319731  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:09  loss: 10.739026 (11.458579)  acc1: 6.250000 (4.044118)  acc5: 18.750000 (20.465686)  time: 0.354297  data: 0.313947  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:08  loss: 11.313253 (11.433147)  acc1: 6.250000 (4.129464)  acc5: 12.500000 (20.200893)  time: 0.381267  data: 0.340907  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:06  loss: 11.313253 (11.424241)  acc1: 6.250000 (4.200820)  acc5: 12.500000 (20.696721)  time: 0.376882  data: 0.336537  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:04  loss: 11.313253 (11.414984)  acc1: 0.000000 (3.882576)  acc5: 18.750000 (20.454545)  time: 0.366163  data: 0.325845  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:03  loss: 11.385376 (11.439243)  acc1: 0.000000 (3.697183)  acc5: 12.500000 (19.894366)  time: 0.369783  data: 0.329464  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 11.385376 (11.404295)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (20.312500)  time: 0.364930  data: 0.324615  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 11.222515 (11.402881)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (20.240000)  time: 0.353407  data: 0.314485  max mem: 210\n",
      "Test: Total time: 0:00:27 (0.349243 s / it)\n",
      "* Acc@1 4.000 Acc@5 20.240 loss 11.403\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: cw on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 3.319745 (3.319745)  acc1: 50.000000 (50.000000)  acc5: 68.750000 (68.750000)  time: 0.201985  data: 0.161675  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 3.319745 (3.568536)  acc1: 43.750000 (42.000000)  acc5: 68.750000 (68.000000)  time: 0.151556  data: 0.118446  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.151930 s / it)\n",
      "* Acc@1 42.000 Acc@5 68.000 loss 3.569\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: cw on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.298653 (0.298653)  acc1: 93.750000 (93.750000)  acc5: 93.750000 (93.750000)  time: 0.187501  data: 0.147221  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.074663)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (98.000000)  time: 0.147653  data: 0.114355  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.147970 s / it)\n",
      "* Acc@1 98.000 Acc@5 98.000 loss 0.075\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: cw on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 4.827378 (4.827378)  acc1: 43.750000 (43.750000)  acc5: 56.250000 (56.250000)  time: 0.188797  data: 0.148307  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 3.839459 (3.874488)  acc1: 43.750000 (42.000000)  acc5: 62.500000 (64.000000)  time: 0.149808  data: 0.116442  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.150123 s / it)\n",
      "* Acc@1 42.000 Acc@5 64.000 loss 3.874\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: cw on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:24  loss: 13.525923 (13.525923)  acc1: 12.500000 (12.500000)  acc5: 18.750000 (18.750000)  time: 0.314523  data: 0.273629  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:23  loss: 13.525923 (14.304344)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (26.041667)  time: 0.314499  data: 0.273974  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:21  loss: 14.369546 (14.516430)  acc1: 6.250000 (5.681818)  acc5: 25.000000 (23.295455)  time: 0.314415  data: 0.273983  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:24  loss: 14.498019 (15.058971)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (19.140625)  time: 0.386312  data: 0.345889  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:23  loss: 14.728824 (15.022107)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (19.940476)  time: 0.402375  data: 0.361988  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:21  loss: 14.875451 (14.944617)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (20.192308)  time: 0.413374  data: 0.373012  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:19  loss: 15.143551 (14.882747)  acc1: 0.000000 (3.830645)  acc5: 18.750000 (19.959677)  time: 0.429803  data: 0.389415  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:16  loss: 15.143551 (14.951828)  acc1: 0.000000 (3.993056)  acc5: 18.750000 (20.833333)  time: 0.373389  data: 0.333001  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:14  loss: 14.830462 (14.869930)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.884146)  time: 0.355115  data: 0.314727  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:12  loss: 14.137599 (14.812295)  acc1: 0.000000 (3.940217)  acc5: 25.000000 (21.331522)  time: 0.362163  data: 0.321780  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:10  loss: 14.249454 (14.806942)  acc1: 6.250000 (4.044118)  acc5: 25.000000 (21.323529)  time: 0.346350  data: 0.305984  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:08  loss: 13.856274 (14.717068)  acc1: 6.250000 (4.129464)  acc5: 18.750000 (21.428571)  time: 0.360886  data: 0.320536  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:07  loss: 13.856274 (14.738635)  acc1: 6.250000 (4.200820)  acc5: 18.750000 (21.004098)  time: 0.351752  data: 0.311408  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:05  loss: 14.457070 (14.790233)  acc1: 0.000000 (3.882576)  acc5: 18.750000 (20.928030)  time: 0.348054  data: 0.307681  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:03  loss: 14.620043 (14.808658)  acc1: 0.000000 (3.697183)  acc5: 18.750000 (21.302817)  time: 0.352221  data: 0.311841  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 14.662786 (14.750938)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (21.299342)  time: 0.341919  data: 0.301518  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 14.662786 (14.743645)  acc1: 0.000000 (4.000000)  acc5: 25.000000 (21.200000)  time: 0.327973  data: 0.288941  max mem: 210\n",
      "Test: Total time: 0:00:28 (0.359637 s / it)\n",
      "* Acc@1 4.000 Acc@5 21.200 loss 14.744\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: fgsm on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 3.319745 (3.319745)  acc1: 50.000000 (50.000000)  acc5: 68.750000 (68.750000)  time: 0.186796  data: 0.146633  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 3.319745 (3.568536)  acc1: 43.750000 (42.000000)  acc5: 68.750000 (68.000000)  time: 0.148535  data: 0.115025  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.148918 s / it)\n",
      "* Acc@1 42.000 Acc@5 68.000 loss 3.569\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: fgsm on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.298653 (0.298653)  acc1: 93.750000 (93.750000)  acc5: 93.750000 (93.750000)  time: 0.184684  data: 0.144389  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.074663)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (98.000000)  time: 0.162506  data: 0.129121  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.162937 s / it)\n",
      "* Acc@1 98.000 Acc@5 98.000 loss 0.075\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: fgsm on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 4.827378 (4.827378)  acc1: 43.750000 (43.750000)  acc5: 56.250000 (56.250000)  time: 0.186411  data: 0.145977  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 3.839459 (3.874488)  acc1: 43.750000 (42.000000)  acc5: 62.500000 (64.000000)  time: 0.147925  data: 0.114567  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.148288 s / it)\n",
      "* Acc@1 42.000 Acc@5 64.000 loss 3.874\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: fgsm on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:24  loss: 11.006817 (11.006817)  acc1: 12.500000 (12.500000)  acc5: 37.500000 (37.500000)  time: 0.307909  data: 0.267633  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:31  loss: 11.006817 (11.411270)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (25.000000)  time: 0.422542  data: 0.382165  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:25  loss: 11.917228 (11.825965)  acc1: 6.250000 (5.681818)  acc5: 25.000000 (24.431818)  time: 0.374328  data: 0.333947  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:22  loss: 12.243926 (12.332522)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (22.265625)  time: 0.354860  data: 0.314496  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:20  loss: 12.243926 (12.178569)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (22.916667)  time: 0.355541  data: 0.315189  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:19  loss: 12.243926 (12.168630)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (23.317308)  time: 0.334220  data: 0.293875  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:17  loss: 12.282470 (12.195342)  acc1: 0.000000 (3.830645)  acc5: 25.000000 (24.395161)  time: 0.350611  data: 0.310215  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:15  loss: 12.161049 (12.265634)  acc1: 0.000000 (3.993056)  acc5: 25.000000 (23.784722)  time: 0.351618  data: 0.311231  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:13  loss: 12.146587 (12.163893)  acc1: 0.000000 (3.963415)  acc5: 25.000000 (23.628049)  time: 0.352689  data: 0.312341  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:11  loss: 12.004870 (12.097364)  acc1: 0.000000 (3.940217)  acc5: 25.000000 (23.913043)  time: 0.345753  data: 0.305471  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:10  loss: 11.792064 (12.083798)  acc1: 6.250000 (4.044118)  acc5: 25.000000 (24.877451)  time: 0.325471  data: 0.285293  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:08  loss: 11.646162 (12.092948)  acc1: 6.250000 (4.129464)  acc5: 25.000000 (24.218750)  time: 0.337746  data: 0.297600  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:06  loss: 11.950278 (12.096414)  acc1: 6.250000 (4.200820)  acc5: 25.000000 (24.077869)  time: 0.323628  data: 0.283491  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:04  loss: 12.443351 (12.123376)  acc1: 0.000000 (3.882576)  acc5: 25.000000 (23.674242)  time: 0.324495  data: 0.284365  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:03  loss: 12.285851 (12.124089)  acc1: 0.000000 (3.697183)  acc5: 18.750000 (23.767606)  time: 0.329601  data: 0.289485  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 12.285851 (12.092785)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (24.177632)  time: 0.317230  data: 0.277126  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 12.237530 (12.061623)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (24.240000)  time: 0.305862  data: 0.267216  max mem: 210\n",
      "Test: Total time: 0:00:26 (0.335202 s / it)\n",
      "* Acc@1 4.000 Acc@5 24.240 loss 12.062\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: pgd on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 3.319745 (3.319745)  acc1: 50.000000 (50.000000)  acc5: 68.750000 (68.750000)  time: 0.178374  data: 0.138324  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 3.319745 (3.568536)  acc1: 43.750000 (42.000000)  acc5: 68.750000 (68.000000)  time: 0.140574  data: 0.107852  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.140910 s / it)\n",
      "* Acc@1 42.000 Acc@5 68.000 loss 3.569\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: pgd on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.298653 (0.298653)  acc1: 93.750000 (93.750000)  acc5: 93.750000 (93.750000)  time: 0.178241  data: 0.137773  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.074664)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (98.000000)  time: 0.140889  data: 0.107965  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.141182 s / it)\n",
      "* Acc@1 98.000 Acc@5 98.000 loss 0.075\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: pgd on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 4.827378 (4.827378)  acc1: 43.750000 (43.750000)  acc5: 56.250000 (56.250000)  time: 0.180035  data: 0.139980  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 3.839459 (3.874488)  acc1: 43.750000 (42.000000)  acc5: 62.500000 (64.000000)  time: 0.142237  data: 0.109280  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.142524 s / it)\n",
      "* Acc@1 42.000 Acc@5 64.000 loss 3.874\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: pgd on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:24  loss: 11.795970 (11.795970)  acc1: 12.500000 (12.500000)  acc5: 25.000000 (25.000000)  time: 0.306461  data: 0.266268  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:22  loss: 11.018625 (11.406765)  acc1: 6.250000 (8.333333)  acc5: 18.750000 (20.833333)  time: 0.306702  data: 0.266604  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:21  loss: 11.580111 (11.467379)  acc1: 6.250000 (5.681818)  acc5: 18.750000 (20.454545)  time: 0.306212  data: 0.266108  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:19  loss: 11.580111 (11.560608)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (21.093750)  time: 0.304743  data: 0.264611  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:18  loss: 11.416915 (11.553357)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (21.428571)  time: 0.313730  data: 0.273543  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:17  loss: 11.416915 (11.531420)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (20.673077)  time: 0.329052  data: 0.288783  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:15  loss: 11.416915 (11.530670)  acc1: 0.000000 (3.830645)  acc5: 18.750000 (20.766129)  time: 0.337501  data: 0.297140  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:14  loss: 11.416915 (11.576890)  acc1: 0.000000 (3.993056)  acc5: 18.750000 (20.833333)  time: 0.342224  data: 0.301800  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:12  loss: 11.172412 (11.517733)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.426829)  time: 0.349851  data: 0.309383  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:11  loss: 11.421421 (11.511051)  acc1: 0.000000 (3.940217)  acc5: 18.750000 (19.565217)  time: 0.346750  data: 0.306266  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:09  loss: 10.739026 (11.458579)  acc1: 6.250000 (4.044118)  acc5: 18.750000 (20.465686)  time: 0.344098  data: 0.303611  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:08  loss: 11.313253 (11.433147)  acc1: 6.250000 (4.129464)  acc5: 12.500000 (20.200893)  time: 0.374513  data: 0.334016  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:06  loss: 11.313253 (11.424241)  acc1: 6.250000 (4.200820)  acc5: 12.500000 (20.696721)  time: 0.371070  data: 0.330613  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:04  loss: 11.313253 (11.414984)  acc1: 0.000000 (3.882576)  acc5: 18.750000 (20.454545)  time: 0.368538  data: 0.328096  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:03  loss: 11.385376 (11.439243)  acc1: 0.000000 (3.697183)  acc5: 12.500000 (19.894366)  time: 0.367883  data: 0.327479  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 11.385376 (11.404295)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (20.312500)  time: 0.338974  data: 0.298598  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 11.222515 (11.402881)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (20.240000)  time: 0.318087  data: 0.279083  max mem: 210\n",
      "Test: Total time: 0:00:26 (0.337775 s / it)\n",
      "* Acc@1 4.000 Acc@5 20.240 loss 11.403\n"
     ]
    }
   ],
   "source": [
    "# Load posthoc\n",
    "posthocs=[\"cw\", \"fgsm_06\", \"pgd_06\"]\n",
    "\n",
    "adv_models = [\"cw\", \"fgsm\", \"pgd\"]\n",
    "\n",
    "# Perform validation on clean dataset\n",
    "for post_model in posthocs:\n",
    "    posthoc = LinearBC(1536)\n",
    "    posthoc.to(DEVICE)\n",
    "    posthoc.load_state_dict(torch.load(f'''/cluster/scratch/mmathys/dl_data/posthoc-models/{post_model}.pt'''))\n",
    "    \n",
    "    for adv_model in adv_models:\n",
    "        adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                          num_labels=len(CLASS_SUBSET))\n",
    "        adv_classifier.to(DEVICE)\n",
    "        adv_classifier.load_state_dict(torch.load(f'''/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/{version}/{adv_model}.pt'''))\n",
    "        \n",
    "        for attack, loaders in loader_dict.items():\n",
    "            pstr = \"#\"*30 + f''' Validating Posthoc: {post_model} and adv_classifier: {adv_model} on {attack} ''' + \"#\"*30\n",
    "            print(len(pstr)*\"#\")\n",
    "            print(pstr)\n",
    "            print(len(pstr)*\"#\")\n",
    "            log_dict, logger = validate_multihead_network(model, \n",
    "                                                          posthoc,\n",
    "                                                          adv_classifier,\n",
    "                                                          clean_classifier,\n",
    "                                                          loader_dict[attack][\"validation\"], \n",
    "                                                          tensor_dir=None, \n",
    "                                                          adversarial_attack=None, \n",
    "                                                          n=4, \n",
    "                                                          avgpool=False)\n",
    "            \n",
    "            # Save adversarial Classifier\n",
    "            save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version, \"benchmark\")\n",
    "            if not os.path.isdir(save_path):\n",
    "                os.makedirs(save_path)\n",
    "            save_file_log = f\"log_post{post_model}_adv{adv_model}_attack{attack}.pt\"\n",
    "            torch.save(logger, str(save_path) + \"/\" + save_file_log)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black Box on Multihead model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
