{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "# from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino, ViTWrapper\n",
    "from src.model.data import *\n",
    "from src.model.train import *\n",
    "from src.model.multihead_model import *\n",
    "from src.helpers.helpers import create_paths\n",
    "\n",
    "from torchattacks import *\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "MAX_PATH = Path('/','cluster', 'scratch', 'mmathys', 'dl_data')\n",
    "\n",
    "BASE_ADV_PATH = Path(MAX_PATH, 'adversarial_data_tensors')\n",
    "BASE_POSTHOC_PATH = Path(MAX_PATH, 'posthoc_tensors')\n",
    "POSTHOC_MODELS_PATH = Path(MAX_PATH, 'posthoc_models')\n",
    "\n",
    "ORI_PATH = Path(DATA_PATH, 'ori_data')\n",
    "CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "CLASS_SUBSET = np.load(CLASS_SUBSET_PATH)\n",
    "\n",
    "ADV_DATASETS = ['cw', 'fgsm_06', 'pgd_03']\n",
    "\n",
    "DATASETS = [*ADV_DATASETS, 'ori_data']\n",
    "\n",
    "\n",
    "# DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "# MAX_PATH = Path('/','cluster', 'scratch', 'mmathys', 'dl_data')\n",
    "\n",
    "# LOG_BASE_PATH = Path(MAX_PATH, 'logs')\n",
    "\n",
    "# # DamageNet\n",
    "# DN_PATH = Path(DATA_PATH, 'damageNet')\n",
    "# DN_LABEL_PATH = Path(DN_PATH, 'val_damagenet.txt')\n",
    "# DN_IMAGES_PATH = Path(DN_PATH, 'images')\n",
    "\n",
    "# # Image Net\n",
    "# ORI_PATH = Path(DATA_PATH, 'ori_data')\n",
    "# CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "\n",
    "# VAL_PATH = Path(ORI_PATH, 'validation')\n",
    "# VAL_IMAGES_PATH = Path(VAL_PATH,'images')\n",
    "# VAL_LABEL_PATH = Path(VAL_PATH, 'correct_labels.txt')\n",
    "\n",
    "# TRAIN_PATH = Path(ORI_PATH, 'train')\n",
    "# TRAIN_IMAGES_PATH = Path(TRAIN_PATH,'images')\n",
    "# TRAIN_LABEL_PATH = Path(TRAIN_PATH, 'correct_labels.txt')\n",
    "\n",
    "# # Adversarial Data\n",
    "# # PGD\n",
    "# ADV_DATA_PATH = Path(MAX_PATH, 'adversarial_data')\n",
    "# PGD_TRAIN_PATH = Path(ADV_DATA_PATH, 'pgd_06', 'train')\n",
    "# PGD_TRAIN_IMAGES_PATH = Path(PGD_TRAIN_PATH,'images')\n",
    "# PGD_TRAIN_LABEL_PATH = Path(PGD_TRAIN_PATH, 'labels.txt')\n",
    "\n",
    "# PGD_VAL_PATH = Path(MAX_PATH, 'adversarial_data', 'pgd_06', 'validation')\n",
    "# PGD_VAL_IMAGES_PATH = Path(PGD_VAL_PATH,'images')\n",
    "# PGD_VAL_LABEL_PATH = Path(PGD_VAL_PATH, 'labels.txt')\n",
    "\n",
    "# # CW\n",
    "# CW_TRAIN_PATH = Path(MAX_PATH, 'adversarial_data', 'cw', 'train')\n",
    "# CW_TRAIN_IMAGES_PATH = Path(CW_TRAIN_PATH,'images')\n",
    "# CW_TRAIN_LABEL_PATH = Path(CW_TRAIN_PATH, 'labels.txt')\n",
    "\n",
    "# CW_VAL_PATH = Path(MAX_PATH, 'adversarial_data', 'cw', 'validation')\n",
    "# CW_VAL_IMAGES_PATH = Path(CW_VAL_PATH,'images')\n",
    "# CW_VAL_LABEL_PATH = Path(CW_VAL_PATH, 'labels.txt')\n",
    "\n",
    "# # FGSM\n",
    "# FGSM_TRAIN_PATH = Path(MAX_PATH, 'adversarial_data', 'fgsm_06', 'train')\n",
    "# FGSM_TRAIN_IMAGES_PATH = Path(FGSM_TRAIN_PATH,'images')\n",
    "# FGSM_TRAIN_LABEL_PATH = Path(FGSM_TRAIN_PATH, 'labels.txt')\n",
    "\n",
    "# FGSM_VAL_PATH = Path(MAX_PATH, 'adversarial_data', 'fgsm_06', 'validation')\n",
    "# FGSM_VAL_IMAGES_PATH = Path(FGSM_VAL_PATH,'images')\n",
    "# FGSM_VAL_LABEL_PATH = Path(FGSM_VAL_PATH, 'labels.txt')\n",
    "\n",
    "\n",
    "# # TB LOG\n",
    "# TB_LOGS_BASE_PATH = Path(LOG_BASE_PATH, 'tb_logs')\n",
    "\n",
    "\n",
    "# # Model save path\n",
    "# ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH = Path(MAX_PATH, 'adversarial_data', 'adv_classifiers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_SUBSET = None\n",
    "NUM_WORKERS= 0\n",
    "PIN_MEMORY=True\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS= 3\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATHS = create_paths(data_name='ori',\n",
    "                 datasets_paths=None,  \n",
    "                 initial_base_path=DATA_PATH, \n",
    "                 posthoc_base_path=BASE_POSTHOC_PATH, \n",
    "                 train_str='train', \n",
    "                 val_str='validation')\n",
    "for adv_ds in ADV_DATASETS:\n",
    "    DATA_PATHS = create_paths(data_name=adv_ds,\n",
    "                 datasets_paths=DATA_PATHS,  \n",
    "                 initial_base_path=BASE_ADV_PATH, \n",
    "                 posthoc_base_path=BASE_POSTHOC_PATH, \n",
    "                 train_str='train', \n",
    "                 val_str='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/cluster/scratch/mmathys/dl_data/adversarial_data_tensors/cw/train/images')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DATA_PATHS[\"cw\"][\"init\"][\"train\"][\"images\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import DINO\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, linear_classifier = get_dino()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ori\n",
      "cw\n",
      "fgsm_06\n",
      "pgd_03\n"
     ]
    }
   ],
   "source": [
    "for k, v in DATA_PATHS.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to set the correct transformation\n",
    "# encoder\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit([i for i in CLASS_SUBSET])\n",
    "\n",
    "\n",
    "loader_dict = {\n",
    "}\n",
    "\n",
    " \"pgd\" : {\n",
    "        \"train\" : pgd_train_loader,\n",
    "        \"validation\" : pgd_val_loader,\n",
    "    },\n",
    "    \"cw\" : {\n",
    "        \"train\" : cw_train_loader,\n",
    "        \"validation\" : cw_val_loader,\n",
    "    }, \n",
    "    \"fgsm\" : {\n",
    "        \"train\" : fgsm_train_loader,\n",
    "        \"validation\" : fgsm_val_loader,\n",
    "    },\n",
    "    \"clean\" : {\n",
    "        \"train\" : clean_train_loader,\n",
    "        \"validation\" : clean_val_loader,\n",
    "    },\n",
    "\n",
    "for k, v in DATA_PATHS.items():\n",
    "    if not k == \"ori\":\n",
    "        adv_train_dataset = AdvTrainingImageDataset(\n",
    "                                                PGD_TRAIN_IMAGES_PATH, \n",
    "                                                PGD_TRAIN_LABEL_PATH, \n",
    "                                                ORIGINAL_TRANSFORM, \n",
    "                                                CLASS_SUBSET, \n",
    "                                                index_subset=None,\n",
    "                                                label_encoder=label_encoder)\n",
    "        \n",
    "        adv_val_dataset = AdvTrainingImageDataset(\n",
    "                                                PGD_TRAIN_IMAGES_PATH, \n",
    "                                                PGD_TRAIN_LABEL_PATH, \n",
    "                                                ORIGINAL_TRANSFORM, \n",
    "                                                CLASS_SUBSET, \n",
    "                                                index_subset=None,\n",
    "                                                label_encoder=label_encoder)\n",
    "\n",
    "        loader_dict[k][\"train\"] = DataLoader(pgd_train_dataset, \n",
    "                                             batch_size=BATCH_SIZE, \n",
    "                                             num_workers=NUM_WORKERS, \n",
    "                                             pin_memory=PIN_MEMORY, \n",
    "                                             shuffle=False)\n",
    "        \n",
    "        loader_dict[k][\"validation\"] = DataLoader(pgd_train_dataset, \n",
    "                                             batch_size=BATCH_SIZE, \n",
    "                                             num_workers=NUM_WORKERS, \n",
    "                                             pin_memory=PIN_MEMORY, \n",
    "                                             shuffle=False)\n",
    "    else:\n",
    "        loader_dict[\"\"][\"validation\"]\n",
    "        clean_train_loader = DataLoader(clean_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "        clean_val_loader = DataLoader(clean_val_dataset, \n",
    "                                      batch_size=BATCH_SIZE, \n",
    "                                      num_workers=NUM_WORKERS, \n",
    "                                      pin_memory=PIN_MEMORY,\n",
    "                                      shuffle=False)\n",
    "    \n",
    "\n",
    "\n",
    "# PGD\n",
    "pgd_train_dataset = AdvTrainingImageDataset(PGD_TRAIN_IMAGES_PATH, \n",
    "                                            PGD_TRAIN_LABEL_PATH, \n",
    "                                            ORIGINAL_TRANSFORM, \n",
    "                                            CLASS_SUBSET, \n",
    "                                            index_subset=None,\n",
    "                                            label_encoder=label_encoder\n",
    "                                           )\n",
    "\n",
    "pgd_val_dataset = AdvTrainingImageDataset(PGD_VAL_IMAGES_PATH, \n",
    "                                          PGD_VAL_LABEL_PATH, \n",
    "                                          ORIGINAL_TRANSFORM, \n",
    "                                          CLASS_SUBSET, \n",
    "                                          index_subset=None,\n",
    "                                          label_encoder=label_encoder\n",
    "                                         )\n",
    "\n",
    "\n",
    "pgd_train_loader = DataLoader(pgd_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "pgd_val_loader = DataLoader(pgd_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY, \n",
    "                            shuffle=False)\n",
    "\n",
    "# CW\n",
    "cw_train_dataset = AdvTrainingImageDataset(CW_TRAIN_IMAGES_PATH, \n",
    "                                            CW_TRAIN_LABEL_PATH, \n",
    "                                            ORIGINAL_TRANSFORM, \n",
    "                                            CLASS_SUBSET, \n",
    "                                            index_subset=None,\n",
    "                                           label_encoder=label_encoder\n",
    "                                          )\n",
    "\n",
    "cw_val_dataset = AdvTrainingImageDataset(CW_VAL_IMAGES_PATH, \n",
    "                                          CW_VAL_LABEL_PATH, \n",
    "                                          ORIGINAL_TRANSFORM, \n",
    "                                          CLASS_SUBSET, \n",
    "                                          index_subset=None,\n",
    "                                         label_encoder=label_encoder\n",
    "                                        )\n",
    "\n",
    "cw_train_loader = DataLoader(cw_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "cw_val_loader = DataLoader(cw_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY, \n",
    "                            shuffle=False)\n",
    "\n",
    "# FGSM\n",
    "fgsm_train_dataset = AdvTrainingImageDataset(FGSM_TRAIN_IMAGES_PATH, \n",
    "                                            FGSM_TRAIN_LABEL_PATH, \n",
    "                                            ORIGINAL_TRANSFORM, \n",
    "                                            CLASS_SUBSET, \n",
    "                                            index_subset=None,\n",
    "                                            label_encoder=label_encoder\n",
    "                                            )\n",
    "\n",
    "fgsm_val_dataset = AdvTrainingImageDataset(FGSM_VAL_IMAGES_PATH, \n",
    "                                          FGSM_VAL_LABEL_PATH, \n",
    "                                          ORIGINAL_TRANSFORM, \n",
    "                                          CLASS_SUBSET, \n",
    "                                          index_subset=None,\n",
    "                                          label_encoder=label_encoder\n",
    "                                          )\n",
    "\n",
    "fgsm_train_loader = DataLoader(fgsm_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "fgsm_val_loader = DataLoader(fgsm_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY, \n",
    "                            shuffle=False)\n",
    "\n",
    "# Clean\n",
    "clean_train_dataset = ImageDataset(TRAIN_IMAGES_PATH, \n",
    "                                  TRAIN_LABEL_PATH, \n",
    "                                  ORIGINAL_TRANSFORM,\n",
    "                                  CLASS_SUBSET, \n",
    "                                  index_subset=None, \n",
    "                                  label_encoder=label_encoder)\n",
    "\n",
    "clean_val_dataset = ImageDataset(VAL_IMAGES_PATH, \n",
    "                                  VAL_LABEL_PATH, \n",
    "                                  ORIGINAL_TRANSFORM,\n",
    "                                  CLASS_SUBSET, \n",
    "                                  index_subset=None, \n",
    "                                  label_encoder=label_encoder)\n",
    "\n",
    "clean_train_loader = DataLoader(clean_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "clean_val_loader = DataLoader(clean_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY,\n",
    "                            shuffle=False)\n",
    "\n",
    "\n",
    "loader_dict = {\n",
    "    \"pgd\" : {\n",
    "        \"train\" : pgd_train_loader,\n",
    "        \"validation\" : pgd_val_loader,\n",
    "    },\n",
    "    \"cw\" : {\n",
    "        \"train\" : cw_train_loader,\n",
    "        \"validation\" : cw_val_loader,\n",
    "    }, \n",
    "    \"fgsm\" : {\n",
    "        \"train\" : fgsm_train_loader,\n",
    "        \"validation\" : fgsm_val_loader,\n",
    "    },\n",
    "    \"clean\" : {\n",
    "        \"train\" : clean_train_loader,\n",
    "        \"validation\" : clean_val_loader,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_dict[\"pgd\"][\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '25_classes_full_v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(dim, num_labels)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # linear layer\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Binary Classifier\n",
    "class LinearBC(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(LinearBC,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train various classifiers on all adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attack, loaders in loader_dict.items():\n",
    "    \n",
    "    # Initialise classifier\n",
    "    adv_linear_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                         num_labels=len(CLASS_SUBSET))\n",
    "    adv_linear_classifier = adv_linear_classifier.cuda()\n",
    "\n",
    "    # Metric logger path\n",
    "    LOG_PATH = Path(LOG_BASE_PATH, 'adv_classifier', version, attack)\n",
    "    if not os.path.isdir(LOG_PATH):\n",
    "        os.makedirs(LOG_PATH)\n",
    "    \n",
    "    # train\n",
    "    pstr = \"#\"*50 + f''' Training classifier for {attack} ''' + \"#\"*50\n",
    "    print(len(pstr)*\"#\")\n",
    "    print(pstr)\n",
    "    print(len(pstr)*\"#\")\n",
    "    loggers = train(model, \n",
    "                    adv_linear_classifier, \n",
    "                    loaders[\"train\"], \n",
    "                    loaders[\"validation\"], \n",
    "                    log_dir=LOG_PATH, \n",
    "                    tensor_dir=None, \n",
    "                    optimizer=None, \n",
    "                    adversarial_attack=None,\n",
    "                    criterion=nn.CrossEntropyLoss(),\n",
    "                    epochs=5, \n",
    "                    val_freq=1, \n",
    "                    batch_size=16,  \n",
    "                    lr=0.001, \n",
    "                    to_restore = {\"epoch\": 0, \"best_acc\": 0.}, \n",
    "                    n=4, \n",
    "                    avgpool_patchtokens=False, \n",
    "                    show_image=False)\n",
    "    \n",
    "    # Save adversarial Classifier\n",
    "    save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version)\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    save_file_model = f\"{attack}.pt\"\n",
    "    save_file_log = f\"log_{attack}.pt\"\n",
    "    torch.save(adv_linear_classifier.state_dict(), str(save_path) + \"/\" + save_file_model)\n",
    "    torch.save(loggers, str(save_path) + \"/\" + save_file_log)\n",
    "    print(f'Finished Training, saving model to {str(save_path)}/{save_file_model} and log to {str(save_path)}/{save_file_model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on all adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = [x for x in loader_dict.keys()]\n",
    "\n",
    "for attack in attacks:\n",
    "    pstr = \"#\"*30 + f''' evaluating adv_classifier trained on {attack} ''' + \"#\"*30\n",
    "    print(len(pstr)*\"#\")\n",
    "    print(pstr)\n",
    "    print(len(pstr)*\"#\")\n",
    "    adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                             num_labels=len(CLASS_SUBSET))\n",
    "    adv_classifier.to(DEVICE)\n",
    "    \n",
    "    save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version)\n",
    "    save_file = f\"{attack}.pt\"\n",
    "    adv_classifier.load_state_dict(torch.load(str(save_path) + \"/\" + save_file))\n",
    "    \n",
    "    for applied_attack in attacks:\n",
    "        \n",
    "#         if applied_attack == attack:\n",
    "#             continue\n",
    "        \n",
    "        print(\"-\"*50 + f\" {applied_attack} dataset \" + \"-\"*50)\n",
    "        logger_dict, logger = validate_network(model, \n",
    "                                               adv_classifier, \n",
    "                                               loader_dict[applied_attack][\"validation\"], \n",
    "                                               criterion=nn.CrossEntropyLoss(),\n",
    "                                               tensor_dir=None, \n",
    "                                               adversarial_attack=None, \n",
    "                                               n=4, \n",
    "                                               avgpool_patchtokens=False, \n",
    "                                               path_predictions=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on newly generated attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = [x for x in loader_dict.keys()]\n",
    "for attack in attacks:\n",
    "    \n",
    "    pstr = \"#\"*30 + f''' evaluating adv_classifier trained on {attack} ''' + \"#\"*30\n",
    "    print(len(pstr)*\"#\")\n",
    "    print(pstr)\n",
    "    print(len(pstr)*\"#\")\n",
    "    \n",
    "    adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                             num_labels=len(CLASS_SUBSET))\n",
    "    adv_classifier.to(DEVICE)\n",
    "    \n",
    "    save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version)\n",
    "    save_file = f\"{attack}.pt\"\n",
    "    adv_classifier.load_state_dict(torch.load(str(save_path) + \"/\" + save_file))\n",
    "    \n",
    "    vits = ViTWrapper(model, adv_classifier, transform=None)\n",
    "\n",
    "    for applied_attack in attacks:\n",
    "        \n",
    "        if applied_attack == \"pgd\":\n",
    "            ev_attack = PGD(vits, eps=0.3, alpha=6/255, steps=15)\n",
    "        elif applied_attack == \"cw\":\n",
    "            ev_attack = CW(vits, c=10, lr=0.003, steps=30)\n",
    "        elif applied_attack == \"fgsm\":\n",
    "            ev_attack = FGSM(vits, eps=0.03)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        model.train()\n",
    "        print(\">\"*5 + f''' applying attack: {ev_attack} ''')\n",
    "        logger_dict, logger = validate_network(model, \n",
    "                                               adv_classifier, \n",
    "                                               loader_dict[\"clean\"][\"validation\"],\n",
    "                                               criterion=nn.CrossEntropyLoss(),\n",
    "                                               tensor_dir=None, \n",
    "                                               adversarial_attack=ev_attack,\n",
    "                                               n=4, \n",
    "                                               avgpool_patchtokens=False, \n",
    "                                               path_predictions=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on full pipeline with post-hoc as multiplexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean_classifier\n",
    "name=\"clean\"\n",
    "clean_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                    num_labels=len(CLASS_SUBSET))\n",
    "clean_classifier.to(DEVICE)\n",
    "clean_classifier.load_state_dict(torch.load(f\"/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/{version}/{name}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load posthoc\n",
    "posthocs=[\"cw\", \"fgsm_06\", \"pgd_06\"]\n",
    "\n",
    "adv_models = [\"cw\", \"fgsm\", \"pgd\"]\n",
    "\n",
    "# Perform validation on clean dataset\n",
    "for post_model in posthocs:\n",
    "    posthoc = LinearBC(1536)\n",
    "    posthoc.to(DEVICE)\n",
    "    posthoc.load_state_dict(torch.load(f'''/cluster/scratch/mmathys/dl_data/posthoc-models/{post_model}.pt'''))\n",
    "    \n",
    "    for adv_model in adv_models:\n",
    "        adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                          num_labels=len(CLASS_SUBSET))\n",
    "        adv_classifier.to(DEVICE)\n",
    "        adv_classifier.load_state_dict(torch.load(f'''/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/{version}/{adv_model}.pt'''))\n",
    "        \n",
    "        for attack, loaders in loader_dict.items():\n",
    "            pstr = \"#\"*30 + f''' Validating Posthoc: {post_model} and adv_classifier: {adv_model} on {attack} ''' + \"#\"*30\n",
    "            print(len(pstr)*\"#\")\n",
    "            print(pstr)\n",
    "            print(len(pstr)*\"#\")\n",
    "            log_dict, logger = validate_multihead_network(model, \n",
    "                                                          posthoc,\n",
    "                                                          adv_classifier,\n",
    "                                                          clean_classifier,\n",
    "                                                          loader_dict[attack][\"validation\"], \n",
    "                                                          tensor_dir=None, \n",
    "                                                          adversarial_attack=None, \n",
    "                                                          n=4, \n",
    "                                                          avgpool=False)\n",
    "            \n",
    "            # Save adversarial Classifier\n",
    "            save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version, \"benchmark\")\n",
    "            if not os.path.isdir(save_path):\n",
    "                os.makedirs(save_path)\n",
    "            save_file_log = f\"log_post{post_model}_adv{adv_model}_attack{attack}.pt\"\n",
    "            torch.save(logger, str(save_path) + \"/\" + save_file_log)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black Box on Multihead model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
