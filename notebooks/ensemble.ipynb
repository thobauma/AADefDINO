{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "# from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino, ViTWrapper\n",
    "from src.model.data import *\n",
    "from src.model.train import *\n",
    "from src.model.multihead_model import *\n",
    "from src.helpers.helpers import create_paths\n",
    "\n",
    "from torchattacks import *\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "username = getpass.getuser()\n",
    "DATA_PATH = Path('/cluster/scratch/thobauma/data/')\n",
    "\n",
    "ORI_PATH = Path(DATA_PATH, 'ori')\n",
    "ORI_PATH_VALIDATION = Path(ORI_PATH, 'validation')\n",
    "ORI_PATH_VALIDATION_LABELS = Path(ORI_PATH,'labels.csv')\n",
    "ORI_PATH_VALIDATION_IMAGES = Path(ORI_PATH,'images')\n",
    "\n",
    "CLEAN_FILTERED_PATH = ORI_PATH = Path(DATA_PATH, 'ori', 'filtered')\n",
    "\n",
    "MODELS_PATH = Path(DATA_PATH, 'models')\n",
    "\n",
    "BASE_ADV_PATH = Path(DATA_PATH, 'adv') # in tensors\n",
    "\n",
    "# BASE_POSTHOC_PATH = TODO\n",
    "# POSTHOC_MODELS_PATH = TODO\n",
    "\n",
    "ADV_DATASETS = ['pgd_0001', 'pgd_003', 'pgd_01']\n",
    "\n",
    "DATASETS = [*ADV_DATASETS, 'ori']\n",
    "\n",
    "#### NOT SURE WHAT THIS IS FOR\n",
    "# LINEAR_CLASSIFIER_EVAL_PATH = Path(MAX_PATH, 'linear_classifier_evaluation')\n",
    "# LINEAR_CLASSIFIER_EVAL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# MULTIHEAD_EVAL_PATH = Path(MAX_PATH, 'multihead_eval')\n",
    "# MULTIHEAD_EVAL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "#### NOT SURE WHAT THIS IS FOR\n",
    "\n",
    "# LINEAR_CLASSIFIER_MODELS_PATH = Path(MAX_PATH, 'linear_classifier_models') # what was stored here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_SUBSET = None\n",
    "NUM_WORKERS= 0\n",
    "PIN_MEMORY=True\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 2\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader_dict = defaultdict(dict)\n",
    "\n",
    "for attack in ADV_DATASETS:\n",
    "    image_path_train = Path(BASE_ADV_PATH, attack, \"train\", \"images\")\n",
    "    label_file_train = str(Path(BASE_ADV_PATH, attack, \"train\")) + \"/labels.csv\"\n",
    "    \n",
    "    image_path_validation = Path(BASE_ADV_PATH, attack, \"validation\", \"images\")\n",
    "    label_file_validation = str(Path(BASE_ADV_PATH, attack, \"validation\")) + \"/labels.csv\"\n",
    "    \n",
    "    adv_train_dataset = TensorImageDataset(image_path_train, \n",
    "                                           label_file_train, None)\n",
    "        \n",
    "    adv_val_dataset = TensorImageDataset(image_path_validation, \n",
    "                                         label_file_validation, None)\n",
    "\n",
    "\n",
    "    loader_dict[attack][\"train\"] = DataLoader(adv_train_dataset, \n",
    "                                         batch_size=BATCH_SIZE, \n",
    "                                         num_workers=NUM_WORKERS, \n",
    "                                         pin_memory=PIN_MEMORY, \n",
    "                                         shuffle=True)\n",
    "\n",
    "    loader_dict[attack][\"validation\"] = DataLoader(adv_val_dataset, \n",
    "                                                     batch_size=BATCH_SIZE, \n",
    "                                                     num_workers=NUM_WORKERS, \n",
    "                                                     pin_memory=PIN_MEMORY, \n",
    "                                                     shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'pgd_0001': {'train': <torch.utils.data.dataloader.DataLoader at 0x2b615ee78580>,\n",
       "              'validation': <torch.utils.data.dataloader.DataLoader at 0x2b615ee78a30>},\n",
       "             'pgd_003': {'train': <torch.utils.data.dataloader.DataLoader at 0x2b615ee788b0>,\n",
       "              'validation': <torch.utils.data.dataloader.DataLoader at 0x2b615ee787c0>},\n",
       "             'pgd_01': {'train': <torch.utils.data.dataloader.DataLoader at 0x2b615ee78880>,\n",
       "              'validation': <torch.utils.data.dataloader.DataLoader at 0x2b62315c83d0>}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_train = Path(CLEAN_FILTERED_PATH, \"train\", \"images\")\n",
    "label_file_train = str(Path(CLEAN_FILTERED_PATH, \"train\")) + \"/labels.csv\"\n",
    "\n",
    "image_path_validation = Path(CLEAN_FILTERED_PATH, \"validation\", \"images\")\n",
    "label_file_validation = str(Path(CLEAN_FILTERED_PATH, \"validation\")) + \"/labels.csv\"\n",
    "\n",
    "clean_train_dataset = AdvTrainingImageDataset(image_path_train, \n",
    "                                              label_file_train, \n",
    "                                              ORIGINAL_TRANSFORM)\n",
    "\n",
    "clean_val_dataset = AdvTrainingImageDataset(image_path_validation, \n",
    "                                            label_file_validation, \n",
    "                                            ORIGINAL_TRANSFORM)\n",
    "\n",
    "clean_train_loader = DataLoader(clean_train_dataset, \n",
    "                                 batch_size=BATCH_SIZE, \n",
    "                                 num_workers=NUM_WORKERS, \n",
    "                                 pin_memory=PIN_MEMORY, \n",
    "                                 shuffle=True)\n",
    "\n",
    "clean_val_loader = DataLoader(clean_val_dataset, \n",
    "                                 batch_size=BATCH_SIZE, \n",
    "                                 num_workers=NUM_WORKERS, \n",
    "                                 pin_memory=PIN_MEMORY, \n",
    "                                 shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Models (Post-Hoc and Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(dim, num_labels)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # linear layer\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Binary Classifier\n",
    "class LinearBC(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        self.num_labels = 2\n",
    "        super(LinearBC,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import DINO\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights.\n"
     ]
    }
   ],
   "source": [
    "model, linear_classifier = get_dino()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train various classifiers on all adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### Training classifier for pgd_003 #########################\n",
      "Epoch: [0]  [   0/1463]  eta: 0:19:05  lr: 0.000250  loss: 2.853913 (2.853913)  time: 0.782746  data: 0.643632  max mem: 572\n",
      "Epoch: [0]  [  20/1463]  eta: 0:16:54  lr: 0.000250  loss: 0.995577 (1.362123)  time: 0.699010  data: 0.583349  max mem: 572\n",
      "Epoch: [0]  [  40/1463]  eta: 0:16:56  lr: 0.000250  loss: 0.252890 (0.826238)  time: 0.725927  data: 0.608607  max mem: 572\n",
      "Epoch: [0]  [  60/1463]  eta: 0:16:30  lr: 0.000250  loss: 0.134853 (0.597236)  time: 0.688626  data: 0.570962  max mem: 572\n",
      "Epoch: [0]  [  80/1463]  eta: 0:16:16  lr: 0.000250  loss: 0.092682 (0.473572)  time: 0.706992  data: 0.590781  max mem: 572\n",
      "Epoch: [0]  [ 100/1463]  eta: 0:16:04  lr: 0.000250  loss: 0.067965 (0.394167)  time: 0.712202  data: 0.594474  max mem: 572\n",
      "Epoch: [0]  [ 120/1463]  eta: 0:15:44  lr: 0.000250  loss: 0.052034 (0.338017)  time: 0.683044  data: 0.566564  max mem: 572\n",
      "Epoch: [0]  [ 140/1463]  eta: 0:15:24  lr: 0.000250  loss: 0.048233 (0.297840)  time: 0.671783  data: 0.555276  max mem: 572\n",
      "Epoch: [0]  [ 160/1463]  eta: 0:15:04  lr: 0.000250  loss: 0.038104 (0.266462)  time: 0.662308  data: 0.544403  max mem: 572\n",
      "Epoch: [0]  [ 180/1463]  eta: 0:14:50  lr: 0.000250  loss: 0.046233 (0.242774)  time: 0.694882  data: 0.578334  max mem: 572\n",
      "Epoch: [0]  [ 200/1463]  eta: 0:14:38  lr: 0.000250  loss: 0.044277 (0.223907)  time: 0.705870  data: 0.589244  max mem: 572\n",
      "Epoch: [0]  [ 220/1463]  eta: 0:14:23  lr: 0.000250  loss: 0.035111 (0.207329)  time: 0.684860  data: 0.568144  max mem: 572\n",
      "Epoch: [0]  [ 240/1463]  eta: 0:14:06  lr: 0.000250  loss: 0.032302 (0.193290)  time: 0.668790  data: 0.551021  max mem: 572\n",
      "Epoch: [0]  [ 260/1463]  eta: 0:13:52  lr: 0.000250  loss: 0.036039 (0.181399)  time: 0.688810  data: 0.572106  max mem: 572\n",
      "Epoch: [0]  [ 280/1463]  eta: 0:13:38  lr: 0.000250  loss: 0.022068 (0.170865)  time: 0.691146  data: 0.574563  max mem: 572\n",
      "Epoch: [0]  [ 300/1463]  eta: 0:13:24  lr: 0.000250  loss: 0.022312 (0.161776)  time: 0.682248  data: 0.564450  max mem: 572\n",
      "Epoch: [0]  [ 320/1463]  eta: 0:13:10  lr: 0.000250  loss: 0.034811 (0.154040)  time: 0.691576  data: 0.574911  max mem: 572\n",
      "Epoch: [0]  [ 340/1463]  eta: 0:12:56  lr: 0.000250  loss: 0.027035 (0.147115)  time: 0.689069  data: 0.572382  max mem: 572\n",
      "Epoch: [0]  [ 360/1463]  eta: 0:12:41  lr: 0.000250  loss: 0.019984 (0.140406)  time: 0.682570  data: 0.565792  max mem: 572\n",
      "Epoch: [0]  [ 380/1463]  eta: 0:12:28  lr: 0.000250  loss: 0.016552 (0.134139)  time: 0.703118  data: 0.585203  max mem: 572\n",
      "Epoch: [0]  [ 400/1463]  eta: 0:12:15  lr: 0.000250  loss: 0.021630 (0.128816)  time: 0.704354  data: 0.587770  max mem: 572\n",
      "Epoch: [0]  [ 420/1463]  eta: 0:12:24  lr: 0.000250  loss: 0.022108 (0.123917)  time: 1.146425  data: 1.024003  max mem: 572\n",
      "Epoch: [0]  [ 440/1463]  eta: 0:12:09  lr: 0.000250  loss: 0.017434 (0.119208)  time: 0.710268  data: 0.593533  max mem: 572\n",
      "Epoch: [0]  [ 460/1463]  eta: 0:11:55  lr: 0.000250  loss: 0.018476 (0.114983)  time: 0.699308  data: 0.580742  max mem: 572\n",
      "Epoch: [0]  [ 480/1463]  eta: 0:11:40  lr: 0.000250  loss: 0.016506 (0.111400)  time: 0.705842  data: 0.589306  max mem: 572\n",
      "Epoch: [0]  [ 500/1463]  eta: 0:11:25  lr: 0.000250  loss: 0.019466 (0.107956)  time: 0.686402  data: 0.569715  max mem: 572\n",
      "Epoch: [0]  [ 520/1463]  eta: 0:11:10  lr: 0.000250  loss: 0.019120 (0.104696)  time: 0.687594  data: 0.570982  max mem: 572\n",
      "Epoch: [0]  [ 540/1463]  eta: 0:10:56  lr: 0.000250  loss: 0.016510 (0.101764)  time: 0.717872  data: 0.600316  max mem: 572\n",
      "Epoch: [0]  [ 560/1463]  eta: 0:10:42  lr: 0.000250  loss: 0.015675 (0.099048)  time: 0.743280  data: 0.626630  max mem: 572\n",
      "Epoch: [0]  [ 580/1463]  eta: 0:10:28  lr: 0.000250  loss: 0.012647 (0.096273)  time: 0.699608  data: 0.583055  max mem: 572\n",
      "Epoch: [0]  [ 600/1463]  eta: 0:10:13  lr: 0.000250  loss: 0.013536 (0.093587)  time: 0.674854  data: 0.558017  max mem: 572\n",
      "Epoch: [0]  [ 620/1463]  eta: 0:09:58  lr: 0.000250  loss: 0.013854 (0.091181)  time: 0.680424  data: 0.563628  max mem: 572\n",
      "Epoch: [0]  [ 640/1463]  eta: 0:09:44  lr: 0.000250  loss: 0.011414 (0.088847)  time: 0.720357  data: 0.603615  max mem: 572\n",
      "Epoch: [0]  [ 660/1463]  eta: 0:09:32  lr: 0.000250  loss: 0.020614 (0.086832)  time: 0.801498  data: 0.683696  max mem: 572\n",
      "Epoch: [0]  [ 680/1463]  eta: 0:09:17  lr: 0.000250  loss: 0.025833 (0.085036)  time: 0.687696  data: 0.571113  max mem: 572\n",
      "Epoch: [0]  [ 700/1463]  eta: 0:09:02  lr: 0.000250  loss: 0.014350 (0.083122)  time: 0.679266  data: 0.562630  max mem: 572\n",
      "Epoch: [0]  [ 720/1463]  eta: 0:08:48  lr: 0.000250  loss: 0.011532 (0.081277)  time: 0.728408  data: 0.610538  max mem: 572\n",
      "Epoch: [0]  [ 740/1463]  eta: 0:08:35  lr: 0.000250  loss: 0.017740 (0.079669)  time: 0.751468  data: 0.633551  max mem: 572\n",
      "Epoch: [0]  [ 760/1463]  eta: 0:08:20  lr: 0.000250  loss: 0.011149 (0.078072)  time: 0.699355  data: 0.582163  max mem: 572\n",
      "Epoch: [0]  [ 780/1463]  eta: 0:08:05  lr: 0.000250  loss: 0.013476 (0.076482)  time: 0.678649  data: 0.561924  max mem: 572\n",
      "Epoch: [0]  [ 800/1463]  eta: 0:07:51  lr: 0.000250  loss: 0.016594 (0.075096)  time: 0.706828  data: 0.587919  max mem: 572\n",
      "Epoch: [0]  [ 820/1463]  eta: 0:07:37  lr: 0.000250  loss: 0.012037 (0.073606)  time: 0.750226  data: 0.630212  max mem: 572\n",
      "Epoch: [0]  [ 840/1463]  eta: 0:07:24  lr: 0.000250  loss: 0.008863 (0.072177)  time: 0.738777  data: 0.622057  max mem: 572\n",
      "Epoch: [0]  [ 860/1463]  eta: 0:07:10  lr: 0.000250  loss: 0.010747 (0.070782)  time: 0.730565  data: 0.614034  max mem: 572\n",
      "Epoch: [0]  [ 880/1463]  eta: 0:06:56  lr: 0.000250  loss: 0.008953 (0.069548)  time: 0.737840  data: 0.621083  max mem: 572\n",
      "Epoch: [0]  [ 900/1463]  eta: 0:06:42  lr: 0.000250  loss: 0.011673 (0.068301)  time: 0.745919  data: 0.628103  max mem: 572\n",
      "Epoch: [0]  [ 920/1463]  eta: 0:06:28  lr: 0.000250  loss: 0.014350 (0.067323)  time: 0.743625  data: 0.626890  max mem: 572\n",
      "Epoch: [0]  [ 940/1463]  eta: 0:06:14  lr: 0.000250  loss: 0.017364 (0.066412)  time: 0.739277  data: 0.622559  max mem: 572\n",
      "Epoch: [0]  [ 960/1463]  eta: 0:06:00  lr: 0.000250  loss: 0.006664 (0.065233)  time: 0.735291  data: 0.617938  max mem: 572\n",
      "Epoch: [0]  [ 980/1463]  eta: 0:05:46  lr: 0.000250  loss: 0.010015 (0.064156)  time: 0.750879  data: 0.632290  max mem: 572\n",
      "Epoch: [0]  [1000/1463]  eta: 0:05:31  lr: 0.000250  loss: 0.012105 (0.063295)  time: 0.707313  data: 0.589019  max mem: 572\n",
      "Epoch: [0]  [1020/1463]  eta: 0:05:17  lr: 0.000250  loss: 0.006961 (0.062280)  time: 0.703280  data: 0.586652  max mem: 572\n",
      "Epoch: [0]  [1040/1463]  eta: 0:05:02  lr: 0.000250  loss: 0.009066 (0.061322)  time: 0.676382  data: 0.558562  max mem: 572\n",
      "Epoch: [0]  [1060/1463]  eta: 0:04:47  lr: 0.000250  loss: 0.009263 (0.060389)  time: 0.667981  data: 0.551327  max mem: 572\n",
      "Epoch: [0]  [1080/1463]  eta: 0:04:33  lr: 0.000250  loss: 0.008678 (0.059591)  time: 0.674476  data: 0.557836  max mem: 572\n",
      "Epoch: [0]  [1100/1463]  eta: 0:04:18  lr: 0.000250  loss: 0.006842 (0.058738)  time: 0.691178  data: 0.574540  max mem: 572\n",
      "Epoch: [0]  [1120/1463]  eta: 0:04:04  lr: 0.000250  loss: 0.009873 (0.057882)  time: 0.689331  data: 0.572790  max mem: 572\n",
      "Epoch: [0]  [1140/1463]  eta: 0:03:50  lr: 0.000250  loss: 0.010178 (0.057340)  time: 0.674408  data: 0.556540  max mem: 572\n",
      "Epoch: [0]  [1160/1463]  eta: 0:03:35  lr: 0.000250  loss: 0.007513 (0.056497)  time: 0.680960  data: 0.564240  max mem: 572\n",
      "Epoch: [0]  [1180/1463]  eta: 0:03:21  lr: 0.000250  loss: 0.009158 (0.055833)  time: 0.680229  data: 0.563483  max mem: 572\n",
      "Epoch: [0]  [1200/1463]  eta: 0:03:06  lr: 0.000250  loss: 0.010743 (0.055182)  time: 0.694931  data: 0.578265  max mem: 572\n",
      "Epoch: [0]  [1220/1463]  eta: 0:02:52  lr: 0.000250  loss: 0.008569 (0.054436)  time: 0.691779  data: 0.575156  max mem: 572\n",
      "Epoch: [0]  [1240/1463]  eta: 0:02:38  lr: 0.000250  loss: 0.010060 (0.053768)  time: 0.696179  data: 0.579531  max mem: 572\n",
      "Epoch: [0]  [1260/1463]  eta: 0:02:24  lr: 0.000250  loss: 0.009269 (0.053175)  time: 0.711804  data: 0.593651  max mem: 572\n",
      "Epoch: [0]  [1280/1463]  eta: 0:02:09  lr: 0.000250  loss: 0.010318 (0.052563)  time: 0.692310  data: 0.575585  max mem: 572\n",
      "Epoch: [0]  [1300/1463]  eta: 0:01:55  lr: 0.000250  loss: 0.008202 (0.051952)  time: 0.701679  data: 0.584831  max mem: 572\n",
      "Epoch: [0]  [1320/1463]  eta: 0:01:41  lr: 0.000250  loss: 0.008333 (0.051392)  time: 0.689698  data: 0.573157  max mem: 572\n",
      "Epoch: [0]  [1340/1463]  eta: 0:01:27  lr: 0.000250  loss: 0.006406 (0.050765)  time: 0.689728  data: 0.572969  max mem: 572\n",
      "Epoch: [0]  [1360/1463]  eta: 0:01:13  lr: 0.000250  loss: 0.008431 (0.050183)  time: 0.683690  data: 0.567187  max mem: 572\n",
      "Epoch: [0]  [1380/1463]  eta: 0:00:58  lr: 0.000250  loss: 0.008118 (0.049623)  time: 0.691092  data: 0.574432  max mem: 572\n",
      "Epoch: [0]  [1400/1463]  eta: 0:00:44  lr: 0.000250  loss: 0.007757 (0.049106)  time: 1.006102  data: 0.887239  max mem: 572\n",
      "Epoch: [0]  [1420/1463]  eta: 0:00:30  lr: 0.000250  loss: 0.009614 (0.048582)  time: 0.786125  data: 0.666236  max mem: 572\n",
      "Epoch: [0]  [1440/1463]  eta: 0:00:16  lr: 0.000250  loss: 0.010638 (0.048089)  time: 0.782214  data: 0.664476  max mem: 572\n",
      "Epoch: [0]  [1460/1463]  eta: 0:00:02  lr: 0.000250  loss: 0.006351 (0.047627)  time: 0.768224  data: 0.647960  max mem: 572\n",
      "Epoch: [0]  [1462/1463]  eta: 0:00:00  lr: 0.000250  loss: 0.006351 (0.047703)  time: 0.749663  data: 0.632855  max mem: 572\n",
      "Epoch: [0] Total time: 0:17:26 (0.715621 s / it)\n",
      "Averaged stats: lr: 0.000250  loss: 0.006351 (0.047703)\n",
      "Test:  [ 0/57]  eta: 0:00:54  loss: 0.002226 (0.002226)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.949999  data: 0.810722  max mem: 572\n",
      "Test:  [ 5/57]  eta: 0:00:42  loss: 0.003208 (0.004571)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.817367  data: 0.697113  max mem: 572\n",
      "Test:  [10/57]  eta: 0:00:37  loss: 0.005834 (0.006978)  acc1: 100.000000 (99.857955)  acc5: 100.000000 (100.000000)  time: 0.797333  data: 0.678794  max mem: 572\n",
      "Test:  [15/57]  eta: 0:00:33  loss: 0.005834 (0.007710)  acc1: 100.000000 (99.804688)  acc5: 100.000000 (100.000000)  time: 0.802413  data: 0.684508  max mem: 572\n",
      "Test:  [20/57]  eta: 0:00:29  loss: 0.005927 (0.009191)  acc1: 100.000000 (99.776786)  acc5: 100.000000 (100.000000)  time: 0.776093  data: 0.659587  max mem: 572\n",
      "Test:  [25/57]  eta: 0:00:25  loss: 0.007425 (0.009584)  acc1: 100.000000 (99.759615)  acc5: 100.000000 (100.000000)  time: 0.776588  data: 0.660055  max mem: 572\n",
      "Test:  [30/57]  eta: 0:00:21  loss: 0.005927 (0.008868)  acc1: 100.000000 (99.798387)  acc5: 100.000000 (100.000000)  time: 0.773638  data: 0.657018  max mem: 572\n",
      "Test:  [35/57]  eta: 0:00:17  loss: 0.008519 (0.009128)  acc1: 100.000000 (99.826389)  acc5: 100.000000 (100.000000)  time: 0.765881  data: 0.648256  max mem: 572\n",
      "Test:  [40/57]  eta: 0:00:13  loss: 0.008519 (0.009840)  acc1: 100.000000 (99.771341)  acc5: 100.000000 (100.000000)  time: 0.773651  data: 0.655858  max mem: 572\n",
      "Test:  [45/57]  eta: 0:00:09  loss: 0.007102 (0.009434)  acc1: 100.000000 (99.796196)  acc5: 100.000000 (100.000000)  time: 0.767540  data: 0.649718  max mem: 572\n",
      "Test:  [50/57]  eta: 0:00:05  loss: 0.008040 (0.009305)  acc1: 100.000000 (99.816176)  acc5: 100.000000 (100.000000)  time: 0.781736  data: 0.663935  max mem: 572\n",
      "Test:  [55/57]  eta: 0:00:01  loss: 0.008292 (0.009829)  acc1: 100.000000 (99.832589)  acc5: 100.000000 (100.000000)  time: 0.783562  data: 0.666699  max mem: 572\n",
      "Test:  [56/57]  eta: 0:00:00  loss: 0.008040 (0.009706)  acc1: 100.000000 (99.833333)  acc5: 100.000000 (100.000000)  time: 0.754208  data: 0.641795  max mem: 572\n",
      "Test: Total time: 0:00:44 (0.772550 s / it)\n",
      "* Acc@1 99.833 Acc@5 100.000 loss 0.010\n",
      "Accuracy at epoch 0 of the network on the 57 test images: 99.8%\n",
      "Max accuracy so far: 99.83%\n",
      "Epoch: [1]  [   0/1463]  eta: 0:19:17  lr: 0.000125  loss: 0.012343 (0.012343)  time: 0.791506  data: 0.674687  max mem: 572\n",
      "Epoch: [1]  [  20/1463]  eta: 0:18:54  lr: 0.000125  loss: 0.010939 (0.012708)  time: 0.786245  data: 0.669587  max mem: 572\n",
      "Epoch: [1]  [  40/1463]  eta: 0:18:24  lr: 0.000125  loss: 0.006621 (0.010562)  time: 0.765119  data: 0.647738  max mem: 572\n",
      "Epoch: [1]  [  60/1463]  eta: 0:18:07  lr: 0.000125  loss: 0.007406 (0.009751)  time: 0.774034  data: 0.653909  max mem: 572\n",
      "Epoch: [1]  [  80/1463]  eta: 0:17:47  lr: 0.000125  loss: 0.006873 (0.009325)  time: 0.760694  data: 0.640046  max mem: 572\n",
      "Epoch: [1]  [ 100/1463]  eta: 0:17:32  lr: 0.000125  loss: 0.008143 (0.009587)  time: 0.772213  data: 0.652615  max mem: 572\n",
      "Epoch: [1]  [ 120/1463]  eta: 0:17:17  lr: 0.000125  loss: 0.006381 (0.009292)  time: 0.778019  data: 0.660305  max mem: 572\n",
      "Epoch: [1]  [ 140/1463]  eta: 0:17:05  lr: 0.000125  loss: 0.007434 (0.009510)  time: 0.790277  data: 0.672473  max mem: 572\n",
      "Epoch: [1]  [ 160/1463]  eta: 0:16:50  lr: 0.000125  loss: 0.006297 (0.009502)  time: 0.775869  data: 0.657746  max mem: 572\n",
      "Epoch: [1]  [ 180/1463]  eta: 0:16:45  lr: 0.000125  loss: 0.009624 (0.009507)  time: 0.847513  data: 0.725585  max mem: 572\n",
      "Epoch: [1]  [ 200/1463]  eta: 0:16:33  lr: 0.000125  loss: 0.005786 (0.009279)  time: 0.812219  data: 0.691218  max mem: 572\n",
      "Epoch: [1]  [ 220/1463]  eta: 0:16:17  lr: 0.000125  loss: 0.005767 (0.009092)  time: 0.790326  data: 0.671762  max mem: 572\n",
      "Epoch: [1]  [ 240/1463]  eta: 0:16:01  lr: 0.000125  loss: 0.007289 (0.009111)  time: 0.785075  data: 0.668443  max mem: 572\n",
      "Epoch: [1]  [ 260/1463]  eta: 0:15:49  lr: 0.000125  loss: 0.006819 (0.009117)  time: 0.821596  data: 0.704241  max mem: 572\n",
      "Epoch: [1]  [ 280/1463]  eta: 0:15:31  lr: 0.000125  loss: 0.005916 (0.009035)  time: 0.768053  data: 0.650173  max mem: 572\n",
      "Epoch: [1]  [ 300/1463]  eta: 0:15:12  lr: 0.000125  loss: 0.006576 (0.008999)  time: 0.735654  data: 0.618130  max mem: 572\n",
      "Epoch: [1]  [ 320/1463]  eta: 0:14:52  lr: 0.000125  loss: 0.010224 (0.009090)  time: 0.727725  data: 0.611175  max mem: 572\n",
      "Epoch: [1]  [ 340/1463]  eta: 0:14:40  lr: 0.000125  loss: 0.005632 (0.009214)  time: 0.836671  data: 0.719830  max mem: 572\n",
      "Epoch: [1]  [ 360/1463]  eta: 0:14:22  lr: 0.000125  loss: 0.005619 (0.009062)  time: 0.753530  data: 0.636842  max mem: 572\n",
      "Epoch: [1]  [ 380/1463]  eta: 0:14:03  lr: 0.000125  loss: 0.004805 (0.008959)  time: 0.722987  data: 0.606231  max mem: 572\n",
      "Epoch: [1]  [ 400/1463]  eta: 0:13:44  lr: 0.000125  loss: 0.004493 (0.009056)  time: 0.717236  data: 0.599714  max mem: 572\n",
      "Epoch: [1]  [ 420/1463]  eta: 0:13:28  lr: 0.000125  loss: 0.005252 (0.009056)  time: 0.755753  data: 0.638967  max mem: 572\n",
      "Epoch: [1]  [ 440/1463]  eta: 0:13:12  lr: 0.000125  loss: 0.006553 (0.009089)  time: 0.756968  data: 0.640270  max mem: 572\n",
      "Epoch: [1]  [ 460/1463]  eta: 0:12:54  lr: 0.000125  loss: 0.005462 (0.009081)  time: 0.725024  data: 0.608311  max mem: 572\n",
      "Epoch: [1]  [ 480/1463]  eta: 0:12:35  lr: 0.000125  loss: 0.007588 (0.009057)  time: 0.697048  data: 0.580243  max mem: 572\n",
      "Epoch: [1]  [ 500/1463]  eta: 0:12:19  lr: 0.000125  loss: 0.006076 (0.009098)  time: 0.743480  data: 0.626649  max mem: 572\n",
      "Epoch: [1]  [ 520/1463]  eta: 0:12:04  lr: 0.000125  loss: 0.006174 (0.009127)  time: 0.775016  data: 0.654680  max mem: 572\n",
      "Epoch: [1]  [ 540/1463]  eta: 0:11:48  lr: 0.000125  loss: 0.005124 (0.009191)  time: 0.743486  data: 0.625487  max mem: 572\n",
      "Epoch: [1]  [ 560/1463]  eta: 0:11:32  lr: 0.000125  loss: 0.007734 (0.009305)  time: 0.753313  data: 0.635282  max mem: 572\n",
      "Epoch: [1]  [ 580/1463]  eta: 0:11:16  lr: 0.000125  loss: 0.006035 (0.009270)  time: 0.744570  data: 0.627091  max mem: 572\n",
      "Epoch: [1]  [ 600/1463]  eta: 0:11:01  lr: 0.000125  loss: 0.004696 (0.009275)  time: 0.765890  data: 0.649227  max mem: 572\n",
      "Epoch: [1]  [ 620/1463]  eta: 0:10:45  lr: 0.000125  loss: 0.008968 (0.009463)  time: 0.759463  data: 0.642947  max mem: 572\n",
      "Epoch: [1]  [ 640/1463]  eta: 0:10:29  lr: 0.000125  loss: 0.005582 (0.009474)  time: 0.727004  data: 0.610492  max mem: 572\n",
      "Epoch: [1]  [ 660/1463]  eta: 0:10:13  lr: 0.000125  loss: 0.004704 (0.009374)  time: 0.723819  data: 0.607088  max mem: 572\n",
      "Epoch: [1]  [ 680/1463]  eta: 0:09:57  lr: 0.000125  loss: 0.003492 (0.009295)  time: 0.771929  data: 0.651863  max mem: 572\n",
      "Epoch: [1]  [ 700/1463]  eta: 0:09:42  lr: 0.000125  loss: 0.006688 (0.009302)  time: 0.765178  data: 0.646292  max mem: 572\n",
      "Epoch: [1]  [ 720/1463]  eta: 0:09:27  lr: 0.000125  loss: 0.007846 (0.009348)  time: 0.746792  data: 0.627563  max mem: 572\n",
      "Epoch: [1]  [ 740/1463]  eta: 0:09:11  lr: 0.000125  loss: 0.005528 (0.009404)  time: 0.765988  data: 0.647433  max mem: 572\n",
      "Epoch: [1]  [ 760/1463]  eta: 0:09:00  lr: 0.000125  loss: 0.005308 (0.009393)  time: 0.973701  data: 0.854855  max mem: 572\n",
      "Epoch: [1]  [ 780/1463]  eta: 0:09:10  lr: 0.000125  loss: 0.008231 (0.009483)  time: 2.242930  data: 2.104594  max mem: 572\n",
      "Epoch: [1]  [ 800/1463]  eta: 0:09:08  lr: 0.000125  loss: 0.004223 (0.009439)  time: 1.618628  data: 1.493774  max mem: 572\n",
      "Epoch: [1]  [ 820/1463]  eta: 0:08:49  lr: 0.000125  loss: 0.004847 (0.009408)  time: 0.694215  data: 0.577814  max mem: 572\n",
      "Epoch: [1]  [ 840/1463]  eta: 0:08:31  lr: 0.000125  loss: 0.008179 (0.009411)  time: 0.706601  data: 0.590194  max mem: 572\n",
      "Epoch: [1]  [ 860/1463]  eta: 0:08:13  lr: 0.000125  loss: 0.008097 (0.009471)  time: 0.706002  data: 0.588583  max mem: 572\n",
      "Epoch: [1]  [ 880/1463]  eta: 0:07:55  lr: 0.000125  loss: 0.004232 (0.009399)  time: 0.705403  data: 0.588805  max mem: 572\n",
      "Epoch: [1]  [ 900/1463]  eta: 0:07:37  lr: 0.000125  loss: 0.005540 (0.009327)  time: 0.693540  data: 0.576679  max mem: 572\n",
      "Epoch: [1]  [ 920/1463]  eta: 0:07:19  lr: 0.000125  loss: 0.004827 (0.009279)  time: 0.690410  data: 0.573222  max mem: 572\n",
      "Epoch: [1]  [ 940/1463]  eta: 0:07:02  lr: 0.000125  loss: 0.005007 (0.009305)  time: 0.721246  data: 0.603821  max mem: 572\n",
      "Epoch: [1]  [ 960/1463]  eta: 0:06:45  lr: 0.000125  loss: 0.010999 (0.009359)  time: 0.723025  data: 0.606338  max mem: 572\n",
      "Epoch: [1]  [ 980/1463]  eta: 0:06:28  lr: 0.000125  loss: 0.003551 (0.009281)  time: 0.702801  data: 0.586172  max mem: 572\n",
      "Epoch: [1]  [1000/1463]  eta: 0:06:11  lr: 0.000125  loss: 0.004957 (0.009239)  time: 0.693216  data: 0.576642  max mem: 572\n",
      "Epoch: [1]  [1020/1463]  eta: 0:05:54  lr: 0.000125  loss: 0.004601 (0.009224)  time: 0.731278  data: 0.614708  max mem: 572\n",
      "Epoch: [1]  [1040/1463]  eta: 0:05:38  lr: 0.000125  loss: 0.003667 (0.009144)  time: 0.723224  data: 0.602900  max mem: 572\n",
      "Epoch: [1]  [1060/1463]  eta: 0:05:21  lr: 0.000125  loss: 0.006284 (0.009241)  time: 0.744103  data: 0.625137  max mem: 572\n",
      "Epoch: [1]  [1080/1463]  eta: 0:05:05  lr: 0.000125  loss: 0.005485 (0.009210)  time: 0.710362  data: 0.593606  max mem: 572\n",
      "Epoch: [1]  [1100/1463]  eta: 0:04:48  lr: 0.000125  loss: 0.007983 (0.009257)  time: 0.731385  data: 0.613402  max mem: 572\n",
      "Epoch: [1]  [1120/1463]  eta: 0:04:32  lr: 0.000125  loss: 0.004410 (0.009224)  time: 0.765807  data: 0.646367  max mem: 572\n",
      "Epoch: [1]  [1140/1463]  eta: 0:04:16  lr: 0.000125  loss: 0.004650 (0.009218)  time: 0.736630  data: 0.620103  max mem: 572\n",
      "Epoch: [1]  [1160/1463]  eta: 0:04:00  lr: 0.000125  loss: 0.006663 (0.009182)  time: 0.732531  data: 0.615907  max mem: 572\n",
      "Epoch: [1]  [1180/1463]  eta: 0:03:44  lr: 0.000125  loss: 0.004592 (0.009118)  time: 0.775917  data: 0.657082  max mem: 572\n",
      "Epoch: [1]  [1200/1463]  eta: 0:03:28  lr: 0.000125  loss: 0.006219 (0.009103)  time: 0.791030  data: 0.671651  max mem: 572\n",
      "Epoch: [1]  [1220/1463]  eta: 0:03:12  lr: 0.000125  loss: 0.004482 (0.009082)  time: 0.757599  data: 0.638533  max mem: 572\n",
      "Epoch: [1]  [1240/1463]  eta: 0:02:56  lr: 0.000125  loss: 0.005902 (0.009076)  time: 0.744657  data: 0.626875  max mem: 572\n",
      "Epoch: [1]  [1260/1463]  eta: 0:02:40  lr: 0.000125  loss: 0.003959 (0.009046)  time: 0.754789  data: 0.635988  max mem: 572\n",
      "Epoch: [1]  [1280/1463]  eta: 0:02:24  lr: 0.000125  loss: 0.006244 (0.009043)  time: 0.791498  data: 0.674885  max mem: 572\n",
      "Epoch: [1]  [1300/1463]  eta: 0:02:08  lr: 0.000125  loss: 0.005153 (0.009029)  time: 0.750087  data: 0.633402  max mem: 572\n",
      "Epoch: [1]  [1320/1463]  eta: 0:01:52  lr: 0.000125  loss: 0.005338 (0.008988)  time: 0.767460  data: 0.648360  max mem: 572\n",
      "Epoch: [1]  [1340/1463]  eta: 0:01:37  lr: 0.000125  loss: 0.005331 (0.008942)  time: 0.785671  data: 0.663843  max mem: 572\n",
      "Epoch: [1]  [1360/1463]  eta: 0:01:21  lr: 0.000125  loss: 0.006568 (0.008936)  time: 0.802297  data: 0.685704  max mem: 572\n",
      "Epoch: [1]  [1380/1463]  eta: 0:01:05  lr: 0.000125  loss: 0.003232 (0.008917)  time: 1.008311  data: 0.890236  max mem: 572\n",
      "Epoch: [1]  [1400/1463]  eta: 0:00:49  lr: 0.000125  loss: 0.006885 (0.008910)  time: 0.739951  data: 0.621398  max mem: 572\n",
      "Epoch: [1]  [1420/1463]  eta: 0:00:34  lr: 0.000125  loss: 0.006949 (0.008943)  time: 0.754759  data: 0.636452  max mem: 572\n",
      "Epoch: [1]  [1440/1463]  eta: 0:00:18  lr: 0.000125  loss: 0.005590 (0.008981)  time: 0.788318  data: 0.670262  max mem: 572\n",
      "Epoch: [1]  [1460/1463]  eta: 0:00:02  lr: 0.000125  loss: 0.005151 (0.008958)  time: 0.739842  data: 0.621518  max mem: 572\n",
      "Epoch: [1]  [1462/1463]  eta: 0:00:00  lr: 0.000125  loss: 0.005000 (0.008951)  time: 0.717722  data: 0.603026  max mem: 572\n",
      "Epoch: [1] Total time: 0:19:16 (0.790672 s / it)\n",
      "Averaged stats: lr: 0.000125  loss: 0.005000 (0.008951)\n",
      "Test:  [ 0/57]  eta: 0:00:42  loss: 0.001729 (0.001729)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.746515  data: 0.630443  max mem: 572\n",
      "Test:  [ 5/57]  eta: 0:00:38  loss: 0.002578 (0.003105)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.734386  data: 0.617885  max mem: 572\n",
      "Test:  [10/57]  eta: 0:00:34  loss: 0.003238 (0.004475)  acc1: 100.000000 (99.857955)  acc5: 100.000000 (100.000000)  time: 0.739844  data: 0.623288  max mem: 572\n",
      "Test:  [15/57]  eta: 0:00:31  loss: 0.003216 (0.005008)  acc1: 100.000000 (99.902344)  acc5: 100.000000 (100.000000)  time: 0.749397  data: 0.632806  max mem: 572\n",
      "Test:  [20/57]  eta: 0:00:27  loss: 0.003800 (0.007119)  acc1: 100.000000 (99.851190)  acc5: 100.000000 (100.000000)  time: 0.741948  data: 0.625298  max mem: 572\n",
      "Test:  [25/57]  eta: 0:00:23  loss: 0.006802 (0.008139)  acc1: 100.000000 (99.819712)  acc5: 100.000000 (100.000000)  time: 0.747137  data: 0.630446  max mem: 572\n",
      "Test:  [30/57]  eta: 0:00:20  loss: 0.006802 (0.007534)  acc1: 100.000000 (99.848790)  acc5: 100.000000 (100.000000)  time: 0.748094  data: 0.631447  max mem: 572\n",
      "Test:  [35/57]  eta: 0:00:16  loss: 0.007220 (0.007490)  acc1: 100.000000 (99.869792)  acc5: 100.000000 (100.000000)  time: 0.755030  data: 0.638369  max mem: 572\n",
      "Test:  [40/57]  eta: 0:00:12  loss: 0.007220 (0.008302)  acc1: 100.000000 (99.809451)  acc5: 100.000000 (100.000000)  time: 0.762728  data: 0.646094  max mem: 572\n",
      "Test:  [45/57]  eta: 0:00:09  loss: 0.005291 (0.007852)  acc1: 100.000000 (99.830163)  acc5: 100.000000 (100.000000)  time: 0.766065  data: 0.649455  max mem: 572\n",
      "Test:  [50/57]  eta: 0:00:05  loss: 0.005291 (0.007534)  acc1: 100.000000 (99.846814)  acc5: 100.000000 (100.000000)  time: 0.783603  data: 0.665709  max mem: 572\n",
      "Test:  [55/57]  eta: 0:00:01  loss: 0.005126 (0.007364)  acc1: 100.000000 (99.860491)  acc5: 100.000000 (100.000000)  time: 0.785301  data: 0.667444  max mem: 572\n",
      "Test:  [56/57]  eta: 0:00:00  loss: 0.004794 (0.007250)  acc1: 100.000000 (99.861111)  acc5: 100.000000 (100.000000)  time: 0.754701  data: 0.641169  max mem: 572\n",
      "Test: Total time: 0:00:42 (0.754011 s / it)\n",
      "* Acc@1 99.861 Acc@5 100.000 loss 0.007\n",
      "Accuracy at epoch 1 of the network on the 57 test images: 99.9%\n",
      "Max accuracy so far: 99.86%\n",
      "Training of the supervised linear classifier on frozen features completed.\n",
      "Top-1 test accuracy: 99.9\n",
      "Total elapsed time (sec): 2290.76\n",
      "Finished training linear classifier on pgd_003\n",
      "######################### Training classifier for pgd_01 #########################\n",
      "Epoch: [0]  [   0/1463]  eta: 0:22:43  lr: 0.000250  loss: 3.589636 (3.589636)  time: 0.931760  data: 0.815490  max mem: 572\n",
      "Epoch: [0]  [  20/1463]  eta: 0:20:02  lr: 0.000250  loss: 1.040023 (1.614325)  time: 0.828561  data: 0.710051  max mem: 572\n",
      "Epoch: [0]  [  40/1463]  eta: 0:19:09  lr: 0.000250  loss: 0.229729 (0.944899)  time: 0.780765  data: 0.662254  max mem: 572\n",
      "Epoch: [0]  [  60/1463]  eta: 0:18:16  lr: 0.000250  loss: 0.091523 (0.671044)  time: 0.726761  data: 0.608438  max mem: 572\n",
      "Epoch: [0]  [  80/1463]  eta: 0:17:55  lr: 0.000250  loss: 0.087711 (0.530452)  time: 0.766713  data: 0.649262  max mem: 572\n",
      "Epoch: [0]  [ 100/1463]  eta: 0:17:32  lr: 0.000250  loss: 0.058741 (0.439926)  time: 0.749188  data: 0.631430  max mem: 572\n",
      "Epoch: [0]  [ 120/1463]  eta: 0:17:12  lr: 0.000250  loss: 0.057942 (0.378829)  time: 0.753616  data: 0.637034  max mem: 572\n",
      "Epoch: [0]  [ 140/1463]  eta: 0:16:54  lr: 0.000250  loss: 0.069512 (0.335610)  time: 0.755852  data: 0.639081  max mem: 572\n",
      "Epoch: [0]  [ 160/1463]  eta: 0:16:39  lr: 0.000250  loss: 0.053591 (0.300919)  time: 0.766686  data: 0.650082  max mem: 572\n",
      "Epoch: [0]  [ 180/1463]  eta: 0:16:19  lr: 0.000250  loss: 0.064898 (0.274529)  time: 0.737521  data: 0.620978  max mem: 572\n",
      "Epoch: [0]  [ 200/1463]  eta: 0:16:02  lr: 0.000250  loss: 0.046448 (0.252158)  time: 0.746660  data: 0.628862  max mem: 572\n",
      "Epoch: [0]  [ 220/1463]  eta: 0:15:46  lr: 0.000250  loss: 0.034563 (0.232753)  time: 0.758991  data: 0.642403  max mem: 572\n",
      "Epoch: [0]  [ 240/1463]  eta: 0:15:32  lr: 0.000250  loss: 0.028361 (0.216643)  time: 0.765045  data: 0.647146  max mem: 572\n",
      "Epoch: [0]  [ 260/1463]  eta: 0:15:13  lr: 0.000250  loss: 0.032686 (0.203057)  time: 0.724861  data: 0.608246  max mem: 572\n",
      "Epoch: [0]  [ 280/1463]  eta: 0:14:55  lr: 0.000250  loss: 0.026395 (0.191433)  time: 0.727986  data: 0.611321  max mem: 572\n",
      "Epoch: [0]  [ 300/1463]  eta: 0:14:38  lr: 0.000250  loss: 0.040087 (0.181713)  time: 0.729405  data: 0.611635  max mem: 572\n",
      "Epoch: [0]  [ 320/1463]  eta: 0:14:21  lr: 0.000250  loss: 0.028162 (0.172754)  time: 0.730414  data: 0.613781  max mem: 572\n",
      "Epoch: [0]  [ 340/1463]  eta: 0:14:05  lr: 0.000250  loss: 0.023378 (0.164310)  time: 0.735083  data: 0.617652  max mem: 572\n",
      "Epoch: [0]  [ 360/1463]  eta: 0:13:48  lr: 0.000250  loss: 0.031161 (0.157151)  time: 0.722766  data: 0.606164  max mem: 572\n",
      "Epoch: [0]  [ 380/1463]  eta: 0:13:29  lr: 0.000250  loss: 0.028450 (0.150906)  time: 0.690337  data: 0.573621  max mem: 572\n",
      "Epoch: [0]  [ 400/1463]  eta: 0:13:10  lr: 0.000250  loss: 0.024792 (0.144974)  time: 0.668442  data: 0.550547  max mem: 572\n",
      "Epoch: [0]  [ 420/1463]  eta: 0:12:52  lr: 0.000250  loss: 0.028573 (0.139602)  time: 0.670937  data: 0.554333  max mem: 572\n",
      "Epoch: [0]  [ 440/1463]  eta: 0:12:35  lr: 0.000250  loss: 0.021076 (0.134688)  time: 0.693650  data: 0.577049  max mem: 572\n",
      "Epoch: [0]  [ 460/1463]  eta: 0:12:17  lr: 0.000250  loss: 0.025850 (0.129921)  time: 0.681182  data: 0.564594  max mem: 572\n",
      "Epoch: [0]  [ 480/1463]  eta: 0:12:01  lr: 0.000250  loss: 0.024565 (0.125836)  time: 0.692802  data: 0.576083  max mem: 572\n",
      "Epoch: [0]  [ 500/1463]  eta: 0:11:44  lr: 0.000250  loss: 0.029089 (0.121911)  time: 0.680730  data: 0.563953  max mem: 572\n",
      "Epoch: [0]  [ 520/1463]  eta: 0:11:28  lr: 0.000250  loss: 0.020712 (0.118177)  time: 0.679217  data: 0.562553  max mem: 572\n",
      "Epoch: [0]  [ 540/1463]  eta: 0:11:11  lr: 0.000250  loss: 0.019409 (0.114825)  time: 0.679127  data: 0.562468  max mem: 572\n",
      "Epoch: [0]  [ 560/1463]  eta: 0:10:55  lr: 0.000250  loss: 0.017212 (0.111686)  time: 0.683661  data: 0.566065  max mem: 572\n",
      "Epoch: [0]  [ 580/1463]  eta: 0:10:40  lr: 0.000250  loss: 0.018635 (0.108804)  time: 0.704071  data: 0.587358  max mem: 572\n",
      "Epoch: [0]  [ 600/1463]  eta: 0:10:24  lr: 0.000250  loss: 0.025666 (0.106080)  time: 0.675723  data: 0.559102  max mem: 572\n",
      "Epoch: [0]  [ 620/1463]  eta: 0:10:09  lr: 0.000250  loss: 0.028306 (0.103530)  time: 0.687683  data: 0.571025  max mem: 572\n",
      "Epoch: [0]  [ 640/1463]  eta: 0:09:53  lr: 0.000250  loss: 0.014735 (0.101062)  time: 0.689684  data: 0.573006  max mem: 572\n",
      "Epoch: [0]  [ 660/1463]  eta: 0:09:38  lr: 0.000250  loss: 0.029191 (0.099086)  time: 0.699605  data: 0.583037  max mem: 572\n",
      "Epoch: [0]  [ 680/1463]  eta: 0:09:24  lr: 0.000250  loss: 0.018422 (0.096998)  time: 0.710827  data: 0.594173  max mem: 572\n",
      "Epoch: [0]  [ 700/1463]  eta: 0:09:09  lr: 0.000250  loss: 0.012685 (0.094958)  time: 0.694568  data: 0.577937  max mem: 572\n",
      "Epoch: [0]  [ 720/1463]  eta: 0:08:55  lr: 0.000250  loss: 0.017941 (0.093055)  time: 0.722330  data: 0.605512  max mem: 572\n",
      "Epoch: [0]  [ 740/1463]  eta: 0:08:40  lr: 0.000250  loss: 0.014056 (0.091128)  time: 0.696610  data: 0.578740  max mem: 572\n",
      "Epoch: [0]  [ 760/1463]  eta: 0:08:25  lr: 0.000250  loss: 0.016953 (0.089375)  time: 0.693837  data: 0.577178  max mem: 572\n",
      "Epoch: [0]  [ 780/1463]  eta: 0:08:10  lr: 0.000250  loss: 0.016804 (0.087749)  time: 0.706020  data: 0.589386  max mem: 572\n",
      "Epoch: [0]  [ 800/1463]  eta: 0:07:55  lr: 0.000250  loss: 0.018034 (0.086209)  time: 0.686503  data: 0.569794  max mem: 572\n",
      "Epoch: [0]  [ 820/1463]  eta: 0:07:40  lr: 0.000250  loss: 0.018760 (0.084618)  time: 0.678225  data: 0.561456  max mem: 572\n",
      "Epoch: [0]  [ 840/1463]  eta: 0:07:26  lr: 0.000250  loss: 0.019537 (0.083274)  time: 0.699851  data: 0.583216  max mem: 572\n",
      "Epoch: [0]  [ 860/1463]  eta: 0:07:11  lr: 0.000250  loss: 0.013923 (0.081808)  time: 0.701033  data: 0.584433  max mem: 572\n",
      "Epoch: [0]  [ 880/1463]  eta: 0:06:57  lr: 0.000250  loss: 0.013119 (0.080471)  time: 0.709499  data: 0.591371  max mem: 572\n",
      "Epoch: [0]  [ 900/1463]  eta: 0:06:42  lr: 0.000250  loss: 0.013350 (0.079118)  time: 0.683367  data: 0.566763  max mem: 572\n",
      "Epoch: [0]  [ 920/1463]  eta: 0:06:27  lr: 0.000250  loss: 0.014845 (0.077793)  time: 0.691280  data: 0.573477  max mem: 572\n",
      "Epoch: [0]  [ 940/1463]  eta: 0:06:13  lr: 0.000250  loss: 0.021864 (0.076954)  time: 0.701328  data: 0.583757  max mem: 572\n",
      "Epoch: [0]  [ 960/1463]  eta: 0:05:59  lr: 0.000250  loss: 0.016243 (0.075769)  time: 0.700394  data: 0.583845  max mem: 572\n",
      "Epoch: [0]  [ 980/1463]  eta: 0:05:44  lr: 0.000250  loss: 0.015516 (0.074661)  time: 0.690793  data: 0.572768  max mem: 572\n",
      "Epoch: [0]  [1000/1463]  eta: 0:05:30  lr: 0.000250  loss: 0.014402 (0.073560)  time: 0.703841  data: 0.587139  max mem: 572\n",
      "Epoch: [0]  [1020/1463]  eta: 0:05:15  lr: 0.000250  loss: 0.012468 (0.072432)  time: 0.677264  data: 0.560494  max mem: 572\n",
      "Epoch: [0]  [1040/1463]  eta: 0:05:01  lr: 0.000250  loss: 0.012762 (0.071339)  time: 0.694368  data: 0.577443  max mem: 572\n",
      "Epoch: [0]  [1060/1463]  eta: 0:04:46  lr: 0.000250  loss: 0.016536 (0.070335)  time: 0.701289  data: 0.583531  max mem: 572\n",
      "Epoch: [0]  [1080/1463]  eta: 0:04:32  lr: 0.000250  loss: 0.015851 (0.069428)  time: 0.705911  data: 0.589274  max mem: 572\n",
      "Epoch: [0]  [1100/1463]  eta: 0:04:18  lr: 0.000250  loss: 0.009727 (0.068496)  time: 0.702881  data: 0.584951  max mem: 572\n",
      "Epoch: [0]  [1120/1463]  eta: 0:04:04  lr: 0.000250  loss: 0.015113 (0.067667)  time: 0.704756  data: 0.587961  max mem: 572\n",
      "Epoch: [0]  [1140/1463]  eta: 0:03:49  lr: 0.000250  loss: 0.011105 (0.066899)  time: 0.681306  data: 0.564591  max mem: 572\n",
      "Epoch: [0]  [1160/1463]  eta: 0:03:35  lr: 0.000250  loss: 0.008687 (0.066049)  time: 0.682890  data: 0.566152  max mem: 572\n",
      "Epoch: [0]  [1180/1463]  eta: 0:03:20  lr: 0.000250  loss: 0.010609 (0.065231)  time: 0.675477  data: 0.558669  max mem: 572\n",
      "Epoch: [0]  [1200/1463]  eta: 0:03:06  lr: 0.000250  loss: 0.012370 (0.064482)  time: 0.674403  data: 0.557709  max mem: 572\n",
      "Epoch: [0]  [1220/1463]  eta: 0:02:52  lr: 0.000250  loss: 0.008250 (0.063730)  time: 0.684486  data: 0.567926  max mem: 572\n",
      "Epoch: [0]  [1240/1463]  eta: 0:02:37  lr: 0.000250  loss: 0.013850 (0.063118)  time: 0.654578  data: 0.537821  max mem: 572\n",
      "Epoch: [0]  [1260/1463]  eta: 0:02:23  lr: 0.000250  loss: 0.007429 (0.062398)  time: 0.690063  data: 0.573435  max mem: 572\n",
      "Epoch: [0]  [1280/1463]  eta: 0:02:09  lr: 0.000250  loss: 0.008181 (0.061625)  time: 0.677669  data: 0.560943  max mem: 572\n",
      "Epoch: [0]  [1300/1463]  eta: 0:01:55  lr: 0.000250  loss: 0.008217 (0.060919)  time: 0.662255  data: 0.545446  max mem: 572\n",
      "Epoch: [0]  [1320/1463]  eta: 0:01:41  lr: 0.000250  loss: 0.018887 (0.060325)  time: 0.688769  data: 0.572108  max mem: 572\n",
      "Epoch: [0]  [1340/1463]  eta: 0:01:26  lr: 0.000250  loss: 0.009422 (0.059672)  time: 0.689620  data: 0.572768  max mem: 572\n",
      "Epoch: [0]  [1360/1463]  eta: 0:01:12  lr: 0.000250  loss: 0.011332 (0.059018)  time: 0.662175  data: 0.545550  max mem: 572\n",
      "Epoch: [0]  [1380/1463]  eta: 0:00:58  lr: 0.000250  loss: 0.015162 (0.058456)  time: 0.665497  data: 0.548826  max mem: 572\n",
      "Epoch: [0]  [1400/1463]  eta: 0:00:44  lr: 0.000250  loss: 0.014374 (0.057882)  time: 0.680292  data: 0.563509  max mem: 572\n",
      "Epoch: [0]  [1420/1463]  eta: 0:00:30  lr: 0.000250  loss: 0.015246 (0.057297)  time: 0.692797  data: 0.576195  max mem: 572\n",
      "Epoch: [0]  [1440/1463]  eta: 0:00:16  lr: 0.000250  loss: 0.013210 (0.056726)  time: 0.694579  data: 0.576835  max mem: 572\n",
      "Epoch: [0]  [1460/1463]  eta: 0:00:02  lr: 0.000250  loss: 0.015056 (0.056150)  time: 0.685351  data: 0.568696  max mem: 572\n",
      "Epoch: [0]  [1462/1463]  eta: 0:00:00  lr: 0.000250  loss: 0.015056 (0.056077)  time: 0.666152  data: 0.553200  max mem: 572\n",
      "Epoch: [0] Total time: 0:17:09 (0.703818 s / it)\n",
      "Averaged stats: lr: 0.000250  loss: 0.015056 (0.056077)\n",
      "Test:  [ 0/57]  eta: 0:00:35  loss: 0.002310 (0.002310)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.623609  data: 0.506827  max mem: 572\n",
      "Test:  [ 5/57]  eta: 0:00:34  loss: 0.013356 (0.033276)  acc1: 98.437500 (99.218750)  acc5: 100.000000 (100.000000)  time: 0.659016  data: 0.542115  max mem: 572\n",
      "Test:  [10/57]  eta: 0:00:30  loss: 0.013356 (0.028757)  acc1: 100.000000 (99.289773)  acc5: 100.000000 (100.000000)  time: 0.640974  data: 0.524155  max mem: 572\n",
      "Test:  [15/57]  eta: 0:00:27  loss: 0.007995 (0.022087)  acc1: 100.000000 (99.414062)  acc5: 100.000000 (100.000000)  time: 0.651794  data: 0.535044  max mem: 572\n",
      "Test:  [20/57]  eta: 0:00:24  loss: 0.013356 (0.024339)  acc1: 100.000000 (99.404762)  acc5: 100.000000 (100.000000)  time: 0.664699  data: 0.547940  max mem: 572\n",
      "Test:  [25/57]  eta: 0:00:21  loss: 0.012326 (0.025822)  acc1: 100.000000 (99.338942)  acc5: 100.000000 (99.939904)  time: 0.669495  data: 0.552833  max mem: 572\n",
      "Test:  [30/57]  eta: 0:00:18  loss: 0.012326 (0.023811)  acc1: 100.000000 (99.395161)  acc5: 100.000000 (99.949597)  time: 0.689155  data: 0.572466  max mem: 572\n",
      "Test:  [35/57]  eta: 0:00:14  loss: 0.014531 (0.022271)  acc1: 100.000000 (99.479167)  acc5: 100.000000 (99.956597)  time: 0.690088  data: 0.573359  max mem: 572\n",
      "Test:  [40/57]  eta: 0:00:11  loss: 0.012326 (0.021022)  acc1: 100.000000 (99.542683)  acc5: 100.000000 (99.961890)  time: 0.688173  data: 0.571486  max mem: 572\n",
      "Test:  [45/57]  eta: 0:00:08  loss: 0.008245 (0.019560)  acc1: 100.000000 (99.592391)  acc5: 100.000000 (99.966033)  time: 0.684581  data: 0.567751  max mem: 572\n",
      "Test:  [50/57]  eta: 0:00:04  loss: 0.012227 (0.020004)  acc1: 100.000000 (99.540441)  acc5: 100.000000 (99.969363)  time: 0.684249  data: 0.567473  max mem: 572\n",
      "Test:  [55/57]  eta: 0:00:01  loss: 0.008142 (0.018725)  acc1: 100.000000 (99.581473)  acc5: 100.000000 (99.972098)  time: 0.687641  data: 0.570891  max mem: 572\n",
      "Test:  [56/57]  eta: 0:00:00  loss: 0.006701 (0.018405)  acc1: 100.000000 (99.583333)  acc5: 100.000000 (99.972222)  time: 0.663312  data: 0.550865  max mem: 572\n",
      "Test: Total time: 0:00:38 (0.669322 s / it)\n",
      "* Acc@1 99.583 Acc@5 99.972 loss 0.018\n",
      "Accuracy at epoch 0 of the network on the 57 test images: 99.6%\n",
      "Max accuracy so far: 99.58%\n",
      "Epoch: [1]  [   0/1463]  eta: 0:16:14  lr: 0.000125  loss: 0.004450 (0.004450)  time: 0.666292  data: 0.549403  max mem: 572\n",
      "Epoch: [1]  [  20/1463]  eta: 0:16:54  lr: 0.000125  loss: 0.009840 (0.014585)  time: 0.705097  data: 0.587298  max mem: 572\n",
      "Epoch: [1]  [  40/1463]  eta: 0:16:30  lr: 0.000125  loss: 0.006790 (0.013230)  time: 0.688444  data: 0.571795  max mem: 572\n",
      "Epoch: [1]  [  60/1463]  eta: 0:16:19  lr: 0.000125  loss: 0.009758 (0.013664)  time: 0.702343  data: 0.585731  max mem: 572\n",
      "Epoch: [1]  [  80/1463]  eta: 0:16:04  lr: 0.000125  loss: 0.006885 (0.014286)  time: 0.696390  data: 0.579870  max mem: 572\n",
      "Epoch: [1]  [ 100/1463]  eta: 0:15:50  lr: 0.000125  loss: 0.011503 (0.015410)  time: 0.695021  data: 0.578324  max mem: 572\n",
      "Epoch: [1]  [ 120/1463]  eta: 0:15:41  lr: 0.000125  loss: 0.008372 (0.015653)  time: 0.720934  data: 0.604285  max mem: 572\n",
      "Epoch: [1]  [ 140/1463]  eta: 0:15:24  lr: 0.000125  loss: 0.007252 (0.014940)  time: 0.687130  data: 0.570513  max mem: 572\n",
      "Epoch: [1]  [ 160/1463]  eta: 0:15:13  lr: 0.000125  loss: 0.017230 (0.015604)  time: 0.714243  data: 0.597550  max mem: 572\n",
      "Epoch: [1]  [ 180/1463]  eta: 0:16:02  lr: 0.000125  loss: 0.007027 (0.014997)  time: 1.145062  data: 1.024895  max mem: 572\n",
      "Epoch: [1]  [ 200/1463]  eta: 0:19:47  lr: 0.000125  loss: 0.007699 (0.014988)  time: 2.658226  data: 2.520395  max mem: 572\n",
      "Epoch: [1]  [ 220/1463]  eta: 0:22:11  lr: 0.000125  loss: 0.013681 (0.015472)  time: 2.388737  data: 2.251868  max mem: 572\n",
      "Epoch: [1]  [ 240/1463]  eta: 0:23:33  lr: 0.000125  loss: 0.009287 (0.015342)  time: 2.088294  data: 1.953594  max mem: 572\n",
      "Epoch: [1]  [ 260/1463]  eta: 0:22:57  lr: 0.000125  loss: 0.010621 (0.015414)  time: 1.022906  data: 0.904412  max mem: 572\n",
      "Epoch: [1]  [ 280/1463]  eta: 0:21:57  lr: 0.000125  loss: 0.008090 (0.015168)  time: 0.702290  data: 0.585297  max mem: 572\n",
      "Epoch: [1]  [ 300/1463]  eta: 0:21:04  lr: 0.000125  loss: 0.008356 (0.015220)  time: 0.713224  data: 0.597188  max mem: 572\n",
      "Epoch: [1]  [ 320/1463]  eta: 0:20:16  lr: 0.000125  loss: 0.011984 (0.015350)  time: 0.717694  data: 0.600876  max mem: 572\n",
      "Epoch: [1]  [ 340/1463]  eta: 0:19:31  lr: 0.000125  loss: 0.007870 (0.015308)  time: 0.709340  data: 0.592523  max mem: 572\n",
      "Epoch: [1]  [ 360/1463]  eta: 0:18:49  lr: 0.000125  loss: 0.010226 (0.015284)  time: 0.699029  data: 0.582140  max mem: 572\n",
      "Epoch: [1]  [ 380/1463]  eta: 0:18:10  lr: 0.000125  loss: 0.007474 (0.015218)  time: 0.693228  data: 0.575258  max mem: 572\n",
      "Epoch: [1]  [ 400/1463]  eta: 0:17:33  lr: 0.000125  loss: 0.008707 (0.015208)  time: 0.697183  data: 0.580442  max mem: 572\n",
      "Epoch: [1]  [ 420/1463]  eta: 0:16:58  lr: 0.000125  loss: 0.009353 (0.015249)  time: 0.685975  data: 0.569065  max mem: 572\n",
      "Epoch: [1]  [ 440/1463]  eta: 0:16:25  lr: 0.000125  loss: 0.005735 (0.015159)  time: 0.686203  data: 0.569469  max mem: 572\n",
      "Epoch: [1]  [ 460/1463]  eta: 0:15:54  lr: 0.000125  loss: 0.018113 (0.015213)  time: 0.683218  data: 0.566405  max mem: 572\n",
      "Epoch: [1]  [ 480/1463]  eta: 0:15:24  lr: 0.000125  loss: 0.005778 (0.015081)  time: 0.696355  data: 0.579462  max mem: 572\n",
      "Epoch: [1]  [ 500/1463]  eta: 0:14:55  lr: 0.000125  loss: 0.009883 (0.015045)  time: 0.671987  data: 0.555001  max mem: 572\n",
      "Epoch: [1]  [ 520/1463]  eta: 0:14:28  lr: 0.000125  loss: 0.009175 (0.014983)  time: 0.699210  data: 0.582284  max mem: 572\n",
      "Epoch: [1]  [ 540/1463]  eta: 0:14:03  lr: 0.000125  loss: 0.007370 (0.014776)  time: 0.715999  data: 0.599038  max mem: 572\n",
      "Epoch: [1]  [ 560/1463]  eta: 0:13:38  lr: 0.000125  loss: 0.009740 (0.014728)  time: 0.696135  data: 0.579240  max mem: 572\n",
      "Epoch: [1]  [ 580/1463]  eta: 0:13:13  lr: 0.000125  loss: 0.007262 (0.014533)  time: 0.703766  data: 0.585878  max mem: 572\n",
      "Epoch: [1]  [ 600/1463]  eta: 0:12:49  lr: 0.000125  loss: 0.005929 (0.014301)  time: 0.691737  data: 0.573480  max mem: 572\n",
      "Epoch: [1]  [ 620/1463]  eta: 0:12:27  lr: 0.000125  loss: 0.006565 (0.014256)  time: 0.708498  data: 0.591550  max mem: 572\n",
      "Epoch: [1]  [ 640/1463]  eta: 0:12:04  lr: 0.000125  loss: 0.007477 (0.014249)  time: 0.693490  data: 0.576683  max mem: 572\n",
      "Epoch: [1]  [ 660/1463]  eta: 0:11:42  lr: 0.000125  loss: 0.012163 (0.014242)  time: 0.701040  data: 0.584235  max mem: 572\n",
      "Epoch: [1]  [ 680/1463]  eta: 0:11:20  lr: 0.000125  loss: 0.009731 (0.014195)  time: 0.700670  data: 0.583841  max mem: 572\n",
      "Epoch: [1]  [ 700/1463]  eta: 0:10:59  lr: 0.000125  loss: 0.005228 (0.014074)  time: 0.685524  data: 0.567817  max mem: 572\n",
      "Epoch: [1]  [ 720/1463]  eta: 0:10:38  lr: 0.000125  loss: 0.009387 (0.014026)  time: 0.673615  data: 0.556677  max mem: 572\n",
      "Epoch: [1]  [ 740/1463]  eta: 0:10:17  lr: 0.000125  loss: 0.004036 (0.013880)  time: 0.694544  data: 0.577762  max mem: 572\n",
      "Epoch: [1]  [ 760/1463]  eta: 0:09:58  lr: 0.000125  loss: 0.011218 (0.013981)  time: 0.704650  data: 0.587754  max mem: 572\n",
      "Epoch: [1]  [ 780/1463]  eta: 0:09:38  lr: 0.000125  loss: 0.006658 (0.014072)  time: 0.718356  data: 0.600356  max mem: 572\n",
      "Epoch: [1]  [ 800/1463]  eta: 0:09:19  lr: 0.000125  loss: 0.008475 (0.014036)  time: 0.703112  data: 0.585616  max mem: 572\n",
      "Epoch: [1]  [ 820/1463]  eta: 0:09:00  lr: 0.000125  loss: 0.006790 (0.013985)  time: 0.682388  data: 0.565542  max mem: 572\n",
      "Epoch: [1]  [ 840/1463]  eta: 0:08:40  lr: 0.000125  loss: 0.013618 (0.014039)  time: 0.686746  data: 0.569856  max mem: 572\n",
      "Epoch: [1]  [ 860/1463]  eta: 0:08:22  lr: 0.000125  loss: 0.009885 (0.014034)  time: 0.681491  data: 0.564646  max mem: 572\n",
      "Epoch: [1]  [ 880/1463]  eta: 0:08:03  lr: 0.000125  loss: 0.007617 (0.014086)  time: 0.684299  data: 0.567486  max mem: 572\n",
      "Epoch: [1]  [ 900/1463]  eta: 0:07:45  lr: 0.000125  loss: 0.006046 (0.014046)  time: 0.699991  data: 0.582992  max mem: 572\n",
      "Epoch: [1]  [ 920/1463]  eta: 0:07:26  lr: 0.000125  loss: 0.011371 (0.014040)  time: 0.663005  data: 0.546081  max mem: 572\n",
      "Epoch: [1]  [ 940/1463]  eta: 0:07:08  lr: 0.000125  loss: 0.006807 (0.014029)  time: 0.685162  data: 0.568241  max mem: 572\n",
      "Epoch: [1]  [ 960/1463]  eta: 0:06:50  lr: 0.000125  loss: 0.005639 (0.014001)  time: 0.679653  data: 0.562821  max mem: 572\n",
      "Epoch: [1]  [ 980/1463]  eta: 0:06:33  lr: 0.000125  loss: 0.006913 (0.013950)  time: 0.684100  data: 0.567228  max mem: 572\n",
      "Epoch: [1]  [1000/1463]  eta: 0:06:15  lr: 0.000125  loss: 0.009070 (0.013966)  time: 0.688399  data: 0.571588  max mem: 572\n",
      "Epoch: [1]  [1020/1463]  eta: 0:05:58  lr: 0.000125  loss: 0.005402 (0.013945)  time: 0.692425  data: 0.574503  max mem: 572\n",
      "Epoch: [1]  [1040/1463]  eta: 0:05:41  lr: 0.000125  loss: 0.006241 (0.013910)  time: 0.681243  data: 0.564375  max mem: 572\n",
      "Epoch: [1]  [1060/1463]  eta: 0:05:24  lr: 0.000125  loss: 0.009014 (0.013868)  time: 0.686098  data: 0.569206  max mem: 572\n",
      "Epoch: [1]  [1080/1463]  eta: 0:05:07  lr: 0.000125  loss: 0.011590 (0.013902)  time: 0.710429  data: 0.593671  max mem: 572\n",
      "Epoch: [1]  [1100/1463]  eta: 0:04:50  lr: 0.000125  loss: 0.008832 (0.013935)  time: 0.708596  data: 0.591757  max mem: 572\n",
      "Epoch: [1]  [1120/1463]  eta: 0:04:34  lr: 0.000125  loss: 0.010807 (0.013908)  time: 0.692659  data: 0.575663  max mem: 572\n",
      "Epoch: [1]  [1140/1463]  eta: 0:04:17  lr: 0.000125  loss: 0.006939 (0.013916)  time: 0.737562  data: 0.619564  max mem: 572\n",
      "Epoch: [1]  [1160/1463]  eta: 0:04:01  lr: 0.000125  loss: 0.006493 (0.013853)  time: 0.739844  data: 0.623010  max mem: 572\n",
      "Epoch: [1]  [1180/1463]  eta: 0:03:45  lr: 0.000125  loss: 0.004088 (0.013780)  time: 0.713365  data: 0.595759  max mem: 572\n",
      "Epoch: [1]  [1200/1463]  eta: 0:03:28  lr: 0.000125  loss: 0.008928 (0.013760)  time: 0.713159  data: 0.595333  max mem: 572\n",
      "Epoch: [1]  [1220/1463]  eta: 0:03:12  lr: 0.000125  loss: 0.005629 (0.013724)  time: 0.724015  data: 0.604698  max mem: 572\n",
      "Epoch: [1]  [1240/1463]  eta: 0:02:56  lr: 0.000125  loss: 0.008738 (0.013700)  time: 0.731373  data: 0.614595  max mem: 572\n",
      "Epoch: [1]  [1260/1463]  eta: 0:02:40  lr: 0.000125  loss: 0.007157 (0.013622)  time: 0.716949  data: 0.600127  max mem: 572\n",
      "Epoch: [1]  [1280/1463]  eta: 0:02:24  lr: 0.000125  loss: 0.007449 (0.013656)  time: 0.701470  data: 0.582980  max mem: 572\n",
      "Epoch: [1]  [1300/1463]  eta: 0:02:08  lr: 0.000125  loss: 0.006024 (0.013604)  time: 0.714096  data: 0.596045  max mem: 572\n",
      "Epoch: [1]  [1320/1463]  eta: 0:01:52  lr: 0.000125  loss: 0.006664 (0.013534)  time: 0.728205  data: 0.610331  max mem: 572\n",
      "Epoch: [1]  [1340/1463]  eta: 0:01:36  lr: 0.000125  loss: 0.010629 (0.013571)  time: 0.709874  data: 0.591970  max mem: 572\n",
      "Epoch: [1]  [1360/1463]  eta: 0:01:20  lr: 0.000125  loss: 0.006355 (0.013561)  time: 0.734693  data: 0.617815  max mem: 572\n",
      "Epoch: [1]  [1380/1463]  eta: 0:01:05  lr: 0.000125  loss: 0.014162 (0.013604)  time: 0.725296  data: 0.608454  max mem: 572\n",
      "Epoch: [1]  [1400/1463]  eta: 0:00:49  lr: 0.000125  loss: 0.008981 (0.013585)  time: 0.741488  data: 0.624526  max mem: 572\n",
      "Epoch: [1]  [1420/1463]  eta: 0:00:33  lr: 0.000125  loss: 0.006490 (0.013591)  time: 0.741411  data: 0.624534  max mem: 572\n",
      "Epoch: [1]  [1440/1463]  eta: 0:00:18  lr: 0.000125  loss: 0.011135 (0.013598)  time: 0.741448  data: 0.624705  max mem: 572\n",
      "Epoch: [1]  [1460/1463]  eta: 0:00:02  lr: 0.000125  loss: 0.007726 (0.013583)  time: 0.713383  data: 0.596515  max mem: 572\n",
      "Epoch: [1]  [1462/1463]  eta: 0:00:00  lr: 0.000125  loss: 0.004812 (0.013569)  time: 0.693766  data: 0.580570  max mem: 572\n",
      "Epoch: [1] Total time: 0:19:03 (0.781721 s / it)\n",
      "Averaged stats: lr: 0.000125  loss: 0.004812 (0.013569)\n",
      "Test:  [ 0/57]  eta: 0:00:47  loss: 0.001161 (0.001161)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.826388  data: 0.709824  max mem: 572\n",
      "Test:  [ 5/57]  eta: 0:00:40  loss: 0.008707 (0.027900)  acc1: 98.437500 (99.218750)  acc5: 100.000000 (100.000000)  time: 0.786424  data: 0.669668  max mem: 572\n",
      "Test:  [10/57]  eta: 0:00:36  loss: 0.014741 (0.029017)  acc1: 100.000000 (99.289773)  acc5: 100.000000 (100.000000)  time: 0.771137  data: 0.654312  max mem: 572\n",
      "Test:  [15/57]  eta: 0:00:30  loss: 0.006083 (0.023246)  acc1: 100.000000 (99.414062)  acc5: 100.000000 (100.000000)  time: 0.733825  data: 0.617000  max mem: 572\n",
      "Test:  [20/57]  eta: 0:00:27  loss: 0.014741 (0.024654)  acc1: 100.000000 (99.330357)  acc5: 100.000000 (100.000000)  time: 0.733087  data: 0.616164  max mem: 572\n",
      "Test:  [25/57]  eta: 0:00:23  loss: 0.014741 (0.024527)  acc1: 100.000000 (99.278846)  acc5: 100.000000 (100.000000)  time: 0.720232  data: 0.603280  max mem: 572\n",
      "Test:  [30/57]  eta: 0:00:19  loss: 0.007835 (0.021968)  acc1: 100.000000 (99.344758)  acc5: 100.000000 (100.000000)  time: 0.704370  data: 0.587425  max mem: 572\n",
      "Test:  [35/57]  eta: 0:00:15  loss: 0.010884 (0.020085)  acc1: 100.000000 (99.435764)  acc5: 100.000000 (100.000000)  time: 0.713264  data: 0.596351  max mem: 572\n",
      "Test:  [40/57]  eta: 0:00:12  loss: 0.007835 (0.018992)  acc1: 100.000000 (99.504573)  acc5: 100.000000 (100.000000)  time: 0.707821  data: 0.590971  max mem: 572\n",
      "Test:  [45/57]  eta: 0:00:08  loss: 0.005017 (0.017718)  acc1: 100.000000 (99.558424)  acc5: 100.000000 (100.000000)  time: 0.701463  data: 0.584649  max mem: 572\n",
      "Test:  [50/57]  eta: 0:00:05  loss: 0.006491 (0.016978)  acc1: 100.000000 (99.601716)  acc5: 100.000000 (100.000000)  time: 0.711880  data: 0.595096  max mem: 572\n",
      "Test:  [55/57]  eta: 0:00:01  loss: 0.005017 (0.015746)  acc1: 100.000000 (99.637277)  acc5: 100.000000 (100.000000)  time: 0.743957  data: 0.627163  max mem: 572\n",
      "Test:  [56/57]  eta: 0:00:00  loss: 0.004477 (0.015474)  acc1: 100.000000 (99.638889)  acc5: 100.000000 (100.000000)  time: 0.713355  data: 0.600879  max mem: 572\n",
      "Test: Total time: 0:00:41 (0.720246 s / it)\n",
      "* Acc@1 99.639 Acc@5 100.000 loss 0.015\n",
      "Accuracy at epoch 1 of the network on the 57 test images: 99.6%\n",
      "Max accuracy so far: 99.64%\n",
      "Training of the supervised linear classifier on frozen features completed.\n",
      "Top-1 test accuracy: 99.6\n",
      "Total elapsed time (sec): 2252.59\n",
      "Finished training linear classifier on pgd_01\n"
     ]
    }
   ],
   "source": [
    "for attack, loaders in loader_dict.items():\n",
    "    if attack == \"pgd_0001\": \n",
    "        continue\n",
    "    pstr = \"#\"*25 + f''' Training classifier for {attack} ''' + \"#\"*25\n",
    "    print(pstr)\n",
    "    start = time.time()\n",
    "    # Initialise linear classifier\n",
    "    adv_linear_classifier = LinearClassifier(linear_classifier.linear.in_features, num_labels=9)\n",
    "    adv_linear_classifier = adv_linear_classifier.cuda()\n",
    "    \n",
    "    loggers = train(model, \n",
    "                    adv_linear_classifier, \n",
    "                    loaders[\"train\"], \n",
    "                    loaders[\"validation\"], \n",
    "                    log_dir=Path(MODELS_PATH, attack + \"_ensemble\"),\n",
    "                    tensor_dir=None, \n",
    "                    optimizer=None, \n",
    "                    adversarial_attack=None,\n",
    "                    criterion=nn.CrossEntropyLoss(),\n",
    "                    epochs=EPOCHS, \n",
    "                    val_freq=1, \n",
    "                    batch_size=BATCH_SIZE,  \n",
    "                    lr=0.001, \n",
    "                    to_restore = {\"epoch\": 0, \"best_acc\": 0.}, \n",
    "                    n=4, \n",
    "                    avgpool_patchtokens=False, \n",
    "                    show_image=False)\n",
    "    \n",
    "    print('Total elapsed time (sec): %.2f' % (time.time() - start))\n",
    "    \n",
    "    # Save adversarial Classifier\n",
    "    save_path = Path(MODELS_PATH, attack + \"_ensemble\")\n",
    "    \n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    save_file_log = f\"log_{attack}.pt\"\n",
    "    torch.save(loggers, Path(save_path, save_file_log))\n",
    "    \n",
    "    print(f'''Finished training linear classifier on {attack}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on all adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### evaluating adv_classifier trained on pgd_0001 #########################\n",
      "Found checkpoint at /cluster/scratch/thobauma/data/models/pgd_0001_ensemble/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/thobauma/data/models/pgd_0001_ensemble/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      ">>>>> pgd_0001 dataset: 3600 \n",
      "saving predictions to: /cluster/scratch/thobauma/data/models/pgd_0001_ensemble/eval_c_pgd_0001_d_pgd_0001.csv\n",
      "Test:  [ 0/57]  eta: 0:00:37  loss: 0.018175 (0.018175)  acc1: 98.437500 (98.437500)  acc5: 100.000000 (100.000000)  time: 0.650984  data: 0.534325  max mem: 609\n",
      "Test:  [10/57]  eta: 0:00:33  loss: 0.035731 (0.063089)  acc1: 98.437500 (98.579545)  acc5: 100.000000 (99.857955)  time: 0.710343  data: 0.591147  max mem: 610\n",
      "Test:  [20/57]  eta: 0:00:24  loss: 0.111997 (0.189978)  acc1: 95.312500 (93.973214)  acc5: 100.000000 (99.776786)  time: 0.671126  data: 0.550544  max mem: 610\n",
      "Test:  [30/57]  eta: 0:00:18  loss: 0.190450 (0.237175)  acc1: 93.750000 (92.842742)  acc5: 100.000000 (99.798387)  time: 0.643848  data: 0.523306  max mem: 610\n",
      "Test:  [40/57]  eta: 0:00:11  loss: 0.188898 (0.213810)  acc1: 95.312500 (93.597561)  acc5: 100.000000 (99.847561)  time: 0.643330  data: 0.525168  max mem: 610\n",
      "Test:  [50/57]  eta: 0:00:04  loss: 0.098477 (0.197672)  acc1: 95.312500 (93.903186)  acc5: 100.000000 (99.846814)  time: 0.650889  data: 0.533960  max mem: 610\n",
      "Test:  [56/57]  eta: 0:00:00  loss: 0.133902 (0.215668)  acc1: 93.750000 (93.583333)  acc5: 100.000000 (99.805556)  time: 0.626299  data: 0.513763  max mem: 610\n",
      "Test: Total time: 0:00:37 (0.649996 s / it)\n",
      "* Acc@1 93.583 Acc@5 99.806 loss 0.216\n",
      "\n",
      "\n",
      ">>>>> pgd_003 dataset: 3600 \n",
      "saving predictions to: /cluster/scratch/thobauma/data/models/pgd_0001_ensemble/eval_c_pgd_0001_d_pgd_003.csv\n",
      "Test:  [ 0/57]  eta: 0:00:39  loss: 9.166490 (9.166490)  acc1: 1.562500 (1.562500)  acc5: 65.625000 (65.625000)  time: 0.689675  data: 0.572682  max mem: 610\n",
      "Test:  [10/57]  eta: 0:00:31  loss: 10.522482 (12.037348)  acc1: 0.000000 (0.284091)  acc5: 50.000000 (35.511364)  time: 0.664337  data: 0.546033  max mem: 610\n",
      "Test:  [20/57]  eta: 0:00:23  loss: 10.102365 (11.252486)  acc1: 0.000000 (0.148810)  acc5: 42.187500 (41.815476)  time: 0.636285  data: 0.518626  max mem: 610\n",
      "Test:  [30/57]  eta: 0:00:17  loss: 10.020187 (10.985251)  acc1: 0.000000 (0.100806)  acc5: 42.187500 (43.649194)  time: 0.650120  data: 0.533274  max mem: 610\n",
      "Test:  [40/57]  eta: 0:00:10  loss: 8.554073 (10.197693)  acc1: 0.000000 (1.371951)  acc5: 71.875000 (51.105183)  time: 0.644782  data: 0.527909  max mem: 610\n",
      "Test:  [50/57]  eta: 0:00:04  loss: 11.514731 (10.455705)  acc1: 0.000000 (1.133578)  acc5: 45.312500 (49.724265)  time: 0.618613  data: 0.501737  max mem: 610\n",
      "Test:  [56/57]  eta: 0:00:00  loss: 11.514731 (10.103849)  acc1: 0.000000 (1.083333)  acc5: 45.312500 (51.861111)  time: 0.608340  data: 0.495775  max mem: 610\n",
      "Test: Total time: 0:00:36 (0.636668 s / it)\n",
      "* Acc@1 1.083 Acc@5 51.861 loss 10.104\n",
      "\n",
      "\n",
      ">>>>> pgd_01 dataset: 3600 \n",
      "saving predictions to: /cluster/scratch/thobauma/data/models/pgd_0001_ensemble/eval_c_pgd_0001_d_pgd_01.csv\n",
      "Test:  [ 0/57]  eta: 0:00:39  loss: 10.196193 (10.196193)  acc1: 1.562500 (1.562500)  acc5: 34.375000 (34.375000)  time: 0.701567  data: 0.584035  max mem: 610\n",
      "Test:  [10/57]  eta: 0:00:32  loss: 10.996619 (12.608644)  acc1: 0.000000 (0.426136)  acc5: 18.750000 (15.198864)  time: 0.696394  data: 0.579480  max mem: 610\n",
      "Test:  [20/57]  eta: 0:00:28  loss: 11.586950 (12.049035)  acc1: 0.000000 (0.223214)  acc5: 18.750000 (22.098214)  time: 0.786855  data: 0.669985  max mem: 610\n",
      "Test:  [30/57]  eta: 0:00:22  loss: 11.663514 (11.654040)  acc1: 0.000000 (0.252016)  acc5: 17.187500 (26.411290)  time: 0.884373  data: 0.765088  max mem: 610\n",
      "Test:  [40/57]  eta: 0:00:13  loss: 8.722858 (10.829230)  acc1: 0.000000 (1.638720)  acc5: 50.000000 (35.670732)  time: 0.794038  data: 0.674812  max mem: 610\n",
      "Test:  [50/57]  eta: 0:00:05  loss: 11.734437 (11.131145)  acc1: 0.000000 (1.348039)  acc5: 23.437500 (32.567402)  time: 0.708777  data: 0.591869  max mem: 610\n",
      "Test:  [56/57]  eta: 0:00:00  loss: 11.734437 (10.826074)  acc1: 0.000000 (1.222222)  acc5: 23.437500 (32.583333)  time: 0.688793  data: 0.576165  max mem: 610\n",
      "Test: Total time: 0:00:43 (0.758493 s / it)\n",
      "* Acc@1 1.222 Acc@5 32.583 loss 10.826\n",
      "\n",
      "\n",
      "######################### evaluating adv_classifier trained on pgd_003 #########################\n",
      "Found checkpoint at /cluster/scratch/thobauma/data/models/pgd_003_ensemble/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/thobauma/data/models/pgd_003_ensemble/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      ">>>>> pgd_0001 dataset: 3600 \n",
      "saving predictions to: /cluster/scratch/thobauma/data/models/pgd_003_ensemble/eval_c_pgd_003_d_pgd_0001.csv\n",
      "Test:  [ 0/57]  eta: 0:00:33  loss: 7.266345 (7.266345)  acc1: 0.000000 (0.000000)  acc5: 4.687500 (4.687500)  time: 0.591992  data: 0.475449  max mem: 610\n",
      "Test:  [10/57]  eta: 0:00:29  loss: 7.542824 (7.973733)  acc1: 0.000000 (0.568182)  acc5: 7.812500 (9.090909)  time: 0.622898  data: 0.505929  max mem: 610\n",
      "Test:  [20/57]  eta: 0:00:22  loss: 7.110862 (7.049300)  acc1: 1.562500 (2.752976)  acc5: 15.625000 (19.791667)  time: 0.603323  data: 0.486311  max mem: 610\n",
      "Test:  [30/57]  eta: 0:00:16  loss: 5.811355 (7.237497)  acc1: 3.125000 (2.872984)  acc5: 29.687500 (19.909274)  time: 0.583807  data: 0.466807  max mem: 610\n",
      "Test:  [40/57]  eta: 0:00:10  loss: 6.225214 (6.989692)  acc1: 1.562500 (2.743902)  acc5: 17.187500 (20.617378)  time: 0.597201  data: 0.480285  max mem: 610\n",
      "Test:  [50/57]  eta: 0:00:04  loss: 6.844575 (7.108331)  acc1: 1.562500 (2.420343)  acc5: 15.625000 (19.301471)  time: 0.587805  data: 0.470951  max mem: 610\n",
      "Test:  [56/57]  eta: 0:00:00  loss: 6.994649 (7.069075)  acc1: 0.000000 (2.222222)  acc5: 12.500000 (18.972222)  time: 0.558450  data: 0.445937  max mem: 610\n",
      "Test: Total time: 0:00:33 (0.584441 s / it)\n",
      "* Acc@1 2.222 Acc@5 18.972 loss 7.069\n",
      "\n",
      "\n",
      ">>>>> pgd_003 dataset: 3600 \n",
      "saving predictions to: /cluster/scratch/thobauma/data/models/pgd_003_ensemble/eval_c_pgd_003_d_pgd_003.csv\n",
      "Test:  [ 0/57]  eta: 0:00:32  loss: 0.001729 (0.001729)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.569281  data: 0.451991  max mem: 610\n",
      "Test:  [10/57]  eta: 0:00:27  loss: 0.003238 (0.004475)  acc1: 100.000000 (99.857955)  acc5: 100.000000 (100.000000)  time: 0.579833  data: 0.462475  max mem: 610\n",
      "Test:  [20/57]  eta: 0:00:21  loss: 0.003800 (0.007119)  acc1: 100.000000 (99.851190)  acc5: 100.000000 (100.000000)  time: 0.586084  data: 0.468968  max mem: 610\n",
      "Test:  [30/57]  eta: 0:00:15  loss: 0.006802 (0.007534)  acc1: 100.000000 (99.848790)  acc5: 100.000000 (100.000000)  time: 0.578048  data: 0.461135  max mem: 610\n",
      "Test:  [40/57]  eta: 0:00:09  loss: 0.007220 (0.008302)  acc1: 100.000000 (99.809451)  acc5: 100.000000 (100.000000)  time: 0.573949  data: 0.456088  max mem: 610\n",
      "Test:  [50/57]  eta: 0:00:04  loss: 0.005291 (0.007534)  acc1: 100.000000 (99.846814)  acc5: 100.000000 (100.000000)  time: 0.589408  data: 0.469562  max mem: 610\n",
      "Test:  [56/57]  eta: 0:00:00  loss: 0.004794 (0.007250)  acc1: 100.000000 (99.861111)  acc5: 100.000000 (100.000000)  time: 0.575853  data: 0.460311  max mem: 610\n",
      "Test: Total time: 0:00:32 (0.577483 s / it)\n",
      "* Acc@1 99.861 Acc@5 100.000 loss 0.007\n",
      "\n",
      "\n",
      ">>>>> pgd_01 dataset: 3600 \n",
      "saving predictions to: /cluster/scratch/thobauma/data/models/pgd_003_ensemble/eval_c_pgd_003_d_pgd_01.csv\n",
      "Test:  [ 0/57]  eta: 0:00:34  loss: 0.001736 (0.001736)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.606653  data: 0.489883  max mem: 610\n",
      "Test:  [10/57]  eta: 0:00:29  loss: 0.014520 (0.019787)  acc1: 100.000000 (99.431818)  acc5: 100.000000 (100.000000)  time: 0.620956  data: 0.504051  max mem: 610\n",
      "Test:  [20/57]  eta: 0:00:22  loss: 0.015467 (0.027315)  acc1: 100.000000 (99.255952)  acc5: 100.000000 (100.000000)  time: 0.613120  data: 0.496199  max mem: 610\n",
      "Test:  [30/57]  eta: 0:00:16  loss: 0.015995 (0.025695)  acc1: 100.000000 (99.294355)  acc5: 100.000000 (100.000000)  time: 0.597925  data: 0.480873  max mem: 610\n",
      "Test:  [40/57]  eta: 0:00:10  loss: 0.021104 (0.029044)  acc1: 100.000000 (99.199695)  acc5: 100.000000 (99.961890)  time: 0.594034  data: 0.476939  max mem: 610\n",
      "Test:  [50/57]  eta: 0:00:04  loss: 0.018998 (0.030775)  acc1: 100.000000 (99.203431)  acc5: 100.000000 (99.908088)  time: 0.594071  data: 0.475721  max mem: 610\n",
      "Test:  [56/57]  eta: 0:00:00  loss: 0.012235 (0.028913)  acc1: 100.000000 (99.250000)  acc5: 100.000000 (99.916667)  time: 0.576483  data: 0.462002  max mem: 610\n",
      "Test: Total time: 0:00:33 (0.595587 s / it)\n",
      "* Acc@1 99.250 Acc@5 99.917 loss 0.029\n",
      "\n",
      "\n",
      "######################### evaluating adv_classifier trained on pgd_01 #########################\n",
      "Found checkpoint at /cluster/scratch/thobauma/data/models/pgd_01_ensemble/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/thobauma/data/models/pgd_01_ensemble/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      ">>>>> pgd_0001 dataset: 3600 \n",
      "saving predictions to: /cluster/scratch/thobauma/data/models/pgd_01_ensemble/eval_c_pgd_01_d_pgd_0001.csv\n",
      "Test:  [ 0/57]  eta: 0:00:36  loss: 9.667666 (9.667666)  acc1: 0.000000 (0.000000)  acc5: 0.000000 (0.000000)  time: 0.646753  data: 0.506791  max mem: 610\n",
      "Test:  [10/57]  eta: 0:00:28  loss: 10.133394 (10.127817)  acc1: 0.000000 (0.142045)  acc5: 3.125000 (2.840909)  time: 0.609573  data: 0.490529  max mem: 610\n",
      "Test:  [20/57]  eta: 0:00:22  loss: 9.496325 (8.690214)  acc1: 0.000000 (2.083333)  acc5: 4.687500 (13.095238)  time: 0.612865  data: 0.494662  max mem: 610\n",
      "Test:  [30/57]  eta: 0:00:16  loss: 7.208076 (8.824438)  acc1: 1.562500 (2.368952)  acc5: 20.312500 (14.919355)  time: 0.631848  data: 0.513637  max mem: 610\n",
      "Test:  [40/57]  eta: 0:00:10  loss: 7.964616 (8.630527)  acc1: 1.562500 (2.019817)  acc5: 6.250000 (13.986280)  time: 0.637860  data: 0.520938  max mem: 610\n",
      "Test:  [50/57]  eta: 0:00:04  loss: 8.238955 (8.675417)  acc1: 0.000000 (1.746324)  acc5: 7.812500 (12.714461)  time: 0.632094  data: 0.515212  max mem: 610\n",
      "Test:  [56/57]  eta: 0:00:00  loss: 8.087496 (8.562867)  acc1: 1.562500 (1.750000)  acc5: 7.812500 (13.305556)  time: 0.603686  data: 0.491074  max mem: 610\n",
      "Test: Total time: 0:00:35 (0.618234 s / it)\n",
      "* Acc@1 1.750 Acc@5 13.306 loss 8.563\n",
      "\n",
      "\n",
      ">>>>> pgd_003 dataset: 3600 \n",
      "saving predictions to: /cluster/scratch/thobauma/data/models/pgd_01_ensemble/eval_c_pgd_01_d_pgd_003.csv\n",
      "Test:  [ 0/57]  eta: 0:00:34  loss: 0.003826 (0.003826)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.597316  data: 0.480129  max mem: 610\n",
      "Test:  [10/57]  eta: 0:00:30  loss: 0.010405 (0.022293)  acc1: 100.000000 (99.573864)  acc5: 100.000000 (100.000000)  time: 0.648697  data: 0.529495  max mem: 610\n",
      "Test:  [20/57]  eta: 0:00:23  loss: 0.010405 (0.019824)  acc1: 100.000000 (99.553571)  acc5: 100.000000 (100.000000)  time: 0.623852  data: 0.505666  max mem: 610\n",
      "Test:  [30/57]  eta: 0:00:16  loss: 0.013659 (0.019266)  acc1: 100.000000 (99.596774)  acc5: 100.000000 (100.000000)  time: 0.604232  data: 0.487244  max mem: 610\n",
      "Test:  [40/57]  eta: 0:00:10  loss: 0.021339 (0.025151)  acc1: 100.000000 (99.352134)  acc5: 100.000000 (100.000000)  time: 0.616534  data: 0.499617  max mem: 610\n",
      "Test:  [50/57]  eta: 0:00:04  loss: 0.007243 (0.021384)  acc1: 100.000000 (99.448529)  acc5: 100.000000 (100.000000)  time: 0.604052  data: 0.487223  max mem: 610\n",
      "Test:  [56/57]  eta: 0:00:00  loss: 0.005053 (0.019748)  acc1: 100.000000 (99.500000)  acc5: 100.000000 (100.000000)  time: 0.580665  data: 0.466830  max mem: 610\n",
      "Test: Total time: 0:00:34 (0.608130 s / it)\n",
      "* Acc@1 99.500 Acc@5 100.000 loss 0.020\n",
      "\n",
      "\n",
      ">>>>> pgd_01 dataset: 3600 \n",
      "saving predictions to: /cluster/scratch/thobauma/data/models/pgd_01_ensemble/eval_c_pgd_01_d_pgd_01.csv\n",
      "Test:  [ 0/57]  eta: 0:00:40  loss: 0.001161 (0.001161)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.718341  data: 0.601063  max mem: 610\n",
      "Test:  [10/57]  eta: 0:00:28  loss: 0.014741 (0.029017)  acc1: 100.000000 (99.289773)  acc5: 100.000000 (100.000000)  time: 0.604824  data: 0.487979  max mem: 610\n",
      "Test:  [20/57]  eta: 0:00:21  loss: 0.014741 (0.024654)  acc1: 100.000000 (99.330357)  acc5: 100.000000 (100.000000)  time: 0.571164  data: 0.454287  max mem: 610\n",
      "Test:  [30/57]  eta: 0:00:16  loss: 0.007835 (0.021968)  acc1: 100.000000 (99.344758)  acc5: 100.000000 (100.000000)  time: 0.601965  data: 0.484905  max mem: 610\n",
      "Test:  [40/57]  eta: 0:00:10  loss: 0.007835 (0.018992)  acc1: 100.000000 (99.504573)  acc5: 100.000000 (100.000000)  time: 0.637451  data: 0.515632  max mem: 610\n",
      "Test:  [50/57]  eta: 0:00:04  loss: 0.006491 (0.016978)  acc1: 100.000000 (99.601716)  acc5: 100.000000 (100.000000)  time: 0.621805  data: 0.498872  max mem: 610\n",
      "Test:  [56/57]  eta: 0:00:00  loss: 0.004477 (0.015474)  acc1: 100.000000 (99.638889)  acc5: 100.000000 (100.000000)  time: 0.608009  data: 0.489879  max mem: 610\n",
      "Test: Total time: 0:00:34 (0.606491 s / it)\n",
      "* Acc@1 99.639 Acc@5 100.000 loss 0.015\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger_dict = defaultdict(dict)\n",
    "for attack in ADV_DATASETS:\n",
    "    pstr = \"#\"*25 + f''' evaluating adv_classifier trained on {attack} ''' + \"#\"*25\n",
    "    print(pstr)\n",
    "    adv_classifier = LinearClassifier(linear_classifier.linear.in_features, num_labels=9)\n",
    "    adv_classifier.to(DEVICE)\n",
    "\n",
    "    # load from checkpoint\n",
    "    log_dir = Path(MODELS_PATH, attack + \"_ensemble\")\n",
    "    to_restore={'epoch': 1}\n",
    "    utils.restart_from_checkpoint(Path(log_dir, \"checkpoint.pth.tar\"),\n",
    "                                  run_variables=to_restore,\n",
    "                                  state_dict=adv_classifier)\n",
    "\n",
    "    for applied_attack in ADV_DATASETS:\n",
    "        print(\">\"*5 + f''' {applied_attack} dataset: {len(loader_dict[applied_attack][\"validation\"].dataset)} ''')\n",
    "        info, logger = validate_network(model, \n",
    "                                       adv_classifier, \n",
    "                                       loader_dict[applied_attack][\"validation\"], \n",
    "                                       criterion=nn.CrossEntropyLoss(),\n",
    "                                       tensor_dir=None, \n",
    "                                       adversarial_attack=None, \n",
    "                                       n=4, \n",
    "                                       avgpool_patchtokens=False, \n",
    "                                       path_predictions=Path(log_dir, 'eval_c_'+attack+'_d_'+applied_attack+'.csv'),\n",
    "                                       log_interval = 10)\n",
    "        logger_dict[attack][applied_attack] = logger\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Adversarial Linear Classifiers on Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### evaluating adv_classifier trained on pgd_0001 on clean data#########################\n",
      "Found checkpoint at /cluster/scratch/thobauma/data/models/pgd_0001_ensemble/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/thobauma/data/models/pgd_0001_ensemble/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      "saving predictions to: /cluster/scratch/thobauma/data/models/pgd_0001_ensemble/eval_c_pgd_0001_clean.csv\n",
      "Test:  [ 0/57]  eta: 0:01:18  loss: 0.003434 (0.003434)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 1.382143  data: 1.242851  max mem: 610\n",
      "Test:  [10/57]  eta: 0:01:09  loss: 0.008220 (0.025999)  acc1: 100.000000 (99.431818)  acc5: 100.000000 (100.000000)  time: 1.476518  data: 1.352885  max mem: 610\n",
      "Test:  [20/57]  eta: 0:00:54  loss: 0.060744 (0.100284)  acc1: 98.437500 (96.949405)  acc5: 100.000000 (99.925595)  time: 1.485005  data: 1.366102  max mem: 610\n",
      "Test:  [30/57]  eta: 0:00:38  loss: 0.071355 (0.118682)  acc1: 96.875000 (96.270161)  acc5: 100.000000 (99.949597)  time: 1.380210  data: 1.263183  max mem: 610\n",
      "Test:  [40/57]  eta: 0:00:23  loss: 0.058813 (0.110329)  acc1: 96.875000 (96.684451)  acc5: 100.000000 (99.961890)  time: 1.298798  data: 1.180315  max mem: 610\n",
      "Test:  [50/57]  eta: 0:00:09  loss: 0.049951 (0.099642)  acc1: 98.437500 (96.997549)  acc5: 100.000000 (99.969363)  time: 1.360080  data: 1.240608  max mem: 610\n",
      "Test:  [56/57]  eta: 0:00:01  loss: 0.058813 (0.108417)  acc1: 98.437500 (96.777778)  acc5: 100.000000 (99.944444)  time: 1.387887  data: 1.270728  max mem: 610\n",
      "Test: Total time: 0:01:19 (1.395868 s / it)\n",
      "* Acc@1 96.778 Acc@5 99.944 loss 0.108\n",
      "\n",
      "\n",
      "######################### evaluating adv_classifier trained on pgd_003 on clean data#########################\n",
      "Found checkpoint at /cluster/scratch/thobauma/data/models/pgd_003_ensemble/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/thobauma/data/models/pgd_003_ensemble/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      "saving predictions to: /cluster/scratch/thobauma/data/models/pgd_003_ensemble/eval_c_pgd_003_clean.csv\n",
      "Test:  [ 0/57]  eta: 0:01:14  loss: 9.415708 (9.415708)  acc1: 0.000000 (0.000000)  acc5: 0.000000 (0.000000)  time: 1.309984  data: 1.193389  max mem: 610\n",
      "Test:  [10/57]  eta: 0:00:54  loss: 9.579427 (9.837637)  acc1: 0.000000 (0.000000)  acc5: 1.562500 (1.278409)  time: 1.169474  data: 1.045476  max mem: 610\n",
      "Test:  [20/57]  eta: 0:00:42  loss: 9.093814 (8.780738)  acc1: 0.000000 (0.818452)  acc5: 3.125000 (7.961310)  time: 1.126490  data: 1.006007  max mem: 610\n",
      "Test:  [30/57]  eta: 0:00:30  loss: 7.442236 (9.046067)  acc1: 1.562500 (0.856855)  acc5: 15.625000 (8.316532)  time: 1.089312  data: 0.971292  max mem: 610\n",
      "Test:  [40/57]  eta: 0:00:18  loss: 7.792565 (8.773993)  acc1: 0.000000 (0.762195)  acc5: 6.250000 (8.269817)  time: 1.078976  data: 0.958172  max mem: 610\n",
      "Test:  [50/57]  eta: 0:00:07  loss: 8.511780 (8.848548)  acc1: 0.000000 (0.674020)  acc5: 4.687500 (7.352941)  time: 0.969328  data: 0.850251  max mem: 610\n",
      "Test:  [56/57]  eta: 0:00:01  loss: 8.686807 (8.818521)  acc1: 0.000000 (0.611111)  acc5: 3.125000 (6.777778)  time: 0.956146  data: 0.842917  max mem: 610\n",
      "Test: Total time: 0:01:00 (1.060008 s / it)\n",
      "* Acc@1 0.611 Acc@5 6.778 loss 8.819\n",
      "\n",
      "\n",
      "######################### evaluating adv_classifier trained on pgd_01 on clean data#########################\n",
      "Found checkpoint at /cluster/scratch/thobauma/data/models/pgd_01_ensemble/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/thobauma/data/models/pgd_01_ensemble/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      "saving predictions to: /cluster/scratch/thobauma/data/models/pgd_01_ensemble/eval_c_pgd_01_clean.csv\n",
      "Test:  [ 0/57]  eta: 0:01:02  loss: 11.753239 (11.753239)  acc1: 0.000000 (0.000000)  acc5: 0.000000 (0.000000)  time: 1.101013  data: 0.984619  max mem: 610\n",
      "Test:  [10/57]  eta: 0:00:47  loss: 11.795234 (11.925652)  acc1: 0.000000 (0.000000)  acc5: 0.000000 (0.142045)  time: 1.020140  data: 0.901500  max mem: 610\n",
      "Test:  [20/57]  eta: 0:00:41  loss: 11.599141 (10.376634)  acc1: 0.000000 (0.520833)  acc5: 1.562500 (5.357143)  time: 1.114488  data: 0.994859  max mem: 610\n",
      "Test:  [30/57]  eta: 0:00:28  loss: 8.899147 (10.603748)  acc1: 0.000000 (0.453629)  acc5: 6.250000 (6.502016)  time: 1.080005  data: 0.960269  max mem: 610\n",
      "Test:  [40/57]  eta: 0:00:17  loss: 9.666268 (10.389713)  acc1: 0.000000 (0.342988)  acc5: 1.562500 (5.564024)  time: 0.945610  data: 0.826503  max mem: 610\n",
      "Test:  [50/57]  eta: 0:00:07  loss: 9.784945 (10.392088)  acc1: 0.000000 (0.337010)  acc5: 3.125000 (5.116422)  time: 0.930971  data: 0.813180  max mem: 610\n",
      "Test:  [56/57]  eta: 0:00:00  loss: 9.784945 (10.303057)  acc1: 0.000000 (0.305556)  acc5: 3.125000 (5.055556)  time: 0.888154  data: 0.775925  max mem: 610\n",
      "Test: Total time: 0:00:56 (0.984066 s / it)\n",
      "* Acc@1 0.306 Acc@5 5.056 loss 10.303\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_logger_dict = defaultdict(dict)\n",
    "for attack in ADV_DATASETS:\n",
    "    pstr = \"#\"*25 + f''' evaluating adv_classifier trained on {attack} on Clean''' + \"#\"*25\n",
    "    print(pstr)\n",
    "    adv_classifier = LinearClassifier(linear_classifier.linear.in_features, num_labels=9)\n",
    "    adv_classifier.to(DEVICE)\n",
    "\n",
    "    # load from checkpoint\n",
    "    log_dir = Path(MODELS_PATH, attack + \"_ensemble\")\n",
    "    to_restore={'epoch': 1}\n",
    "    utils.restart_from_checkpoint(Path(log_dir, \"checkpoint.pth.tar\"),\n",
    "                                  run_variables=to_restore,\n",
    "                                  state_dict=adv_classifier)\n",
    "\n",
    "    print(\">\"*5 + f''' {applied_attack} dataset: {len(clean_val_loader.dataset)} ''')\n",
    "    info, logger = validate_network(model, \n",
    "                                   adv_classifier, \n",
    "                                   clean_val_loader, \n",
    "                                   criterion=nn.CrossEntropyLoss(),\n",
    "                                   tensor_dir=None, \n",
    "                                   adversarial_attack=None, \n",
    "                                   n=4, \n",
    "                                   avgpool_patchtokens=False, \n",
    "                                   path_predictions=Path(log_dir, 'eval_c_'+attack+'_d_'+applied_attack+'.csv'),\n",
    "                                   log_interval = 10)\n",
    "        clean_logger_dict[attack] = logger\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of clean classifier on adversarial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### evaluating clean classifier on pgd_0001 data#########################\n",
      "saving predictions to: /cluster/scratch/thobauma/data/models/pgd_0001_ensemble/eval_clean_classifier_pgd_0001.csv\n",
      "Test:  [ 0/57]  eta: 0:00:47  loss: 0.058591 (0.058591)  acc1: 98.437500 (98.437500)  acc5: 100.000000 (100.000000)  time: 0.837618  data: 0.697621  max mem: 610\n",
      "Test:  [10/57]  eta: 0:00:38  loss: 0.118617 (0.134464)  acc1: 96.875000 (96.022727)  acc5: 100.000000 (99.715909)  time: 0.822920  data: 0.702236  max mem: 610\n",
      "Test:  [20/57]  eta: 0:00:30  loss: 0.198215 (0.300271)  acc1: 93.750000 (90.848214)  acc5: 100.000000 (99.627976)  time: 0.826395  data: 0.706558  max mem: 610\n",
      "Test:  [30/57]  eta: 0:00:22  loss: 0.361949 (0.367304)  acc1: 89.062500 (89.868952)  acc5: 100.000000 (99.596774)  time: 0.835299  data: 0.716723  max mem: 610\n",
      "Test:  [40/57]  eta: 0:00:14  loss: 0.320487 (0.375101)  acc1: 89.062500 (89.710366)  acc5: 100.000000 (99.657012)  time: 0.830645  data: 0.714230  max mem: 610\n",
      "Test:  [50/57]  eta: 0:00:05  loss: 0.293827 (0.360262)  acc1: 90.625000 (90.042892)  acc5: 100.000000 (99.724265)  time: 0.827243  data: 0.710727  max mem: 610\n",
      "Test:  [56/57]  eta: 0:00:00  loss: 0.293827 (0.417828)  acc1: 90.625000 (89.166667)  acc5: 100.000000 (99.666667)  time: 0.803181  data: 0.691002  max mem: 610\n",
      "Test: Total time: 0:00:46 (0.820251 s / it)\n",
      "* Acc@1 89.167 Acc@5 99.667 loss 0.418\n",
      "\n",
      "\n",
      "######################### evaluating clean classifier on pgd_003 data#########################\n",
      "saving predictions to: /cluster/scratch/thobauma/data/models/pgd_003_ensemble/eval_clean_classifier_pgd_003.csv\n",
      "Test:  [ 0/57]  eta: 0:00:52  loss: 30.459637 (30.459637)  acc1: 0.000000 (0.000000)  acc5: 0.000000 (0.000000)  time: 0.913978  data: 0.774008  max mem: 610\n",
      "Test:  [10/57]  eta: 0:00:39  loss: 31.217615 (31.499899)  acc1: 0.000000 (0.000000)  acc5: 0.000000 (0.426136)  time: 0.841249  data: 0.716251  max mem: 610\n",
      "Test:  [20/57]  eta: 0:00:31  loss: 30.547186 (29.966507)  acc1: 0.000000 (0.000000)  acc5: 0.000000 (3.422619)  time: 0.839142  data: 0.719174  max mem: 610\n",
      "Test:  [30/57]  eta: 0:00:22  loss: 28.715300 (29.403801)  acc1: 0.000000 (0.000000)  acc5: 1.562500 (6.350806)  time: 0.826102  data: 0.709544  max mem: 610\n",
      "Test:  [40/57]  eta: 0:00:14  loss: 27.429501 (28.831706)  acc1: 0.000000 (0.000000)  acc5: 3.125000 (5.945122)  time: 0.841192  data: 0.724512  max mem: 610\n",
      "Test:  [50/57]  eta: 0:00:05  loss: 28.116163 (28.893505)  acc1: 0.000000 (0.000000)  acc5: 3.125000 (5.269608)  time: 0.827376  data: 0.710671  max mem: 610\n",
      "Test:  [56/57]  eta: 0:00:00  loss: 28.232775 (28.810328)  acc1: 0.000000 (0.000000)  acc5: 4.687500 (5.500000)  time: 0.804222  data: 0.689711  max mem: 610\n",
      "Test: Total time: 0:00:46 (0.822446 s / it)\n",
      "* Acc@1 0.000 Acc@5 5.500 loss 28.810\n",
      "\n",
      "\n",
      "######################### evaluating clean classifier on pgd_01 data#########################\n",
      "saving predictions to: /cluster/scratch/thobauma/data/models/pgd_01_ensemble/eval_clean_classifier_pgd_01.csv\n",
      "Test:  [ 0/57]  eta: 0:00:39  loss: 30.359194 (30.359194)  acc1: 0.000000 (0.000000)  acc5: 0.000000 (0.000000)  time: 0.688678  data: 0.571739  max mem: 610\n",
      "Test:  [10/57]  eta: 0:00:31  loss: 30.873793 (31.047616)  acc1: 0.000000 (0.000000)  acc5: 0.000000 (0.568182)  time: 0.679584  data: 0.559348  max mem: 610\n",
      "Test:  [20/57]  eta: 0:00:25  loss: 29.381248 (29.337430)  acc1: 0.000000 (0.000000)  acc5: 1.562500 (2.604167)  time: 0.701863  data: 0.583356  max mem: 610\n",
      "Test:  [30/57]  eta: 0:00:19  loss: 27.253870 (28.637923)  acc1: 0.000000 (0.000000)  acc5: 1.562500 (4.737903)  time: 0.772667  data: 0.654959  max mem: 610\n",
      "Test:  [40/57]  eta: 0:00:12  loss: 26.508488 (28.199794)  acc1: 0.000000 (0.076220)  acc5: 1.562500 (4.573171)  time: 0.828436  data: 0.709460  max mem: 610\n",
      "Test:  [50/57]  eta: 0:00:05  loss: 27.342760 (28.211330)  acc1: 0.000000 (0.091912)  acc5: 3.125000 (4.258578)  time: 0.799641  data: 0.678692  max mem: 610\n",
      "Test:  [56/57]  eta: 0:00:00  loss: 28.048836 (28.161420)  acc1: 0.000000 (0.083333)  acc5: 3.125000 (4.194444)  time: 0.755659  data: 0.637843  max mem: 610\n",
      "Test: Total time: 0:00:42 (0.752797 s / it)\n",
      "* Acc@1 0.083 Acc@5 4.194 loss 28.161\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "some_dict = defaultdict(dict)\n",
    "\n",
    "clean_classifier = LinearClassifier(linear_classifier.linear.in_features, num_labels=9)\n",
    "clean_classifier.to(DEVICE)\n",
    "\n",
    "clean_classifier.load_state_dict(torch.load(Path(DATA_PATH,'models','base_lin_clf', 'checkpoint.pth.tar'))[\"state_dict\"])\n",
    "clean_classifier.cuda()\n",
    "\n",
    "for attack in ADV_DATASETS:\n",
    "    pstr = \"#\"*25 + f''' evaluating clean classifier on {attack} data''' + \"#\"*25\n",
    "    print(pstr)\n",
    "\n",
    "    # load from checkpoint\n",
    "    log_dir = Path(MODELS_PATH, attack + \"_ensemble\")\n",
    "    to_restore={'epoch': 1}\n",
    "\n",
    "    info, logger = validate_network(model, \n",
    "                                   clean_classifier, \n",
    "                                   loader_dict[attack][\"validation\"], \n",
    "                                   criterion=nn.CrossEntropyLoss(),\n",
    "                                   tensor_dir=None, \n",
    "                                   adversarial_attack=None, \n",
    "                                   n=4, \n",
    "                                   avgpool_patchtokens=False, \n",
    "                                   path_predictions=Path(log_dir, 'eval_clean_classifier_'+attack+'.csv'),\n",
    "                                   log_interval = 10)\n",
    "                                    \n",
    "    some_dict[attack] = logger\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean classifier and clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving predictions to: /cluster/scratch/thobauma/data/models/pgd_01_ensemble/eval_clean_classifier_clean_data.csv\n",
      "Test:  [ 0/57]  eta: 0:01:21  loss: 0.001938 (0.001938)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 1.422227  data: 1.282189  max mem: 610\n",
      "Test:  [10/57]  eta: 0:01:07  loss: 0.003794 (0.016414)  acc1: 100.000000 (99.573864)  acc5: 100.000000 (100.000000)  time: 1.438976  data: 1.312541  max mem: 610\n",
      "Test:  [20/57]  eta: 0:00:52  loss: 0.020681 (0.048217)  acc1: 98.437500 (98.214286)  acc5: 100.000000 (100.000000)  time: 1.420146  data: 1.293535  max mem: 610\n",
      "Test:  [30/57]  eta: 0:00:37  loss: 0.044338 (0.064708)  acc1: 96.875000 (97.631048)  acc5: 100.000000 (100.000000)  time: 1.386508  data: 1.260545  max mem: 610\n",
      "Test:  [40/57]  eta: 0:00:23  loss: 0.055858 (0.068185)  acc1: 98.437500 (97.599085)  acc5: 100.000000 (100.000000)  time: 1.397151  data: 1.276247  max mem: 610\n",
      "Test:  [50/57]  eta: 0:00:09  loss: 0.059332 (0.066406)  acc1: 98.437500 (97.763480)  acc5: 100.000000 (100.000000)  time: 1.397004  data: 1.279995  max mem: 610\n",
      "Test:  [56/57]  eta: 0:00:01  loss: 0.056248 (0.074538)  acc1: 98.437500 (97.527778)  acc5: 100.000000 (99.972222)  time: 1.378554  data: 1.263319  max mem: 610\n",
      "Test: Total time: 0:01:19 (1.396430 s / it)\n",
      "* Acc@1 97.528 Acc@5 99.972 loss 0.075\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "what_dict = defaultdict(dict)\n",
    "\n",
    "clean_classifier = LinearClassifier(linear_classifier.linear.in_features, num_labels=9)\n",
    "clean_classifier.to(DEVICE)\n",
    "\n",
    "clean_classifier.load_state_dict(torch.load(Path(DATA_PATH,'models','base_lin_clf', 'checkpoint.pth.tar'))[\"state_dict\"])\n",
    "clean_classifier.cuda()\n",
    "\n",
    "\n",
    "pstr = \"#\"*25 + f''' evaluating clean classifier on clean data''' + \"#\"*25\n",
    "\n",
    "# load from checkpoint\n",
    "log_dir = Path(MODELS_PATH, attack + \"_ensemble\")\n",
    "to_restore={'epoch': 1}\n",
    "\n",
    "info, logger = validate_network(model, \n",
    "                               clean_classifier, \n",
    "                               clean_val_loader, \n",
    "                               criterion=nn.CrossEntropyLoss(),\n",
    "                               tensor_dir=None, \n",
    "                               adversarial_attack=None, \n",
    "                               n=4, \n",
    "                               avgpool_patchtokens=False, \n",
    "                               path_predictions=Path(log_dir, 'eval_clean_classifier_clean_data.csv'),\n",
    "                               log_interval = 10)\n",
    "                                    \n",
    "what_dict[attack] = logger\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on full pipeline with post-hoc as multiplexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean_classifier\n",
    "clean_classifier = LinearClassifier(linear_classifier.linear.in_features, num_labels=9)\n",
    "clean_classifier.to(DEVICE)\n",
    "\n",
    "clean_classifier.load_state_dict(torch.load(Path(DATA_PATH,'models','base_lin_clf', 'checkpoint.pth.tar'))[\"state_dict\"])\n",
    "clean_classifier.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load posthoc\n",
    "# Perform validation on clean dataset\n",
    "log_dir = Path(MODELS_PATH, \"ensemble\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for post_model in ADV_DATASETS:\n",
    "    posthoc = LinearBC(1536)\n",
    "    posthoc.cuda()\n",
    "    to_restore={'epoch':3}\n",
    "    utils.restart_from_checkpoint(Path(POST_HOC_PATH, \"checkpoint.pth.tar\"),\n",
    "                                  run_variables=to_restore,\n",
    "                                  state_dict=posthoc)\n",
    "    \n",
    "    for adv_model in attacks:\n",
    "        adv_classifier = LinearClassifier(linear_classifier.linear.in_features, num_labels=9)\n",
    "        adv_classifier.to(DEVICE)\n",
    "        \n",
    "        to_restore={'epoch': 1}\n",
    "        \n",
    "        utils.restart_from_checkpoint(Path(ADV_CLASSIFIER, \"checkpoint.pth.tar\"),\n",
    "                                      run_variables=to_restore,\n",
    "                                      state_dict=adv_classifier)\n",
    "        \n",
    "        for attack, loaders in loader_dict.items():\n",
    "            \n",
    "            pstr = \"#\"*25 + f''' Validating Posthoc: {post_model} and adv_classifier: {adv_model} on {attack} ''' + \"#\"*25\n",
    "            print(pstr)\n",
    "            \n",
    "            log_dict, logger = validate_multihead_network(model, \n",
    "                                                          posthoc,\n",
    "                                                          adv_classifier,\n",
    "                                                          clean_classifier,\n",
    "                                                          loader_dict[attack][\"validation\"], \n",
    "                                                          tensor_dir=None, \n",
    "                                                          adversarial_attack=None, \n",
    "                                                          n=4, \n",
    "                                                          avgpool=False,\n",
    "                                                          path_predictions=Path(log_dir, 'ensemble_p_'+ post_model +'_c_'+adv_model+'_d_'+attack+'.csv'))\n",
    "            \n",
    "            # Save adversarial Classifier\n",
    "            save_file_log = f\"log_p_{post_model}_c_{adv_model}_d_{attack}.pt\"\n",
    "            torch.save(logger, Path(log_dir, save_file_log))"
   ]
  }
 ],
 "metadata": {
  "datasets": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
