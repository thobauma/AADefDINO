{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "# from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino, ViTWrapper\n",
    "from src.model.data import *\n",
    "from src.model.train import *\n",
    "from src.model.multihead_model import *\n",
    "\n",
    "from torchattacks import *\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "MAX_PATH = Path('/','cluster', 'scratch', 'mmathys', 'dl_data')\n",
    "\n",
    "LOG_BASE_PATH = Path(MAX_PATH, 'logs')\n",
    "\n",
    "# DamageNet\n",
    "DN_PATH = Path(DATA_PATH, 'damageNet')\n",
    "DN_LABEL_PATH = Path(DN_PATH, 'val_damagenet.txt')\n",
    "DN_IMAGES_PATH = Path(DN_PATH, 'images')\n",
    "\n",
    "# Image Net\n",
    "ORI_PATH = Path(DATA_PATH, 'ori_data')\n",
    "CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "\n",
    "VAL_PATH = Path(ORI_PATH, 'validation')\n",
    "VAL_IMAGES_PATH = Path(VAL_PATH,'images')\n",
    "VAL_LABEL_PATH = Path(VAL_PATH, 'correct_labels.txt')\n",
    "\n",
    "TRAIN_PATH = Path(ORI_PATH, 'train')\n",
    "TRAIN_IMAGES_PATH = Path(TRAIN_PATH,'images')\n",
    "TRAIN_LABEL_PATH = Path(TRAIN_PATH, 'correct_labels.txt')\n",
    "\n",
    "# Adversarial Data\n",
    "# PGD\n",
    "PGD_TRAIN_PATH = Path(MAX_PATH, 'adversarial_data', 'pgd_06', 'train')\n",
    "PGD_TRAIN_IMAGES_PATH = Path(PGD_TRAIN_PATH,'images')\n",
    "PGD_TRAIN_LABEL_PATH = Path(PGD_TRAIN_PATH, 'labels.txt')\n",
    "\n",
    "PGD_VAL_PATH = Path(MAX_PATH, 'adversarial_data', 'pgd_06', 'validation')\n",
    "PGD_VAL_IMAGES_PATH = Path(PGD_VAL_PATH,'images')\n",
    "PGD_VAL_LABEL_PATH = Path(PGD_VAL_PATH, 'labels.txt')\n",
    "\n",
    "# CW\n",
    "CW_TRAIN_PATH = Path(MAX_PATH, 'adversarial_data', 'cw', 'train')\n",
    "CW_TRAIN_IMAGES_PATH = Path(CW_TRAIN_PATH,'images')\n",
    "CW_TRAIN_LABEL_PATH = Path(CW_TRAIN_PATH, 'labels.txt')\n",
    "\n",
    "CW_VAL_PATH = Path(MAX_PATH, 'adversarial_data', 'cw', 'validation')\n",
    "CW_VAL_IMAGES_PATH = Path(CW_VAL_PATH,'images')\n",
    "CW_VAL_LABEL_PATH = Path(CW_VAL_PATH, 'labels.txt')\n",
    "\n",
    "# FGSM\n",
    "FGSM_TRAIN_PATH = Path(MAX_PATH, 'adversarial_data', 'fgsm_06', 'train')\n",
    "FGSM_TRAIN_IMAGES_PATH = Path(FGSM_TRAIN_PATH,'images')\n",
    "FGSM_TRAIN_LABEL_PATH = Path(FGSM_TRAIN_PATH, 'labels.txt')\n",
    "\n",
    "FGSM_VAL_PATH = Path(MAX_PATH, 'adversarial_data', 'fgsm_06', 'validation')\n",
    "FGSM_VAL_IMAGES_PATH = Path(FGSM_VAL_PATH,'images')\n",
    "FGSM_VAL_LABEL_PATH = Path(FGSM_VAL_PATH, 'labels.txt')\n",
    "\n",
    "\n",
    "# TB LOG\n",
    "TB_LOGS_BASE_PATH = Path(LOG_BASE_PATH, 'tb_logs')\n",
    "\n",
    "\n",
    "# Model save path\n",
    "ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH = Path(MAX_PATH, 'adversarial_data', 'adv_classifiers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If CLASS_SUBSET is specified, INDEX_SUBSET will be ignored. Set CLASS_SUBSET=None if you want to use indexes.\n",
    "# INDEX_SUBSET = get_random_indexes(number_of_images = 50000, n_samples=1000)\n",
    "# CLASS_SUBSET = get_random_classes(number_of_classes = 25, min_rand_class = 1, max_rand_class = 1001)\n",
    "\n",
    "\n",
    "CLASS_SUBSET = np.load(CLASS_SUBSET_PATH)\n",
    "\n",
    "# CLASS_SUBSET = CLASS_SUBSET[:25] \n",
    "\n",
    "NUM_WORKERS= 0\n",
    "PIN_MEMORY=True\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import DINO\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights.\n"
     ]
    }
   ],
   "source": [
    "model, linear_classifier = get_dino()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to set the correct transformation\n",
    "# encoder\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit([i for i in CLASS_SUBSET])\n",
    "\n",
    "\n",
    "# PGD\n",
    "pgd_train_dataset = AdvTrainingImageDataset(PGD_TRAIN_IMAGES_PATH, \n",
    "                                            PGD_TRAIN_LABEL_PATH, \n",
    "                                            ORIGINAL_TRANSFORM, \n",
    "                                            CLASS_SUBSET, \n",
    "                                            index_subset=None,\n",
    "                                            label_encoder=label_encoder\n",
    "                                           )\n",
    "\n",
    "pgd_val_dataset = AdvTrainingImageDataset(PGD_VAL_IMAGES_PATH, \n",
    "                                          PGD_VAL_LABEL_PATH, \n",
    "                                          ORIGINAL_TRANSFORM, \n",
    "                                          CLASS_SUBSET, \n",
    "                                          index_subset=None,\n",
    "                                          label_encoder=label_encoder\n",
    "                                         )\n",
    "\n",
    "\n",
    "pgd_train_loader = DataLoader(pgd_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "pgd_val_loader = DataLoader(pgd_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY, \n",
    "                            shuffle=False)\n",
    "\n",
    "# CW\n",
    "cw_train_dataset = AdvTrainingImageDataset(CW_TRAIN_IMAGES_PATH, \n",
    "                                            CW_TRAIN_LABEL_PATH, \n",
    "                                            ORIGINAL_TRANSFORM, \n",
    "                                            CLASS_SUBSET, \n",
    "                                            index_subset=None,\n",
    "                                           label_encoder=label_encoder\n",
    "                                          )\n",
    "\n",
    "cw_val_dataset = AdvTrainingImageDataset(CW_VAL_IMAGES_PATH, \n",
    "                                          CW_VAL_LABEL_PATH, \n",
    "                                          ORIGINAL_TRANSFORM, \n",
    "                                          CLASS_SUBSET, \n",
    "                                          index_subset=None,\n",
    "                                         label_encoder=label_encoder\n",
    "                                        )\n",
    "\n",
    "cw_train_loader = DataLoader(cw_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "cw_val_loader = DataLoader(cw_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY, \n",
    "                            shuffle=False)\n",
    "\n",
    "# FGSM\n",
    "fgsm_train_dataset = AdvTrainingImageDataset(FGSM_TRAIN_IMAGES_PATH, \n",
    "                                            FGSM_TRAIN_LABEL_PATH, \n",
    "                                            ORIGINAL_TRANSFORM, \n",
    "                                            CLASS_SUBSET, \n",
    "                                            index_subset=None,\n",
    "                                            label_encoder=label_encoder\n",
    "                                            )\n",
    "\n",
    "fgsm_val_dataset = AdvTrainingImageDataset(FGSM_VAL_IMAGES_PATH, \n",
    "                                          FGSM_VAL_LABEL_PATH, \n",
    "                                          ORIGINAL_TRANSFORM, \n",
    "                                          CLASS_SUBSET, \n",
    "                                          index_subset=None,\n",
    "                                          label_encoder=label_encoder\n",
    "                                          )\n",
    "\n",
    "fgsm_train_loader = DataLoader(fgsm_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "fgsm_val_loader = DataLoader(fgsm_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY, \n",
    "                            shuffle=False)\n",
    "\n",
    "# Clean\n",
    "clean_train_dataset = ImageDataset(TRAIN_IMAGES_PATH, \n",
    "                                  TRAIN_LABEL_PATH, \n",
    "                                  ORIGINAL_TRANSFORM,\n",
    "                                  CLASS_SUBSET, \n",
    "                                  index_subset=None, \n",
    "                                  label_encoder=label_encoder)\n",
    "\n",
    "clean_val_dataset = ImageDataset(VAL_IMAGES_PATH, \n",
    "                                  VAL_LABEL_PATH, \n",
    "                                  ORIGINAL_TRANSFORM,\n",
    "                                  CLASS_SUBSET, \n",
    "                                  index_subset=None, \n",
    "                                  label_encoder=label_encoder)\n",
    "\n",
    "clean_train_loader = DataLoader(clean_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "clean_val_loader = DataLoader(clean_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY,\n",
    "                            shuffle=False)\n",
    "\n",
    "\n",
    "loader_dict = {\n",
    "    \"pgd\" : {\n",
    "        \"train\" : pgd_train_loader,\n",
    "        \"validation\" : pgd_val_loader,\n",
    "    },\n",
    "    \"cw\" : {\n",
    "        \"train\" : cw_train_loader,\n",
    "        \"validation\" : cw_val_loader,\n",
    "    }, \n",
    "    \"fgsm\" : {\n",
    "        \"train\" : fgsm_train_loader,\n",
    "        \"validation\" : fgsm_val_loader,\n",
    "    },\n",
    "    \"clean\" : {\n",
    "        \"train\" : clean_train_loader,\n",
    "        \"validation\" : clean_val_loader,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2b05dece0910>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_dict[\"pgd\"][\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '25_classes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(dim, num_labels)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # linear layer\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Binary Classifier\n",
    "class LinearBC(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(LinearBC,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train various classifiers on all adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier for clean\n",
      "Epoch: [0]  [   0/2012]  eta: 0:07:42  lr: 0.000063  loss: 4.070327 (4.070327)  time: 0.229926  data: 0.190067  max mem: 210\n",
      "Epoch: [0]  [  20/2012]  eta: 0:07:45  lr: 0.000063  loss: 3.364912 (3.396890)  time: 0.233918  data: 0.195904  max mem: 211\n",
      "Epoch: [0]  [  40/2012]  eta: 0:07:25  lr: 0.000063  loss: 1.061000 (2.291339)  time: 0.217225  data: 0.179242  max mem: 211\n",
      "Epoch: [0]  [  60/2012]  eta: 0:07:07  lr: 0.000063  loss: 0.637545 (1.764758)  time: 0.206038  data: 0.168053  max mem: 211\n",
      "Epoch: [0]  [  80/2012]  eta: 0:06:56  lr: 0.000063  loss: 0.350218 (1.438930)  time: 0.204567  data: 0.166585  max mem: 211\n",
      "Epoch: [0]  [ 100/2012]  eta: 0:06:53  lr: 0.000063  loss: 0.228549 (1.213146)  time: 0.219306  data: 0.181325  max mem: 211\n",
      "Epoch: [0]  [ 120/2012]  eta: 0:06:44  lr: 0.000063  loss: 0.336601 (1.068877)  time: 0.201224  data: 0.163227  max mem: 211\n",
      "Epoch: [0]  [ 140/2012]  eta: 0:06:38  lr: 0.000063  loss: 0.299253 (0.958927)  time: 0.205310  data: 0.167319  max mem: 211\n",
      "Epoch: [0]  [ 160/2012]  eta: 0:06:32  lr: 0.000063  loss: 0.195812 (0.869668)  time: 0.206228  data: 0.168224  max mem: 211\n",
      "Epoch: [0]  [ 180/2012]  eta: 0:06:26  lr: 0.000063  loss: 0.226854 (0.799860)  time: 0.202910  data: 0.164928  max mem: 211\n",
      "Epoch: [0]  [ 200/2012]  eta: 0:06:22  lr: 0.000063  loss: 0.139051 (0.740949)  time: 0.214776  data: 0.176781  max mem: 211\n",
      "Epoch: [0]  [ 220/2012]  eta: 0:06:17  lr: 0.000063  loss: 0.162165 (0.691239)  time: 0.206873  data: 0.168886  max mem: 211\n",
      "Epoch: [0]  [ 240/2012]  eta: 0:06:14  lr: 0.000063  loss: 0.167063 (0.649869)  time: 0.214100  data: 0.176095  max mem: 211\n",
      "Epoch: [0]  [ 260/2012]  eta: 0:06:12  lr: 0.000063  loss: 0.118953 (0.612220)  time: 0.230956  data: 0.192966  max mem: 211\n",
      "Epoch: [0]  [ 280/2012]  eta: 0:06:09  lr: 0.000063  loss: 0.127655 (0.582747)  time: 0.222709  data: 0.184724  max mem: 211\n",
      "Epoch: [0]  [ 300/2012]  eta: 0:06:04  lr: 0.000063  loss: 0.129688 (0.556522)  time: 0.204795  data: 0.166793  max mem: 211\n",
      "Epoch: [0]  [ 320/2012]  eta: 0:06:00  lr: 0.000063  loss: 0.135158 (0.532325)  time: 0.216414  data: 0.178387  max mem: 211\n",
      "Epoch: [0]  [ 340/2012]  eta: 0:05:55  lr: 0.000063  loss: 0.118780 (0.510678)  time: 0.207661  data: 0.169679  max mem: 211\n",
      "Epoch: [0]  [ 360/2012]  eta: 0:05:52  lr: 0.000063  loss: 0.091949 (0.490932)  time: 0.224432  data: 0.186434  max mem: 211\n",
      "Epoch: [0]  [ 380/2012]  eta: 0:05:47  lr: 0.000063  loss: 0.078607 (0.471724)  time: 0.208532  data: 0.170555  max mem: 211\n",
      "Epoch: [0]  [ 400/2012]  eta: 0:05:44  lr: 0.000063  loss: 0.108062 (0.456755)  time: 0.220157  data: 0.182157  max mem: 211\n",
      "Epoch: [0]  [ 420/2012]  eta: 0:05:39  lr: 0.000063  loss: 0.067917 (0.439278)  time: 0.215441  data: 0.177434  max mem: 211\n",
      "Epoch: [0]  [ 440/2012]  eta: 0:05:35  lr: 0.000063  loss: 0.099343 (0.426036)  time: 0.204762  data: 0.166798  max mem: 211\n",
      "Epoch: [0]  [ 460/2012]  eta: 0:05:32  lr: 0.000063  loss: 0.078168 (0.412705)  time: 0.232913  data: 0.194899  max mem: 211\n",
      "Epoch: [0]  [ 480/2012]  eta: 0:05:27  lr: 0.000063  loss: 0.150450 (0.402988)  time: 0.207671  data: 0.169696  max mem: 211\n",
      "Epoch: [0]  [ 500/2012]  eta: 0:05:22  lr: 0.000063  loss: 0.151450 (0.395237)  time: 0.210866  data: 0.172888  max mem: 211\n",
      "Epoch: [0]  [ 520/2012]  eta: 0:05:18  lr: 0.000063  loss: 0.128976 (0.385937)  time: 0.206888  data: 0.168919  max mem: 211\n",
      "Epoch: [0]  [ 540/2012]  eta: 0:05:14  lr: 0.000063  loss: 0.124808 (0.377958)  time: 0.214089  data: 0.176113  max mem: 211\n",
      "Epoch: [0]  [ 560/2012]  eta: 0:05:09  lr: 0.000063  loss: 0.138064 (0.369559)  time: 0.209944  data: 0.171979  max mem: 211\n",
      "Epoch: [0]  [ 580/2012]  eta: 0:05:05  lr: 0.000063  loss: 0.132235 (0.362447)  time: 0.214133  data: 0.176139  max mem: 211\n",
      "Epoch: [0]  [ 600/2012]  eta: 0:05:00  lr: 0.000063  loss: 0.093887 (0.353493)  time: 0.200961  data: 0.162980  max mem: 211\n",
      "Epoch: [0]  [ 620/2012]  eta: 0:04:56  lr: 0.000063  loss: 0.043076 (0.346282)  time: 0.219013  data: 0.181041  max mem: 211\n",
      "Epoch: [0]  [ 640/2012]  eta: 0:04:51  lr: 0.000063  loss: 0.073704 (0.339323)  time: 0.202091  data: 0.164110  max mem: 211\n",
      "Epoch: [0]  [ 660/2012]  eta: 0:04:47  lr: 0.000063  loss: 0.040423 (0.331416)  time: 0.201593  data: 0.163577  max mem: 211\n",
      "Epoch: [0]  [ 680/2012]  eta: 0:04:42  lr: 0.000063  loss: 0.101072 (0.326596)  time: 0.201799  data: 0.163805  max mem: 211\n",
      "Epoch: [0]  [ 700/2012]  eta: 0:04:37  lr: 0.000063  loss: 0.131953 (0.322795)  time: 0.199083  data: 0.161112  max mem: 211\n",
      "Epoch: [0]  [ 720/2012]  eta: 0:04:33  lr: 0.000063  loss: 0.058156 (0.317496)  time: 0.203884  data: 0.165875  max mem: 211\n",
      "Epoch: [0]  [ 740/2012]  eta: 0:04:28  lr: 0.000063  loss: 0.060551 (0.311783)  time: 0.208844  data: 0.170831  max mem: 211\n",
      "Epoch: [0]  [ 760/2012]  eta: 0:04:24  lr: 0.000063  loss: 0.057030 (0.305628)  time: 0.211424  data: 0.173440  max mem: 211\n",
      "Epoch: [0]  [ 780/2012]  eta: 0:04:20  lr: 0.000063  loss: 0.075977 (0.300429)  time: 0.200342  data: 0.162357  max mem: 211\n",
      "Epoch: [0]  [ 800/2012]  eta: 0:04:15  lr: 0.000063  loss: 0.060701 (0.296487)  time: 0.199489  data: 0.161500  max mem: 211\n",
      "Epoch: [0]  [ 820/2012]  eta: 0:04:11  lr: 0.000063  loss: 0.082563 (0.292057)  time: 0.203666  data: 0.165680  max mem: 211\n",
      "Epoch: [0]  [ 840/2012]  eta: 0:04:07  lr: 0.000063  loss: 0.075978 (0.287932)  time: 0.215629  data: 0.177627  max mem: 211\n",
      "Epoch: [0]  [ 860/2012]  eta: 0:04:02  lr: 0.000063  loss: 0.076062 (0.284201)  time: 0.211518  data: 0.173535  max mem: 211\n",
      "Epoch: [0]  [ 880/2012]  eta: 0:03:58  lr: 0.000063  loss: 0.064339 (0.280630)  time: 0.202424  data: 0.164406  max mem: 211\n",
      "Epoch: [0]  [ 900/2012]  eta: 0:03:54  lr: 0.000063  loss: 0.072014 (0.276993)  time: 0.210984  data: 0.172980  max mem: 211\n",
      "Epoch: [0]  [ 920/2012]  eta: 0:03:49  lr: 0.000063  loss: 0.172770 (0.274769)  time: 0.208230  data: 0.170252  max mem: 211\n",
      "Epoch: [0]  [ 940/2012]  eta: 0:03:45  lr: 0.000063  loss: 0.036555 (0.270481)  time: 0.217014  data: 0.179012  max mem: 211\n",
      "Epoch: [0]  [ 960/2012]  eta: 0:03:41  lr: 0.000063  loss: 0.090589 (0.267478)  time: 0.208742  data: 0.170716  max mem: 211\n",
      "Epoch: [0]  [ 980/2012]  eta: 0:03:37  lr: 0.000063  loss: 0.060420 (0.264369)  time: 0.205102  data: 0.167119  max mem: 211\n",
      "Epoch: [0]  [1000/2012]  eta: 0:03:33  lr: 0.000063  loss: 0.058771 (0.261550)  time: 0.221639  data: 0.183654  max mem: 211\n",
      "Epoch: [0]  [1020/2012]  eta: 0:03:28  lr: 0.000063  loss: 0.074138 (0.259679)  time: 0.201045  data: 0.163070  max mem: 211\n",
      "Epoch: [0]  [1040/2012]  eta: 0:03:24  lr: 0.000063  loss: 0.057566 (0.256100)  time: 0.206973  data: 0.168978  max mem: 211\n",
      "Epoch: [0]  [1060/2012]  eta: 0:03:20  lr: 0.000063  loss: 0.055645 (0.253725)  time: 0.209791  data: 0.171794  max mem: 211\n",
      "Epoch: [0]  [1080/2012]  eta: 0:03:16  lr: 0.000063  loss: 0.018430 (0.250309)  time: 0.205425  data: 0.167449  max mem: 211\n",
      "Epoch: [0]  [1100/2012]  eta: 0:03:11  lr: 0.000063  loss: 0.075526 (0.247530)  time: 0.204919  data: 0.166935  max mem: 211\n",
      "Epoch: [0]  [1120/2012]  eta: 0:03:07  lr: 0.000063  loss: 0.030650 (0.245465)  time: 0.204751  data: 0.166752  max mem: 211\n",
      "Epoch: [0]  [1140/2012]  eta: 0:03:03  lr: 0.000063  loss: 0.116762 (0.243616)  time: 0.212199  data: 0.174200  max mem: 211\n",
      "Epoch: [0]  [1160/2012]  eta: 0:02:59  lr: 0.000063  loss: 0.067476 (0.241258)  time: 0.218021  data: 0.180032  max mem: 211\n",
      "Epoch: [0]  [1180/2012]  eta: 0:02:55  lr: 0.000063  loss: 0.052283 (0.238235)  time: 0.224370  data: 0.186397  max mem: 211\n",
      "Epoch: [0]  [1200/2012]  eta: 0:02:51  lr: 0.000063  loss: 0.107709 (0.236862)  time: 0.222339  data: 0.184377  max mem: 211\n",
      "Epoch: [0]  [1220/2012]  eta: 0:02:46  lr: 0.000063  loss: 0.041326 (0.234473)  time: 0.211930  data: 0.173937  max mem: 211\n",
      "Epoch: [0]  [1240/2012]  eta: 0:02:42  lr: 0.000063  loss: 0.068608 (0.232453)  time: 0.208587  data: 0.170573  max mem: 211\n",
      "Epoch: [0]  [1260/2012]  eta: 0:02:38  lr: 0.000063  loss: 0.045408 (0.230081)  time: 0.204971  data: 0.166988  max mem: 211\n",
      "Epoch: [0]  [1280/2012]  eta: 0:02:34  lr: 0.000063  loss: 0.031522 (0.227483)  time: 0.208369  data: 0.170387  max mem: 211\n",
      "Epoch: [0]  [1300/2012]  eta: 0:02:29  lr: 0.000063  loss: 0.025382 (0.225233)  time: 0.204353  data: 0.166373  max mem: 211\n",
      "Epoch: [0]  [1320/2012]  eta: 0:02:25  lr: 0.000063  loss: 0.062261 (0.223155)  time: 0.223858  data: 0.185891  max mem: 211\n",
      "Epoch: [0]  [1340/2012]  eta: 0:02:21  lr: 0.000063  loss: 0.042727 (0.221593)  time: 0.212542  data: 0.174535  max mem: 211\n",
      "Epoch: [0]  [1360/2012]  eta: 0:02:17  lr: 0.000063  loss: 0.036333 (0.219559)  time: 0.208815  data: 0.170783  max mem: 211\n",
      "Epoch: [0]  [1380/2012]  eta: 0:02:13  lr: 0.000063  loss: 0.017438 (0.217726)  time: 0.208395  data: 0.170415  max mem: 211\n",
      "Epoch: [0]  [1400/2012]  eta: 0:02:08  lr: 0.000063  loss: 0.029547 (0.215647)  time: 0.206823  data: 0.168830  max mem: 211\n",
      "Epoch: [0]  [1420/2012]  eta: 0:02:04  lr: 0.000063  loss: 0.071605 (0.215306)  time: 0.201158  data: 0.163180  max mem: 211\n",
      "Epoch: [0]  [1440/2012]  eta: 0:02:00  lr: 0.000063  loss: 0.039023 (0.213518)  time: 0.214533  data: 0.176536  max mem: 211\n",
      "Epoch: [0]  [1460/2012]  eta: 0:01:56  lr: 0.000063  loss: 0.035443 (0.211508)  time: 0.212658  data: 0.174685  max mem: 211\n",
      "Epoch: [0]  [1480/2012]  eta: 0:01:51  lr: 0.000063  loss: 0.044855 (0.209818)  time: 0.202099  data: 0.164127  max mem: 211\n",
      "Epoch: [0]  [1500/2012]  eta: 0:01:47  lr: 0.000063  loss: 0.023196 (0.207831)  time: 0.207501  data: 0.169540  max mem: 211\n",
      "Epoch: [0]  [1520/2012]  eta: 0:01:43  lr: 0.000063  loss: 0.086333 (0.206462)  time: 0.216828  data: 0.178832  max mem: 211\n",
      "Epoch: [0]  [1540/2012]  eta: 0:01:39  lr: 0.000063  loss: 0.039359 (0.205092)  time: 0.219542  data: 0.181571  max mem: 211\n",
      "Epoch: [0]  [1560/2012]  eta: 0:01:35  lr: 0.000063  loss: 0.050359 (0.204012)  time: 0.213309  data: 0.175323  max mem: 211\n",
      "Epoch: [0]  [1580/2012]  eta: 0:01:31  lr: 0.000063  loss: 0.053013 (0.203337)  time: 0.208908  data: 0.170905  max mem: 211\n",
      "Epoch: [0]  [1600/2012]  eta: 0:01:26  lr: 0.000063  loss: 0.053669 (0.201865)  time: 0.212835  data: 0.174859  max mem: 211\n",
      "Epoch: [0]  [1620/2012]  eta: 0:01:22  lr: 0.000063  loss: 0.027033 (0.200374)  time: 0.207738  data: 0.169783  max mem: 211\n",
      "Epoch: [0]  [1640/2012]  eta: 0:01:18  lr: 0.000063  loss: 0.060292 (0.199047)  time: 0.208363  data: 0.170370  max mem: 211\n",
      "Epoch: [0]  [1660/2012]  eta: 0:01:14  lr: 0.000063  loss: 0.028552 (0.197597)  time: 0.200062  data: 0.162070  max mem: 211\n",
      "Epoch: [0]  [1680/2012]  eta: 0:01:09  lr: 0.000063  loss: 0.041600 (0.196038)  time: 0.206771  data: 0.168798  max mem: 211\n",
      "Epoch: [0]  [1700/2012]  eta: 0:01:05  lr: 0.000063  loss: 0.067806 (0.195113)  time: 0.202001  data: 0.164004  max mem: 211\n",
      "Epoch: [0]  [1720/2012]  eta: 0:01:01  lr: 0.000063  loss: 0.036446 (0.193964)  time: 0.205462  data: 0.167477  max mem: 211\n",
      "Epoch: [0]  [1740/2012]  eta: 0:00:57  lr: 0.000063  loss: 0.047609 (0.193001)  time: 0.227512  data: 0.189468  max mem: 211\n",
      "Epoch: [0]  [1760/2012]  eta: 0:00:53  lr: 0.000063  loss: 0.037554 (0.191710)  time: 0.203686  data: 0.165702  max mem: 211\n",
      "Epoch: [0]  [1780/2012]  eta: 0:00:48  lr: 0.000063  loss: 0.040006 (0.191031)  time: 0.223994  data: 0.186011  max mem: 211\n",
      "Epoch: [0]  [1800/2012]  eta: 0:00:44  lr: 0.000063  loss: 0.027124 (0.189647)  time: 0.210003  data: 0.172024  max mem: 211\n",
      "Epoch: [0]  [1820/2012]  eta: 0:00:40  lr: 0.000063  loss: 0.074352 (0.188894)  time: 0.211112  data: 0.173123  max mem: 211\n",
      "Epoch: [0]  [1840/2012]  eta: 0:00:36  lr: 0.000063  loss: 0.044202 (0.187937)  time: 0.210400  data: 0.172399  max mem: 211\n",
      "Epoch: [0]  [1860/2012]  eta: 0:00:32  lr: 0.000063  loss: 0.069442 (0.187294)  time: 0.206880  data: 0.168868  max mem: 211\n",
      "Epoch: [0]  [1880/2012]  eta: 0:00:27  lr: 0.000063  loss: 0.064016 (0.186679)  time: 0.207916  data: 0.169926  max mem: 211\n",
      "Epoch: [0]  [1900/2012]  eta: 0:00:23  lr: 0.000063  loss: 0.028192 (0.185277)  time: 0.214003  data: 0.175993  max mem: 211\n",
      "Epoch: [0]  [1920/2012]  eta: 0:00:19  lr: 0.000063  loss: 0.043447 (0.184093)  time: 0.220420  data: 0.182439  max mem: 211\n",
      "Epoch: [0]  [1940/2012]  eta: 0:00:15  lr: 0.000063  loss: 0.027430 (0.182972)  time: 0.209031  data: 0.171050  max mem: 211\n",
      "Epoch: [0]  [1960/2012]  eta: 0:00:10  lr: 0.000063  loss: 0.052088 (0.182344)  time: 0.207515  data: 0.169499  max mem: 211\n",
      "Epoch: [0]  [1980/2012]  eta: 0:00:06  lr: 0.000063  loss: 0.032689 (0.181457)  time: 0.204151  data: 0.166153  max mem: 211\n",
      "Epoch: [0]  [2000/2012]  eta: 0:00:02  lr: 0.000063  loss: 0.028382 (0.180255)  time: 0.211376  data: 0.173394  max mem: 211\n",
      "Epoch: [0]  [2011/2012]  eta: 0:00:00  lr: 0.000063  loss: 0.036717 (0.179932)  time: 0.195687  data: 0.159062  max mem: 211\n",
      "Epoch: [0] Total time: 0:07:03 (0.210488 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 0.036717 (0.179932)\n",
      "Test:  [ 0/79]  eta: 0:00:16  loss: 0.011867 (0.011867)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.213223  data: 0.175199  max mem: 211\n",
      "Test:  [20/79]  eta: 0:00:12  loss: 0.087917 (0.131604)  acc1: 93.750000 (95.535714)  acc5: 100.000000 (99.404762)  time: 0.218805  data: 0.180783  max mem: 211\n",
      "Test:  [40/79]  eta: 0:00:09  loss: 0.054749 (0.114649)  acc1: 100.000000 (96.189024)  acc5: 100.000000 (99.542683)  time: 0.244252  data: 0.206250  max mem: 211\n",
      "Test:  [60/79]  eta: 0:00:04  loss: 0.049547 (0.114329)  acc1: 93.750000 (96.106557)  acc5: 100.000000 (99.487705)  time: 0.232036  data: 0.194031  max mem: 211\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 0.033946 (0.111176)  acc1: 100.000000 (96.560000)  acc5: 100.000000 (99.600000)  time: 0.214036  data: 0.177440  max mem: 211\n",
      "Test: Total time: 0:00:17 (0.227809 s / it)\n",
      "* Acc@1 96.560 Acc@5 99.600 loss 0.111\n",
      "Accuracy at epoch 0 of the network on the 79 test images: 96.6%\n",
      "Accuracy at epoch 0 of the network on the 1250 test images: 96.6%\n",
      "Max accuracy so far: 96.56%\n",
      "Epoch: [1]  [   0/2012]  eta: 0:06:26  lr: 0.000057  loss: 0.013184 (0.013184)  time: 0.192113  data: 0.154144  max mem: 211\n",
      "Epoch: [1]  [  20/2012]  eta: 0:07:08  lr: 0.000057  loss: 0.009659 (0.059948)  time: 0.216295  data: 0.178292  max mem: 211\n",
      "Epoch: [1]  [  40/2012]  eta: 0:06:55  lr: 0.000057  loss: 0.039451 (0.070447)  time: 0.205597  data: 0.167612  max mem: 211\n",
      "Epoch: [1]  [  60/2012]  eta: 0:06:42  lr: 0.000057  loss: 0.029256 (0.078032)  time: 0.197554  data: 0.159540  max mem: 211\n",
      "Epoch: [1]  [  80/2012]  eta: 0:06:32  lr: 0.000057  loss: 0.048567 (0.078348)  time: 0.193275  data: 0.155295  max mem: 211\n",
      "Epoch: [1]  [ 100/2012]  eta: 0:06:32  lr: 0.000057  loss: 0.023017 (0.070677)  time: 0.213637  data: 0.175648  max mem: 211\n",
      "Epoch: [1]  [ 120/2012]  eta: 0:06:24  lr: 0.000057  loss: 0.040755 (0.072298)  time: 0.193868  data: 0.155859  max mem: 211\n",
      "Epoch: [1]  [ 140/2012]  eta: 0:06:20  lr: 0.000057  loss: 0.038074 (0.074607)  time: 0.203352  data: 0.165371  max mem: 211\n",
      "Epoch: [1]  [ 160/2012]  eta: 0:06:15  lr: 0.000057  loss: 0.026286 (0.072197)  time: 0.200334  data: 0.162356  max mem: 211\n",
      "Epoch: [1]  [ 180/2012]  eta: 0:06:10  lr: 0.000057  loss: 0.028567 (0.072401)  time: 0.194630  data: 0.156652  max mem: 211\n",
      "Epoch: [1]  [ 200/2012]  eta: 0:06:06  lr: 0.000057  loss: 0.012255 (0.071899)  time: 0.202425  data: 0.164448  max mem: 211\n",
      "Epoch: [1]  [ 220/2012]  eta: 0:06:02  lr: 0.000057  loss: 0.045772 (0.071819)  time: 0.201690  data: 0.163700  max mem: 211\n",
      "Epoch: [1]  [ 240/2012]  eta: 0:05:58  lr: 0.000057  loss: 0.023476 (0.069349)  time: 0.205075  data: 0.167082  max mem: 211\n",
      "Epoch: [1]  [ 260/2012]  eta: 0:05:57  lr: 0.000057  loss: 0.024837 (0.068076)  time: 0.225479  data: 0.187485  max mem: 211\n",
      "Epoch: [1]  [ 280/2012]  eta: 0:05:54  lr: 0.000057  loss: 0.025420 (0.070511)  time: 0.210997  data: 0.173012  max mem: 211\n",
      "Epoch: [1]  [ 300/2012]  eta: 0:05:49  lr: 0.000057  loss: 0.028691 (0.070412)  time: 0.194279  data: 0.156306  max mem: 211\n",
      "Epoch: [1]  [ 320/2012]  eta: 0:05:45  lr: 0.000057  loss: 0.037847 (0.069789)  time: 0.205591  data: 0.167589  max mem: 211\n",
      "Epoch: [1]  [ 340/2012]  eta: 0:05:40  lr: 0.000057  loss: 0.031099 (0.069499)  time: 0.203227  data: 0.165225  max mem: 211\n",
      "Epoch: [1]  [ 360/2012]  eta: 0:05:37  lr: 0.000057  loss: 0.017286 (0.068962)  time: 0.214079  data: 0.176115  max mem: 211\n",
      "Epoch: [1]  [ 380/2012]  eta: 0:05:33  lr: 0.000057  loss: 0.015364 (0.068528)  time: 0.202335  data: 0.164358  max mem: 211\n",
      "Epoch: [1]  [ 400/2012]  eta: 0:05:29  lr: 0.000057  loss: 0.033889 (0.069036)  time: 0.207030  data: 0.169060  max mem: 211\n",
      "Epoch: [1]  [ 420/2012]  eta: 0:05:25  lr: 0.000057  loss: 0.014927 (0.067426)  time: 0.202495  data: 0.164525  max mem: 211\n",
      "Epoch: [1]  [ 440/2012]  eta: 0:05:21  lr: 0.000057  loss: 0.026858 (0.068064)  time: 0.199850  data: 0.161890  max mem: 211\n",
      "Epoch: [1]  [ 460/2012]  eta: 0:05:18  lr: 0.000057  loss: 0.016224 (0.067520)  time: 0.220240  data: 0.182264  max mem: 211\n",
      "Epoch: [1]  [ 480/2012]  eta: 0:05:13  lr: 0.000057  loss: 0.059973 (0.068168)  time: 0.196057  data: 0.158109  max mem: 211\n",
      "Epoch: [1]  [ 500/2012]  eta: 0:05:09  lr: 0.000057  loss: 0.076690 (0.070542)  time: 0.210917  data: 0.172971  max mem: 211\n",
      "Epoch: [1]  [ 520/2012]  eta: 0:05:05  lr: 0.000057  loss: 0.035427 (0.070705)  time: 0.198279  data: 0.160308  max mem: 211\n",
      "Epoch: [1]  [ 540/2012]  eta: 0:05:01  lr: 0.000057  loss: 0.026071 (0.070777)  time: 0.205658  data: 0.167561  max mem: 211\n",
      "Epoch: [1]  [ 560/2012]  eta: 0:04:56  lr: 0.000057  loss: 0.033941 (0.070643)  time: 0.202840  data: 0.164871  max mem: 211\n",
      "Epoch: [1]  [ 580/2012]  eta: 0:04:52  lr: 0.000057  loss: 0.082752 (0.071410)  time: 0.200298  data: 0.162318  max mem: 211\n",
      "Epoch: [1]  [ 600/2012]  eta: 0:04:48  lr: 0.000057  loss: 0.024573 (0.070398)  time: 0.193595  data: 0.155615  max mem: 211\n",
      "Epoch: [1]  [ 620/2012]  eta: 0:04:44  lr: 0.000057  loss: 0.029743 (0.070817)  time: 0.220518  data: 0.182520  max mem: 211\n",
      "Epoch: [1]  [ 640/2012]  eta: 0:04:40  lr: 0.000057  loss: 0.027186 (0.070448)  time: 0.189944  data: 0.151972  max mem: 211\n",
      "Epoch: [1]  [ 660/2012]  eta: 0:04:35  lr: 0.000057  loss: 0.011082 (0.069192)  time: 0.195972  data: 0.157990  max mem: 211\n",
      "Epoch: [1]  [ 680/2012]  eta: 0:04:31  lr: 0.000057  loss: 0.063772 (0.070130)  time: 0.196322  data: 0.158356  max mem: 211\n",
      "Epoch: [1]  [ 700/2012]  eta: 0:04:26  lr: 0.000057  loss: 0.037179 (0.071307)  time: 0.196410  data: 0.158412  max mem: 211\n",
      "Epoch: [1]  [ 720/2012]  eta: 0:04:22  lr: 0.000057  loss: 0.020545 (0.071578)  time: 0.199870  data: 0.161873  max mem: 211\n",
      "Epoch: [1]  [ 740/2012]  eta: 0:04:18  lr: 0.000057  loss: 0.016384 (0.071188)  time: 0.199211  data: 0.161233  max mem: 211\n",
      "Epoch: [1]  [ 760/2012]  eta: 0:04:14  lr: 0.000057  loss: 0.021779 (0.070253)  time: 0.205694  data: 0.167718  max mem: 211\n",
      "Epoch: [1]  [ 780/2012]  eta: 0:04:10  lr: 0.000057  loss: 0.022675 (0.069779)  time: 0.198731  data: 0.160747  max mem: 211\n",
      "Epoch: [1]  [ 800/2012]  eta: 0:04:05  lr: 0.000057  loss: 0.030565 (0.070135)  time: 0.194860  data: 0.156896  max mem: 211\n",
      "Epoch: [1]  [ 820/2012]  eta: 0:04:01  lr: 0.000057  loss: 0.042398 (0.069947)  time: 0.200112  data: 0.162114  max mem: 211\n",
      "Epoch: [1]  [ 840/2012]  eta: 0:03:57  lr: 0.000057  loss: 0.037614 (0.069799)  time: 0.209466  data: 0.171474  max mem: 211\n",
      "Epoch: [1]  [ 860/2012]  eta: 0:03:53  lr: 0.000057  loss: 0.024467 (0.070241)  time: 0.204795  data: 0.166773  max mem: 211\n",
      "Epoch: [1]  [ 880/2012]  eta: 0:03:49  lr: 0.000057  loss: 0.030819 (0.070203)  time: 0.193177  data: 0.155197  max mem: 211\n",
      "Epoch: [1]  [ 900/2012]  eta: 0:03:45  lr: 0.000057  loss: 0.036384 (0.070212)  time: 0.208161  data: 0.170159  max mem: 211\n",
      "Epoch: [1]  [ 920/2012]  eta: 0:03:41  lr: 0.000057  loss: 0.073985 (0.070819)  time: 0.202065  data: 0.164074  max mem: 211\n",
      "Epoch: [1]  [ 940/2012]  eta: 0:03:37  lr: 0.000057  loss: 0.016513 (0.070076)  time: 0.210037  data: 0.172052  max mem: 211\n",
      "Epoch: [1]  [ 960/2012]  eta: 0:03:33  lr: 0.000057  loss: 0.032248 (0.070362)  time: 0.202909  data: 0.164890  max mem: 211\n",
      "Epoch: [1]  [ 980/2012]  eta: 0:03:29  lr: 0.000057  loss: 0.030448 (0.070291)  time: 0.194222  data: 0.156245  max mem: 211\n",
      "Epoch: [1]  [1000/2012]  eta: 0:03:25  lr: 0.000057  loss: 0.021138 (0.070464)  time: 0.217810  data: 0.179826  max mem: 211\n",
      "Epoch: [1]  [1020/2012]  eta: 0:03:21  lr: 0.000057  loss: 0.035175 (0.071362)  time: 0.198106  data: 0.160134  max mem: 211\n",
      "Epoch: [1]  [1040/2012]  eta: 0:03:17  lr: 0.000057  loss: 0.021081 (0.070767)  time: 0.199568  data: 0.161575  max mem: 211\n",
      "Epoch: [1]  [1060/2012]  eta: 0:03:13  lr: 0.000057  loss: 0.043043 (0.070803)  time: 0.204438  data: 0.166450  max mem: 211\n",
      "Epoch: [1]  [1080/2012]  eta: 0:03:09  lr: 0.000057  loss: 0.005939 (0.070387)  time: 0.198309  data: 0.160318  max mem: 211\n",
      "Epoch: [1]  [1100/2012]  eta: 0:03:04  lr: 0.000057  loss: 0.025598 (0.070172)  time: 0.195323  data: 0.157341  max mem: 211\n",
      "Epoch: [1]  [1120/2012]  eta: 0:03:00  lr: 0.000057  loss: 0.023388 (0.070624)  time: 0.201991  data: 0.163978  max mem: 211\n",
      "Epoch: [1]  [1140/2012]  eta: 0:02:56  lr: 0.000057  loss: 0.054648 (0.070932)  time: 0.209436  data: 0.171446  max mem: 211\n",
      "Epoch: [1]  [1160/2012]  eta: 0:02:53  lr: 0.000057  loss: 0.028503 (0.070842)  time: 0.212424  data: 0.174445  max mem: 211\n",
      "Epoch: [1]  [1180/2012]  eta: 0:02:49  lr: 0.000057  loss: 0.020559 (0.070214)  time: 0.218155  data: 0.180173  max mem: 211\n",
      "Epoch: [1]  [1200/2012]  eta: 0:02:45  lr: 0.000057  loss: 0.084066 (0.070593)  time: 0.222112  data: 0.184095  max mem: 211\n",
      "Epoch: [1]  [1220/2012]  eta: 0:02:41  lr: 0.000057  loss: 0.019156 (0.070420)  time: 0.203931  data: 0.165888  max mem: 211\n",
      "Epoch: [1]  [1240/2012]  eta: 0:02:37  lr: 0.000057  loss: 0.027552 (0.070403)  time: 0.207292  data: 0.169272  max mem: 211\n",
      "Epoch: [1]  [1260/2012]  eta: 0:02:33  lr: 0.000057  loss: 0.013936 (0.070094)  time: 0.207691  data: 0.169698  max mem: 211\n",
      "Epoch: [1]  [1280/2012]  eta: 0:02:29  lr: 0.000057  loss: 0.014266 (0.069570)  time: 0.223313  data: 0.185333  max mem: 211\n",
      "Epoch: [1]  [1300/2012]  eta: 0:02:25  lr: 0.000057  loss: 0.013794 (0.069230)  time: 0.199027  data: 0.161049  max mem: 211\n",
      "Epoch: [1]  [1320/2012]  eta: 0:02:21  lr: 0.000057  loss: 0.021247 (0.068977)  time: 0.219222  data: 0.181241  max mem: 211\n",
      "Epoch: [1]  [1340/2012]  eta: 0:02:17  lr: 0.000057  loss: 0.030624 (0.069071)  time: 0.206820  data: 0.168818  max mem: 211\n",
      "Epoch: [1]  [1360/2012]  eta: 0:02:13  lr: 0.000057  loss: 0.020616 (0.068869)  time: 0.204505  data: 0.166522  max mem: 211\n",
      "Epoch: [1]  [1380/2012]  eta: 0:02:09  lr: 0.000057  loss: 0.006549 (0.068819)  time: 0.203909  data: 0.165942  max mem: 211\n",
      "Epoch: [1]  [1400/2012]  eta: 0:02:05  lr: 0.000057  loss: 0.012485 (0.068490)  time: 0.202202  data: 0.164222  max mem: 211\n",
      "Epoch: [1]  [1420/2012]  eta: 0:02:00  lr: 0.000057  loss: 0.039016 (0.069710)  time: 0.197128  data: 0.159134  max mem: 211\n",
      "Epoch: [1]  [1440/2012]  eta: 0:01:56  lr: 0.000057  loss: 0.018038 (0.069510)  time: 0.207565  data: 0.169598  max mem: 211\n",
      "Epoch: [1]  [1460/2012]  eta: 0:01:52  lr: 0.000057  loss: 0.021300 (0.069074)  time: 0.208238  data: 0.170247  max mem: 211\n",
      "Epoch: [1]  [1480/2012]  eta: 0:01:48  lr: 0.000057  loss: 0.026716 (0.068869)  time: 0.196681  data: 0.158699  max mem: 211\n",
      "Epoch: [1]  [1500/2012]  eta: 0:01:44  lr: 0.000057  loss: 0.013329 (0.068528)  time: 0.203440  data: 0.165480  max mem: 211\n",
      "Epoch: [1]  [1520/2012]  eta: 0:01:40  lr: 0.000057  loss: 0.041733 (0.068518)  time: 0.208829  data: 0.170834  max mem: 211\n",
      "Epoch: [1]  [1540/2012]  eta: 0:01:36  lr: 0.000057  loss: 0.022673 (0.068486)  time: 0.214405  data: 0.176435  max mem: 211\n",
      "Epoch: [1]  [1560/2012]  eta: 0:01:32  lr: 0.000057  loss: 0.038302 (0.068718)  time: 0.205745  data: 0.167771  max mem: 211\n",
      "Epoch: [1]  [1580/2012]  eta: 0:01:28  lr: 0.000057  loss: 0.024294 (0.069306)  time: 0.203566  data: 0.165581  max mem: 211\n",
      "Epoch: [1]  [1600/2012]  eta: 0:01:24  lr: 0.000057  loss: 0.037187 (0.069089)  time: 0.207295  data: 0.169327  max mem: 211\n",
      "Epoch: [1]  [1620/2012]  eta: 0:01:20  lr: 0.000057  loss: 0.018513 (0.068884)  time: 0.209952  data: 0.171966  max mem: 211\n",
      "Epoch: [1]  [1640/2012]  eta: 0:01:16  lr: 0.000057  loss: 0.027810 (0.068756)  time: 0.203056  data: 0.165066  max mem: 211\n",
      "Epoch: [1]  [1660/2012]  eta: 0:01:11  lr: 0.000057  loss: 0.018684 (0.068575)  time: 0.197919  data: 0.159919  max mem: 211\n",
      "Epoch: [1]  [1680/2012]  eta: 0:01:07  lr: 0.000057  loss: 0.021076 (0.068263)  time: 0.200697  data: 0.162709  max mem: 211\n",
      "Epoch: [1]  [1700/2012]  eta: 0:01:03  lr: 0.000057  loss: 0.045414 (0.068483)  time: 0.200558  data: 0.162551  max mem: 211\n",
      "Epoch: [1]  [1720/2012]  eta: 0:00:59  lr: 0.000057  loss: 0.022658 (0.068325)  time: 0.198750  data: 0.160776  max mem: 211\n",
      "Epoch: [1]  [1740/2012]  eta: 0:00:55  lr: 0.000057  loss: 0.031650 (0.068462)  time: 0.217829  data: 0.179845  max mem: 211\n",
      "Epoch: [1]  [1760/2012]  eta: 0:00:51  lr: 0.000057  loss: 0.018408 (0.068275)  time: 0.195656  data: 0.157673  max mem: 211\n",
      "Epoch: [1]  [1780/2012]  eta: 0:00:47  lr: 0.000057  loss: 0.026974 (0.068651)  time: 0.217293  data: 0.179308  max mem: 211\n",
      "Epoch: [1]  [1800/2012]  eta: 0:00:43  lr: 0.000057  loss: 0.018216 (0.068365)  time: 0.203029  data: 0.165057  max mem: 211\n",
      "Epoch: [1]  [1820/2012]  eta: 0:00:39  lr: 0.000057  loss: 0.050304 (0.068538)  time: 0.205752  data: 0.167777  max mem: 211\n",
      "Epoch: [1]  [1840/2012]  eta: 0:00:35  lr: 0.000057  loss: 0.029174 (0.068640)  time: 0.208840  data: 0.170854  max mem: 211\n",
      "Epoch: [1]  [1860/2012]  eta: 0:00:31  lr: 0.000057  loss: 0.047021 (0.068761)  time: 0.202940  data: 0.164962  max mem: 211\n",
      "Epoch: [1]  [1880/2012]  eta: 0:00:26  lr: 0.000057  loss: 0.023839 (0.068967)  time: 0.206307  data: 0.168317  max mem: 211\n",
      "Epoch: [1]  [1900/2012]  eta: 0:00:22  lr: 0.000057  loss: 0.014696 (0.068600)  time: 0.207092  data: 0.169112  max mem: 211\n",
      "Epoch: [1]  [1920/2012]  eta: 0:00:18  lr: 0.000057  loss: 0.031854 (0.068381)  time: 0.216901  data: 0.178915  max mem: 211\n",
      "Epoch: [1]  [1940/2012]  eta: 0:00:14  lr: 0.000057  loss: 0.018284 (0.068235)  time: 0.200055  data: 0.162078  max mem: 211\n",
      "Epoch: [1]  [1960/2012]  eta: 0:00:10  lr: 0.000057  loss: 0.033074 (0.068387)  time: 0.200399  data: 0.162393  max mem: 211\n",
      "Epoch: [1]  [1980/2012]  eta: 0:00:06  lr: 0.000057  loss: 0.021297 (0.068362)  time: 0.197392  data: 0.159407  max mem: 211\n",
      "Epoch: [1]  [2000/2012]  eta: 0:00:02  lr: 0.000057  loss: 0.016476 (0.068065)  time: 0.203282  data: 0.165307  max mem: 211\n",
      "Epoch: [1]  [2011/2012]  eta: 0:00:00  lr: 0.000057  loss: 0.017391 (0.068164)  time: 0.191227  data: 0.154632  max mem: 211\n",
      "Epoch: [1] Total time: 0:06:51 (0.204406 s / it)\n",
      "Averaged stats: lr: 0.000057  loss: 0.017391 (0.068164)\n",
      "Test:  [ 0/79]  eta: 0:00:16  loss: 0.005134 (0.005134)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.204122  data: 0.166035  max mem: 211\n",
      "Test:  [20/79]  eta: 0:00:12  loss: 0.071240 (0.122118)  acc1: 93.750000 (95.833333)  acc5: 100.000000 (99.404762)  time: 0.211785  data: 0.173786  max mem: 211\n",
      "Test:  [40/79]  eta: 0:00:08  loss: 0.032633 (0.099566)  acc1: 100.000000 (96.646341)  acc5: 100.000000 (99.542683)  time: 0.241303  data: 0.203307  max mem: 211\n",
      "Test:  [60/79]  eta: 0:00:04  loss: 0.036663 (0.100463)  acc1: 100.000000 (96.516393)  acc5: 100.000000 (99.590164)  time: 0.235807  data: 0.197807  max mem: 211\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 0.024871 (0.095041)  acc1: 100.000000 (97.040000)  acc5: 100.000000 (99.680000)  time: 0.213414  data: 0.176821  max mem: 211\n",
      "Test: Total time: 0:00:17 (0.225887 s / it)\n",
      "* Acc@1 97.040 Acc@5 99.680 loss 0.095\n",
      "Accuracy at epoch 1 of the network on the 79 test images: 97.0%\n",
      "Accuracy at epoch 1 of the network on the 1250 test images: 97.0%\n",
      "Max accuracy so far: 97.04%\n",
      "Epoch: [2]  [   0/2012]  eta: 0:06:06  lr: 0.000041  loss: 0.007397 (0.007397)  time: 0.182223  data: 0.144302  max mem: 211\n",
      "Epoch: [2]  [  20/2012]  eta: 0:07:05  lr: 0.000041  loss: 0.007334 (0.043210)  time: 0.215122  data: 0.177141  max mem: 211\n",
      "Epoch: [2]  [  40/2012]  eta: 0:06:50  lr: 0.000041  loss: 0.023129 (0.054073)  time: 0.202978  data: 0.164999  max mem: 211\n",
      "Epoch: [2]  [  60/2012]  eta: 0:06:38  lr: 0.000041  loss: 0.019194 (0.059363)  time: 0.195123  data: 0.157133  max mem: 211\n",
      "Epoch: [2]  [  80/2012]  eta: 0:06:28  lr: 0.000041  loss: 0.040796 (0.060288)  time: 0.191495  data: 0.153530  max mem: 211\n",
      "Epoch: [2]  [ 100/2012]  eta: 0:06:27  lr: 0.000041  loss: 0.016071 (0.053629)  time: 0.210631  data: 0.172649  max mem: 211\n",
      "Epoch: [2]  [ 120/2012]  eta: 0:06:20  lr: 0.000041  loss: 0.031036 (0.054180)  time: 0.193101  data: 0.155090  max mem: 211\n",
      "Epoch: [2]  [ 140/2012]  eta: 0:06:15  lr: 0.000041  loss: 0.029537 (0.055932)  time: 0.196560  data: 0.158566  max mem: 211\n",
      "Epoch: [2]  [ 160/2012]  eta: 0:06:10  lr: 0.000041  loss: 0.019662 (0.054226)  time: 0.195348  data: 0.157361  max mem: 211\n",
      "Epoch: [2]  [ 180/2012]  eta: 0:06:05  lr: 0.000041  loss: 0.019317 (0.054424)  time: 0.197347  data: 0.159353  max mem: 211\n",
      "Epoch: [2]  [ 200/2012]  eta: 0:06:02  lr: 0.000041  loss: 0.009490 (0.053741)  time: 0.203538  data: 0.165565  max mem: 211\n",
      "Epoch: [2]  [ 220/2012]  eta: 0:05:57  lr: 0.000041  loss: 0.044931 (0.053764)  time: 0.196234  data: 0.158258  max mem: 211\n",
      "Epoch: [2]  [ 240/2012]  eta: 0:05:54  lr: 0.000041  loss: 0.016958 (0.051706)  time: 0.204405  data: 0.166440  max mem: 211\n",
      "Epoch: [2]  [ 260/2012]  eta: 0:05:53  lr: 0.000041  loss: 0.018099 (0.050630)  time: 0.221582  data: 0.183619  max mem: 211\n",
      "Epoch: [2]  [ 280/2012]  eta: 0:05:50  lr: 0.000041  loss: 0.013117 (0.052987)  time: 0.210405  data: 0.172415  max mem: 211\n",
      "Epoch: [2]  [ 300/2012]  eta: 0:05:45  lr: 0.000041  loss: 0.024738 (0.052701)  time: 0.195531  data: 0.157563  max mem: 211\n",
      "Epoch: [2]  [ 320/2012]  eta: 0:05:42  lr: 0.000041  loss: 0.026729 (0.052496)  time: 0.208174  data: 0.170183  max mem: 211\n",
      "Epoch: [2]  [ 340/2012]  eta: 0:05:37  lr: 0.000041  loss: 0.018768 (0.052353)  time: 0.196340  data: 0.158354  max mem: 211\n",
      "Epoch: [2]  [ 360/2012]  eta: 0:05:35  lr: 0.000041  loss: 0.011804 (0.051842)  time: 0.218137  data: 0.180157  max mem: 211\n",
      "Epoch: [2]  [ 380/2012]  eta: 0:05:31  lr: 0.000041  loss: 0.010167 (0.051705)  time: 0.203130  data: 0.165166  max mem: 211\n",
      "Epoch: [2]  [ 400/2012]  eta: 0:05:27  lr: 0.000041  loss: 0.025219 (0.052098)  time: 0.211693  data: 0.173701  max mem: 211\n",
      "Epoch: [2]  [ 420/2012]  eta: 0:05:23  lr: 0.000041  loss: 0.009342 (0.050842)  time: 0.205690  data: 0.167699  max mem: 211\n",
      "Epoch: [2]  [ 440/2012]  eta: 0:05:19  lr: 0.000041  loss: 0.024617 (0.051255)  time: 0.201298  data: 0.163311  max mem: 211\n",
      "Epoch: [2]  [ 460/2012]  eta: 0:05:16  lr: 0.000041  loss: 0.010783 (0.050980)  time: 0.224076  data: 0.186096  max mem: 211\n",
      "Epoch: [2]  [ 480/2012]  eta: 0:05:12  lr: 0.000041  loss: 0.030204 (0.051565)  time: 0.199060  data: 0.161076  max mem: 211\n",
      "Epoch: [2]  [ 500/2012]  eta: 0:05:08  lr: 0.000041  loss: 0.045213 (0.053514)  time: 0.206264  data: 0.168280  max mem: 211\n",
      "Epoch: [2]  [ 520/2012]  eta: 0:05:04  lr: 0.000041  loss: 0.023753 (0.053716)  time: 0.199325  data: 0.161346  max mem: 211\n",
      "Epoch: [2]  [ 540/2012]  eta: 0:05:00  lr: 0.000041  loss: 0.019554 (0.053665)  time: 0.207129  data: 0.169145  max mem: 211\n",
      "Epoch: [2]  [ 560/2012]  eta: 0:04:56  lr: 0.000041  loss: 0.025037 (0.053468)  time: 0.204351  data: 0.166375  max mem: 211\n",
      "Epoch: [2]  [ 580/2012]  eta: 0:04:52  lr: 0.000041  loss: 0.058022 (0.054046)  time: 0.204368  data: 0.166378  max mem: 211\n",
      "Epoch: [2]  [ 600/2012]  eta: 0:04:47  lr: 0.000041  loss: 0.012898 (0.053243)  time: 0.193547  data: 0.155567  max mem: 211\n",
      "Epoch: [2]  [ 620/2012]  eta: 0:04:43  lr: 0.000041  loss: 0.020770 (0.053741)  time: 0.212157  data: 0.174169  max mem: 211\n",
      "Epoch: [2]  [ 640/2012]  eta: 0:04:39  lr: 0.000041  loss: 0.016702 (0.053419)  time: 0.193841  data: 0.155832  max mem: 211\n",
      "Epoch: [2]  [ 660/2012]  eta: 0:04:34  lr: 0.000041  loss: 0.007353 (0.052441)  time: 0.191098  data: 0.153121  max mem: 211\n",
      "Epoch: [2]  [ 680/2012]  eta: 0:04:30  lr: 0.000041  loss: 0.047338 (0.053081)  time: 0.193038  data: 0.155052  max mem: 211\n",
      "Epoch: [2]  [ 700/2012]  eta: 0:04:25  lr: 0.000041  loss: 0.029067 (0.054058)  time: 0.194174  data: 0.156199  max mem: 211\n",
      "Epoch: [2]  [ 720/2012]  eta: 0:04:21  lr: 0.000041  loss: 0.012672 (0.054392)  time: 0.199983  data: 0.161981  max mem: 211\n",
      "Epoch: [2]  [ 740/2012]  eta: 0:04:17  lr: 0.000041  loss: 0.010344 (0.054039)  time: 0.197248  data: 0.159279  max mem: 211\n",
      "Epoch: [2]  [ 760/2012]  eta: 0:04:13  lr: 0.000041  loss: 0.013413 (0.053365)  time: 0.204599  data: 0.166606  max mem: 211\n",
      "Epoch: [2]  [ 780/2012]  eta: 0:04:09  lr: 0.000041  loss: 0.011719 (0.053016)  time: 0.194420  data: 0.156421  max mem: 211\n",
      "Epoch: [2]  [ 800/2012]  eta: 0:04:05  lr: 0.000041  loss: 0.018959 (0.053221)  time: 0.202046  data: 0.164055  max mem: 211\n",
      "Epoch: [2]  [ 820/2012]  eta: 0:04:01  lr: 0.000041  loss: 0.029514 (0.053055)  time: 0.201539  data: 0.163528  max mem: 211\n",
      "Epoch: [2]  [ 840/2012]  eta: 0:03:57  lr: 0.000041  loss: 0.027949 (0.052871)  time: 0.209102  data: 0.171108  max mem: 211\n",
      "Epoch: [2]  [ 860/2012]  eta: 0:03:53  lr: 0.000041  loss: 0.020993 (0.053308)  time: 0.200653  data: 0.162648  max mem: 211\n",
      "Epoch: [2]  [ 880/2012]  eta: 0:03:48  lr: 0.000041  loss: 0.025337 (0.053263)  time: 0.191435  data: 0.153456  max mem: 211\n",
      "Epoch: [2]  [ 900/2012]  eta: 0:03:44  lr: 0.000041  loss: 0.025043 (0.053222)  time: 0.208212  data: 0.170190  max mem: 211\n",
      "Epoch: [2]  [ 920/2012]  eta: 0:03:40  lr: 0.000041  loss: 0.039404 (0.053600)  time: 0.201476  data: 0.163501  max mem: 211\n",
      "Epoch: [2]  [ 940/2012]  eta: 0:03:37  lr: 0.000041  loss: 0.014652 (0.053030)  time: 0.211327  data: 0.173331  max mem: 211\n",
      "Epoch: [2]  [ 960/2012]  eta: 0:03:32  lr: 0.000041  loss: 0.026646 (0.053357)  time: 0.199982  data: 0.161972  max mem: 211\n",
      "Epoch: [2]  [ 980/2012]  eta: 0:03:28  lr: 0.000041  loss: 0.022574 (0.053330)  time: 0.194672  data: 0.156691  max mem: 211\n",
      "Epoch: [2]  [1000/2012]  eta: 0:03:24  lr: 0.000041  loss: 0.017561 (0.053473)  time: 0.212887  data: 0.174895  max mem: 211\n",
      "Epoch: [2]  [1020/2012]  eta: 0:03:20  lr: 0.000041  loss: 0.027893 (0.054262)  time: 0.196576  data: 0.158590  max mem: 211\n",
      "Epoch: [2]  [1040/2012]  eta: 0:03:16  lr: 0.000041  loss: 0.018649 (0.053808)  time: 0.197964  data: 0.159976  max mem: 211\n",
      "Epoch: [2]  [1060/2012]  eta: 0:03:12  lr: 0.000041  loss: 0.031340 (0.053766)  time: 0.205033  data: 0.167044  max mem: 211\n",
      "Epoch: [2]  [1080/2012]  eta: 0:03:08  lr: 0.000041  loss: 0.004740 (0.053507)  time: 0.200574  data: 0.162580  max mem: 211\n",
      "Epoch: [2]  [1100/2012]  eta: 0:03:04  lr: 0.000041  loss: 0.018034 (0.053339)  time: 0.193359  data: 0.155386  max mem: 211\n",
      "Epoch: [2]  [1120/2012]  eta: 0:03:00  lr: 0.000041  loss: 0.011641 (0.053835)  time: 0.192638  data: 0.154659  max mem: 211\n",
      "Epoch: [2]  [1140/2012]  eta: 0:02:56  lr: 0.000041  loss: 0.031415 (0.054111)  time: 0.202697  data: 0.164707  max mem: 211\n",
      "Epoch: [2]  [1160/2012]  eta: 0:02:52  lr: 0.000041  loss: 0.024265 (0.054053)  time: 0.205948  data: 0.167971  max mem: 211\n",
      "Epoch: [2]  [1180/2012]  eta: 0:02:48  lr: 0.000041  loss: 0.013522 (0.053558)  time: 0.215087  data: 0.177108  max mem: 211\n",
      "Epoch: [2]  [1200/2012]  eta: 0:02:44  lr: 0.000041  loss: 0.058195 (0.053811)  time: 0.214770  data: 0.176784  max mem: 211\n",
      "Epoch: [2]  [1220/2012]  eta: 0:02:40  lr: 0.000041  loss: 0.012300 (0.053704)  time: 0.200329  data: 0.162354  max mem: 211\n",
      "Epoch: [2]  [1240/2012]  eta: 0:02:36  lr: 0.000041  loss: 0.018887 (0.053682)  time: 0.197922  data: 0.159942  max mem: 211\n",
      "Epoch: [2]  [1260/2012]  eta: 0:02:32  lr: 0.000041  loss: 0.009921 (0.053486)  time: 0.194121  data: 0.156142  max mem: 211\n",
      "Epoch: [2]  [1280/2012]  eta: 0:02:28  lr: 0.000041  loss: 0.009277 (0.053085)  time: 0.203370  data: 0.165378  max mem: 211\n",
      "Epoch: [2]  [1300/2012]  eta: 0:02:23  lr: 0.000041  loss: 0.010997 (0.052817)  time: 0.193971  data: 0.156005  max mem: 211\n",
      "Epoch: [2]  [1320/2012]  eta: 0:02:20  lr: 0.000041  loss: 0.018060 (0.052628)  time: 0.216062  data: 0.178074  max mem: 211\n",
      "Epoch: [2]  [1340/2012]  eta: 0:02:15  lr: 0.000041  loss: 0.025632 (0.052691)  time: 0.199863  data: 0.161887  max mem: 211\n",
      "Epoch: [2]  [1360/2012]  eta: 0:02:11  lr: 0.000041  loss: 0.016249 (0.052585)  time: 0.203630  data: 0.165639  max mem: 211\n",
      "Epoch: [2]  [1380/2012]  eta: 0:02:07  lr: 0.000041  loss: 0.005394 (0.052589)  time: 0.200751  data: 0.162771  max mem: 211\n",
      "Epoch: [2]  [1400/2012]  eta: 0:02:03  lr: 0.000041  loss: 0.008272 (0.052346)  time: 0.196307  data: 0.158331  max mem: 211\n",
      "Epoch: [2]  [1420/2012]  eta: 0:01:59  lr: 0.000041  loss: 0.027216 (0.053498)  time: 0.195456  data: 0.157469  max mem: 211\n",
      "Epoch: [2]  [1440/2012]  eta: 0:01:55  lr: 0.000041  loss: 0.011793 (0.053343)  time: 0.207254  data: 0.169279  max mem: 211\n",
      "Epoch: [2]  [1460/2012]  eta: 0:01:51  lr: 0.000041  loss: 0.015663 (0.052972)  time: 0.199757  data: 0.161778  max mem: 211\n",
      "Epoch: [2]  [1480/2012]  eta: 0:01:47  lr: 0.000041  loss: 0.019427 (0.052805)  time: 0.192992  data: 0.155028  max mem: 211\n",
      "Epoch: [2]  [1500/2012]  eta: 0:01:43  lr: 0.000041  loss: 0.010997 (0.052570)  time: 0.203830  data: 0.165836  max mem: 211\n",
      "Epoch: [2]  [1520/2012]  eta: 0:01:39  lr: 0.000041  loss: 0.028101 (0.052535)  time: 0.206710  data: 0.168725  max mem: 211\n",
      "Epoch: [2]  [1540/2012]  eta: 0:01:35  lr: 0.000041  loss: 0.020323 (0.052496)  time: 0.208380  data: 0.170391  max mem: 211\n",
      "Epoch: [2]  [1560/2012]  eta: 0:01:31  lr: 0.000041  loss: 0.030685 (0.052671)  time: 0.208543  data: 0.170537  max mem: 211\n",
      "Epoch: [2]  [1580/2012]  eta: 0:01:27  lr: 0.000041  loss: 0.018478 (0.053220)  time: 0.206539  data: 0.168555  max mem: 211\n",
      "Epoch: [2]  [1600/2012]  eta: 0:01:23  lr: 0.000041  loss: 0.025651 (0.053059)  time: 0.210164  data: 0.172177  max mem: 211\n",
      "Epoch: [2]  [1620/2012]  eta: 0:01:19  lr: 0.000041  loss: 0.020483 (0.052926)  time: 0.205656  data: 0.167674  max mem: 211\n",
      "Epoch: [2]  [1640/2012]  eta: 0:01:15  lr: 0.000041  loss: 0.019401 (0.052828)  time: 0.199082  data: 0.161075  max mem: 211\n",
      "Epoch: [2]  [1660/2012]  eta: 0:01:11  lr: 0.000041  loss: 0.014225 (0.052708)  time: 0.193928  data: 0.155923  max mem: 211\n",
      "Epoch: [2]  [1680/2012]  eta: 0:01:07  lr: 0.000041  loss: 0.015178 (0.052484)  time: 0.203330  data: 0.165337  max mem: 211\n",
      "Epoch: [2]  [1700/2012]  eta: 0:01:03  lr: 0.000041  loss: 0.031773 (0.052661)  time: 0.192537  data: 0.154550  max mem: 211\n",
      "Epoch: [2]  [1720/2012]  eta: 0:00:59  lr: 0.000041  loss: 0.015039 (0.052498)  time: 0.195700  data: 0.157705  max mem: 211\n",
      "Epoch: [2]  [1740/2012]  eta: 0:00:55  lr: 0.000041  loss: 0.024641 (0.052649)  time: 0.212596  data: 0.174605  max mem: 211\n",
      "Epoch: [2]  [1760/2012]  eta: 0:00:50  lr: 0.000041  loss: 0.016918 (0.052506)  time: 0.195318  data: 0.157342  max mem: 211\n",
      "Epoch: [2]  [1780/2012]  eta: 0:00:46  lr: 0.000041  loss: 0.022632 (0.052860)  time: 0.213575  data: 0.175585  max mem: 211\n",
      "Epoch: [2]  [1800/2012]  eta: 0:00:42  lr: 0.000041  loss: 0.013046 (0.052642)  time: 0.197451  data: 0.159470  max mem: 211\n",
      "Epoch: [2]  [1820/2012]  eta: 0:00:38  lr: 0.000041  loss: 0.039960 (0.052774)  time: 0.205214  data: 0.167224  max mem: 211\n",
      "Epoch: [2]  [1840/2012]  eta: 0:00:34  lr: 0.000041  loss: 0.023782 (0.052912)  time: 0.204905  data: 0.166908  max mem: 211\n",
      "Epoch: [2]  [1860/2012]  eta: 0:00:30  lr: 0.000041  loss: 0.026295 (0.052973)  time: 0.195280  data: 0.157283  max mem: 211\n",
      "Epoch: [2]  [1880/2012]  eta: 0:00:26  lr: 0.000041  loss: 0.020288 (0.053136)  time: 0.197534  data: 0.159551  max mem: 211\n",
      "Epoch: [2]  [1900/2012]  eta: 0:00:22  lr: 0.000041  loss: 0.016384 (0.052855)  time: 0.199640  data: 0.161663  max mem: 211\n",
      "Epoch: [2]  [1920/2012]  eta: 0:00:18  lr: 0.000041  loss: 0.024604 (0.052674)  time: 0.212084  data: 0.174079  max mem: 211\n",
      "Epoch: [2]  [1940/2012]  eta: 0:00:14  lr: 0.000041  loss: 0.014726 (0.052581)  time: 0.195516  data: 0.157523  max mem: 211\n",
      "Epoch: [2]  [1960/2012]  eta: 0:00:10  lr: 0.000041  loss: 0.021592 (0.052696)  time: 0.198916  data: 0.160918  max mem: 211\n",
      "Epoch: [2]  [1980/2012]  eta: 0:00:06  lr: 0.000041  loss: 0.013827 (0.052680)  time: 0.186787  data: 0.148753  max mem: 211\n",
      "Epoch: [2]  [2000/2012]  eta: 0:00:02  lr: 0.000041  loss: 0.013832 (0.052432)  time: 0.199438  data: 0.161445  max mem: 211\n",
      "Epoch: [2]  [2011/2012]  eta: 0:00:00  lr: 0.000041  loss: 0.014420 (0.052536)  time: 0.183145  data: 0.146553  max mem: 211\n",
      "Epoch: [2] Total time: 0:06:46 (0.201916 s / it)\n",
      "Averaged stats: lr: 0.000041  loss: 0.014420 (0.052536)\n",
      "Test:  [ 0/79]  eta: 0:00:15  loss: 0.003668 (0.003668)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.197943  data: 0.159956  max mem: 211\n",
      "Test:  [20/79]  eta: 0:00:12  loss: 0.061248 (0.120251)  acc1: 93.750000 (96.130952)  acc5: 100.000000 (99.702381)  time: 0.209048  data: 0.171047  max mem: 211\n",
      "Test:  [40/79]  eta: 0:00:08  loss: 0.024261 (0.095097)  acc1: 100.000000 (97.103659)  acc5: 100.000000 (99.695122)  time: 0.232736  data: 0.194740  max mem: 211\n",
      "Test:  [60/79]  eta: 0:00:04  loss: 0.037309 (0.096409)  acc1: 100.000000 (96.823770)  acc5: 100.000000 (99.795082)  time: 0.219488  data: 0.181516  max mem: 211\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 0.022172 (0.089802)  acc1: 100.000000 (97.280000)  acc5: 100.000000 (99.840000)  time: 0.203076  data: 0.166507  max mem: 211\n",
      "Test: Total time: 0:00:17 (0.216459 s / it)\n",
      "* Acc@1 97.280 Acc@5 99.840 loss 0.090\n",
      "Accuracy at epoch 2 of the network on the 79 test images: 97.3%\n",
      "Accuracy at epoch 2 of the network on the 1250 test images: 97.3%\n",
      "Max accuracy so far: 97.28%\n",
      "Epoch: [3]  [   0/2012]  eta: 0:05:56  lr: 0.000022  loss: 0.006422 (0.006422)  time: 0.176956  data: 0.139000  max mem: 211\n",
      "Epoch: [3]  [  20/2012]  eta: 0:07:02  lr: 0.000022  loss: 0.006875 (0.037222)  time: 0.214079  data: 0.176113  max mem: 211\n",
      "Epoch: [3]  [  40/2012]  eta: 0:06:51  lr: 0.000022  loss: 0.015977 (0.046733)  time: 0.205160  data: 0.167201  max mem: 211\n",
      "Epoch: [3]  [  60/2012]  eta: 0:06:37  lr: 0.000022  loss: 0.015236 (0.050933)  time: 0.192879  data: 0.154907  max mem: 211\n",
      "Epoch: [3]  [  80/2012]  eta: 0:06:27  lr: 0.000022  loss: 0.033818 (0.052370)  time: 0.192263  data: 0.154282  max mem: 211\n",
      "Epoch: [3]  [ 100/2012]  eta: 0:06:26  lr: 0.000022  loss: 0.012075 (0.046469)  time: 0.208577  data: 0.170598  max mem: 211\n",
      "Epoch: [3]  [ 120/2012]  eta: 0:06:20  lr: 0.000022  loss: 0.024137 (0.046378)  time: 0.193446  data: 0.155450  max mem: 211\n",
      "Epoch: [3]  [ 140/2012]  eta: 0:06:14  lr: 0.000022  loss: 0.018906 (0.047590)  time: 0.194245  data: 0.156266  max mem: 211\n",
      "Epoch: [3]  [ 160/2012]  eta: 0:06:08  lr: 0.000022  loss: 0.018652 (0.046192)  time: 0.194183  data: 0.156212  max mem: 211\n",
      "Epoch: [3]  [ 180/2012]  eta: 0:06:02  lr: 0.000022  loss: 0.014015 (0.046726)  time: 0.188245  data: 0.150276  max mem: 211\n",
      "Epoch: [3]  [ 200/2012]  eta: 0:05:59  lr: 0.000022  loss: 0.009076 (0.045898)  time: 0.199803  data: 0.161825  max mem: 211\n",
      "Epoch: [3]  [ 220/2012]  eta: 0:05:55  lr: 0.000022  loss: 0.033768 (0.045775)  time: 0.200456  data: 0.162480  max mem: 211\n",
      "Epoch: [3]  [ 240/2012]  eta: 0:05:51  lr: 0.000022  loss: 0.016192 (0.044036)  time: 0.198519  data: 0.160548  max mem: 211\n",
      "Epoch: [3]  [ 260/2012]  eta: 0:05:50  lr: 0.000022  loss: 0.018225 (0.043066)  time: 0.218215  data: 0.180232  max mem: 211\n",
      "Epoch: [3]  [ 280/2012]  eta: 0:05:46  lr: 0.000022  loss: 0.012326 (0.045316)  time: 0.203729  data: 0.165758  max mem: 211\n",
      "Epoch: [3]  [ 300/2012]  eta: 0:05:42  lr: 0.000022  loss: 0.023651 (0.045130)  time: 0.194633  data: 0.156662  max mem: 211\n",
      "Epoch: [3]  [ 320/2012]  eta: 0:05:38  lr: 0.000022  loss: 0.021671 (0.045004)  time: 0.200380  data: 0.162399  max mem: 211\n",
      "Epoch: [3]  [ 340/2012]  eta: 0:05:33  lr: 0.000022  loss: 0.013914 (0.044840)  time: 0.192764  data: 0.154790  max mem: 211\n",
      "Epoch: [3]  [ 360/2012]  eta: 0:05:30  lr: 0.000022  loss: 0.012224 (0.044310)  time: 0.208168  data: 0.170198  max mem: 211\n",
      "Epoch: [3]  [ 380/2012]  eta: 0:05:26  lr: 0.000022  loss: 0.010043 (0.044224)  time: 0.197404  data: 0.159427  max mem: 211\n",
      "Epoch: [3]  [ 400/2012]  eta: 0:05:22  lr: 0.000022  loss: 0.021800 (0.044587)  time: 0.203567  data: 0.165596  max mem: 211\n",
      "Epoch: [3]  [ 420/2012]  eta: 0:05:18  lr: 0.000022  loss: 0.006796 (0.043487)  time: 0.204119  data: 0.166144  max mem: 211\n",
      "Epoch: [3]  [ 440/2012]  eta: 0:05:14  lr: 0.000022  loss: 0.016135 (0.043749)  time: 0.196596  data: 0.158622  max mem: 211\n",
      "Epoch: [3]  [ 460/2012]  eta: 0:05:11  lr: 0.000022  loss: 0.008649 (0.043543)  time: 0.214375  data: 0.176374  max mem: 211\n",
      "Epoch: [3]  [ 480/2012]  eta: 0:05:06  lr: 0.000022  loss: 0.028315 (0.044144)  time: 0.193073  data: 0.155092  max mem: 211\n",
      "Epoch: [3]  [ 500/2012]  eta: 0:05:02  lr: 0.000022  loss: 0.038096 (0.045834)  time: 0.198469  data: 0.160490  max mem: 211\n",
      "Epoch: [3]  [ 520/2012]  eta: 0:04:58  lr: 0.000022  loss: 0.020190 (0.046025)  time: 0.196249  data: 0.158254  max mem: 211\n",
      "Epoch: [3]  [ 540/2012]  eta: 0:04:54  lr: 0.000022  loss: 0.017628 (0.046013)  time: 0.205571  data: 0.167585  max mem: 211\n",
      "Epoch: [3]  [ 560/2012]  eta: 0:04:50  lr: 0.000022  loss: 0.021652 (0.045765)  time: 0.201268  data: 0.163290  max mem: 211\n",
      "Epoch: [3]  [ 580/2012]  eta: 0:04:46  lr: 0.000022  loss: 0.046436 (0.046122)  time: 0.196502  data: 0.158511  max mem: 211\n",
      "Epoch: [3]  [ 600/2012]  eta: 0:04:42  lr: 0.000022  loss: 0.010618 (0.045428)  time: 0.187751  data: 0.149772  max mem: 211\n",
      "Epoch: [3]  [ 620/2012]  eta: 0:04:39  lr: 0.000022  loss: 0.013992 (0.045874)  time: 0.221195  data: 0.183202  max mem: 211\n",
      "Epoch: [3]  [ 640/2012]  eta: 0:04:34  lr: 0.000022  loss: 0.013873 (0.045581)  time: 0.184348  data: 0.146365  max mem: 211\n",
      "Epoch: [3]  [ 660/2012]  eta: 0:04:29  lr: 0.000022  loss: 0.006242 (0.044769)  time: 0.186951  data: 0.148968  max mem: 211\n",
      "Epoch: [3]  [ 680/2012]  eta: 0:04:25  lr: 0.000022  loss: 0.038505 (0.045212)  time: 0.189460  data: 0.151488  max mem: 211\n",
      "Epoch: [3]  [ 700/2012]  eta: 0:04:21  lr: 0.000022  loss: 0.026278 (0.046068)  time: 0.195918  data: 0.157934  max mem: 211\n",
      "Epoch: [3]  [ 720/2012]  eta: 0:04:17  lr: 0.000022  loss: 0.009939 (0.046494)  time: 0.211786  data: 0.173739  max mem: 211\n",
      "Epoch: [3]  [ 740/2012]  eta: 0:04:13  lr: 0.000022  loss: 0.009179 (0.046128)  time: 0.191388  data: 0.153401  max mem: 211\n",
      "Epoch: [3]  [ 760/2012]  eta: 0:04:10  lr: 0.000022  loss: 0.012287 (0.045583)  time: 0.221642  data: 0.183644  max mem: 211\n",
      "Epoch: [3]  [ 780/2012]  eta: 0:04:06  lr: 0.000022  loss: 0.008007 (0.045312)  time: 0.197045  data: 0.159018  max mem: 211\n",
      "Epoch: [3]  [ 800/2012]  eta: 0:04:01  lr: 0.000022  loss: 0.015070 (0.045388)  time: 0.188024  data: 0.150024  max mem: 211\n",
      "Epoch: [3]  [ 820/2012]  eta: 0:03:58  lr: 0.000022  loss: 0.021446 (0.045221)  time: 0.216204  data: 0.178166  max mem: 211\n",
      "Epoch: [3]  [ 840/2012]  eta: 0:03:54  lr: 0.000022  loss: 0.019415 (0.045046)  time: 0.211666  data: 0.173638  max mem: 211\n",
      "Epoch: [3]  [ 860/2012]  eta: 0:03:51  lr: 0.000022  loss: 0.020484 (0.045368)  time: 0.217064  data: 0.179031  max mem: 211\n",
      "Epoch: [3]  [ 880/2012]  eta: 0:03:47  lr: 0.000022  loss: 0.020194 (0.045330)  time: 0.203176  data: 0.165160  max mem: 211\n",
      "Epoch: [3]  [ 900/2012]  eta: 0:03:43  lr: 0.000022  loss: 0.019713 (0.045213)  time: 0.201047  data: 0.163064  max mem: 211\n",
      "Epoch: [3]  [ 920/2012]  eta: 0:03:39  lr: 0.000022  loss: 0.031853 (0.045486)  time: 0.196980  data: 0.158978  max mem: 211\n",
      "Epoch: [3]  [ 940/2012]  eta: 0:03:35  lr: 0.000022  loss: 0.014108 (0.045012)  time: 0.211792  data: 0.173794  max mem: 211\n",
      "Epoch: [3]  [ 960/2012]  eta: 0:03:31  lr: 0.000022  loss: 0.024285 (0.045387)  time: 0.196105  data: 0.158100  max mem: 211\n",
      "Epoch: [3]  [ 980/2012]  eta: 0:03:27  lr: 0.000022  loss: 0.017534 (0.045420)  time: 0.193640  data: 0.155643  max mem: 211\n",
      "Epoch: [3]  [1000/2012]  eta: 0:03:23  lr: 0.000022  loss: 0.016626 (0.045482)  time: 0.218702  data: 0.180696  max mem: 211\n",
      "Epoch: [3]  [1020/2012]  eta: 0:03:19  lr: 0.000022  loss: 0.024974 (0.046128)  time: 0.206300  data: 0.168316  max mem: 211\n",
      "Epoch: [3]  [1040/2012]  eta: 0:03:15  lr: 0.000022  loss: 0.015200 (0.045726)  time: 0.209662  data: 0.171664  max mem: 211\n",
      "Epoch: [3]  [1060/2012]  eta: 0:03:11  lr: 0.000022  loss: 0.027741 (0.045654)  time: 0.199420  data: 0.161345  max mem: 211\n",
      "Epoch: [3]  [1080/2012]  eta: 0:03:07  lr: 0.000022  loss: 0.004658 (0.045449)  time: 0.196334  data: 0.158336  max mem: 211\n",
      "Epoch: [3]  [1100/2012]  eta: 0:03:03  lr: 0.000022  loss: 0.015620 (0.045291)  time: 0.206312  data: 0.168306  max mem: 211\n",
      "Epoch: [3]  [1120/2012]  eta: 0:02:59  lr: 0.000022  loss: 0.009034 (0.045774)  time: 0.196021  data: 0.158033  max mem: 211\n",
      "Epoch: [3]  [1140/2012]  eta: 0:02:55  lr: 0.000022  loss: 0.022969 (0.046018)  time: 0.204076  data: 0.166075  max mem: 211\n",
      "Epoch: [3]  [1160/2012]  eta: 0:02:51  lr: 0.000022  loss: 0.023801 (0.045981)  time: 0.201306  data: 0.163333  max mem: 211\n",
      "Epoch: [3]  [1180/2012]  eta: 0:02:47  lr: 0.000022  loss: 0.013221 (0.045562)  time: 0.215549  data: 0.177573  max mem: 211\n",
      "Epoch: [3]  [1200/2012]  eta: 0:02:43  lr: 0.000022  loss: 0.046638 (0.045766)  time: 0.228836  data: 0.190821  max mem: 211\n",
      "Epoch: [3]  [1220/2012]  eta: 0:02:39  lr: 0.000022  loss: 0.008419 (0.045676)  time: 0.210152  data: 0.172063  max mem: 211\n",
      "Epoch: [3]  [1240/2012]  eta: 0:02:35  lr: 0.000022  loss: 0.015610 (0.045623)  time: 0.200615  data: 0.162600  max mem: 211\n",
      "Epoch: [3]  [1260/2012]  eta: 0:02:31  lr: 0.000022  loss: 0.008202 (0.045480)  time: 0.201121  data: 0.163090  max mem: 211\n",
      "Epoch: [3]  [1280/2012]  eta: 0:02:27  lr: 0.000022  loss: 0.007599 (0.045156)  time: 0.212203  data: 0.174179  max mem: 211\n",
      "Epoch: [3]  [1300/2012]  eta: 0:02:23  lr: 0.000022  loss: 0.011465 (0.044936)  time: 0.197735  data: 0.159689  max mem: 211\n",
      "Epoch: [3]  [1320/2012]  eta: 0:02:19  lr: 0.000022  loss: 0.018867 (0.044785)  time: 0.210098  data: 0.172103  max mem: 211\n",
      "Epoch: [3]  [1340/2012]  eta: 0:02:15  lr: 0.000022  loss: 0.016822 (0.044825)  time: 0.198468  data: 0.160459  max mem: 211\n",
      "Epoch: [3]  [1360/2012]  eta: 0:02:11  lr: 0.000022  loss: 0.014720 (0.044761)  time: 0.209194  data: 0.171219  max mem: 211\n",
      "Epoch: [3]  [1380/2012]  eta: 0:02:07  lr: 0.000022  loss: 0.005157 (0.044774)  time: 0.206230  data: 0.168250  max mem: 211\n",
      "Epoch: [3]  [1400/2012]  eta: 0:02:03  lr: 0.000022  loss: 0.007528 (0.044549)  time: 0.195820  data: 0.157838  max mem: 211\n",
      "Epoch: [3]  [1420/2012]  eta: 0:01:59  lr: 0.000022  loss: 0.024653 (0.045620)  time: 0.198218  data: 0.160232  max mem: 211\n",
      "Epoch: [3]  [1440/2012]  eta: 0:01:55  lr: 0.000022  loss: 0.009660 (0.045480)  time: 0.205283  data: 0.167297  max mem: 211\n",
      "Epoch: [3]  [1460/2012]  eta: 0:01:51  lr: 0.000022  loss: 0.012573 (0.045147)  time: 0.207827  data: 0.169816  max mem: 211\n",
      "Epoch: [3]  [1480/2012]  eta: 0:01:47  lr: 0.000022  loss: 0.016932 (0.044988)  time: 0.195208  data: 0.157199  max mem: 211\n",
      "Epoch: [3]  [1500/2012]  eta: 0:01:43  lr: 0.000022  loss: 0.010769 (0.044799)  time: 0.201042  data: 0.163042  max mem: 211\n",
      "Epoch: [3]  [1520/2012]  eta: 0:01:39  lr: 0.000022  loss: 0.020353 (0.044741)  time: 0.220039  data: 0.182065  max mem: 211\n",
      "Epoch: [3]  [1540/2012]  eta: 0:01:35  lr: 0.000022  loss: 0.021543 (0.044684)  time: 0.215793  data: 0.177807  max mem: 211\n",
      "Epoch: [3]  [1560/2012]  eta: 0:01:31  lr: 0.000022  loss: 0.026773 (0.044803)  time: 0.213796  data: 0.175784  max mem: 211\n",
      "Epoch: [3]  [1580/2012]  eta: 0:01:27  lr: 0.000022  loss: 0.019126 (0.045309)  time: 0.198903  data: 0.160920  max mem: 211\n",
      "Epoch: [3]  [1600/2012]  eta: 0:01:23  lr: 0.000022  loss: 0.020523 (0.045177)  time: 0.203551  data: 0.165554  max mem: 211\n",
      "Epoch: [3]  [1620/2012]  eta: 0:01:19  lr: 0.000022  loss: 0.019959 (0.045063)  time: 0.198212  data: 0.160225  max mem: 211\n",
      "Epoch: [3]  [1640/2012]  eta: 0:01:15  lr: 0.000022  loss: 0.015113 (0.044969)  time: 0.200614  data: 0.162622  max mem: 211\n",
      "Epoch: [3]  [1660/2012]  eta: 0:01:11  lr: 0.000022  loss: 0.016182 (0.044878)  time: 0.203578  data: 0.165558  max mem: 211\n",
      "Epoch: [3]  [1680/2012]  eta: 0:01:07  lr: 0.000022  loss: 0.015114 (0.044691)  time: 0.214122  data: 0.176125  max mem: 211\n",
      "Epoch: [3]  [1700/2012]  eta: 0:01:03  lr: 0.000022  loss: 0.024216 (0.044819)  time: 0.201122  data: 0.163083  max mem: 211\n",
      "Epoch: [3]  [1720/2012]  eta: 0:00:59  lr: 0.000022  loss: 0.011946 (0.044659)  time: 0.202298  data: 0.164299  max mem: 211\n",
      "Epoch: [3]  [1740/2012]  eta: 0:00:55  lr: 0.000022  loss: 0.021985 (0.044796)  time: 0.219398  data: 0.181363  max mem: 211\n",
      "Epoch: [3]  [1760/2012]  eta: 0:00:51  lr: 0.000022  loss: 0.016307 (0.044684)  time: 0.192275  data: 0.154280  max mem: 211\n",
      "Epoch: [3]  [1780/2012]  eta: 0:00:47  lr: 0.000022  loss: 0.021592 (0.044986)  time: 0.213353  data: 0.175352  max mem: 211\n",
      "Epoch: [3]  [1800/2012]  eta: 0:00:43  lr: 0.000022  loss: 0.008741 (0.044811)  time: 0.211586  data: 0.173526  max mem: 211\n",
      "Epoch: [3]  [1820/2012]  eta: 0:00:39  lr: 0.000022  loss: 0.030473 (0.044894)  time: 0.220437  data: 0.182417  max mem: 211\n",
      "Epoch: [3]  [1840/2012]  eta: 0:00:34  lr: 0.000022  loss: 0.022878 (0.045017)  time: 0.224938  data: 0.186916  max mem: 211\n",
      "Epoch: [3]  [1860/2012]  eta: 0:00:30  lr: 0.000022  loss: 0.023762 (0.045053)  time: 0.206787  data: 0.168755  max mem: 211\n",
      "Epoch: [3]  [1880/2012]  eta: 0:00:26  lr: 0.000022  loss: 0.018593 (0.045176)  time: 0.203478  data: 0.165482  max mem: 211\n",
      "Epoch: [3]  [1900/2012]  eta: 0:00:22  lr: 0.000022  loss: 0.016354 (0.044944)  time: 0.214571  data: 0.176569  max mem: 211\n",
      "Epoch: [3]  [1920/2012]  eta: 0:00:18  lr: 0.000022  loss: 0.021433 (0.044772)  time: 0.210522  data: 0.172475  max mem: 211\n",
      "Epoch: [3]  [1940/2012]  eta: 0:00:14  lr: 0.000022  loss: 0.013599 (0.044693)  time: 0.213222  data: 0.175213  max mem: 211\n",
      "Epoch: [3]  [1960/2012]  eta: 0:00:10  lr: 0.000022  loss: 0.017087 (0.044775)  time: 0.206383  data: 0.168338  max mem: 211\n",
      "Epoch: [3]  [1980/2012]  eta: 0:00:06  lr: 0.000022  loss: 0.012742 (0.044771)  time: 0.207963  data: 0.169946  max mem: 211\n",
      "Epoch: [3]  [2000/2012]  eta: 0:00:02  lr: 0.000022  loss: 0.013698 (0.044552)  time: 0.220903  data: 0.182899  max mem: 211\n",
      "Epoch: [3]  [2011/2012]  eta: 0:00:00  lr: 0.000022  loss: 0.013305 (0.044661)  time: 0.206516  data: 0.169857  max mem: 211\n",
      "Epoch: [3] Total time: 0:06:50 (0.204013 s / it)\n",
      "Averaged stats: lr: 0.000022  loss: 0.013305 (0.044661)\n",
      "Test:  [ 0/79]  eta: 0:00:17  loss: 0.003235 (0.003235)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.216395  data: 0.178368  max mem: 211\n",
      "Test:  [20/79]  eta: 0:00:12  loss: 0.058541 (0.118730)  acc1: 93.750000 (96.428571)  acc5: 100.000000 (100.000000)  time: 0.218126  data: 0.180087  max mem: 211\n",
      "Test:  [40/79]  eta: 0:00:08  loss: 0.020452 (0.093495)  acc1: 100.000000 (96.951220)  acc5: 100.000000 (99.847561)  time: 0.241155  data: 0.203135  max mem: 211\n",
      "Test:  [60/79]  eta: 0:00:04  loss: 0.038376 (0.095444)  acc1: 100.000000 (96.721311)  acc5: 100.000000 (99.897541)  time: 0.241195  data: 0.203185  max mem: 211\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 0.022295 (0.088663)  acc1: 100.000000 (97.200000)  acc5: 100.000000 (99.920000)  time: 0.228899  data: 0.192235  max mem: 211\n",
      "Test: Total time: 0:00:18 (0.232538 s / it)\n",
      "* Acc@1 97.200 Acc@5 99.920 loss 0.089\n",
      "Accuracy at epoch 3 of the network on the 79 test images: 97.2%\n",
      "Accuracy at epoch 3 of the network on the 1250 test images: 97.2%\n",
      "Max accuracy so far: 97.28%\n",
      "Epoch: [4]  [   0/2012]  eta: 0:06:14  lr: 0.000006  loss: 0.006414 (0.006414)  time: 0.186117  data: 0.148162  max mem: 211\n",
      "Epoch: [4]  [  20/2012]  eta: 0:07:41  lr: 0.000006  loss: 0.005315 (0.034371)  time: 0.233957  data: 0.195934  max mem: 211\n",
      "Epoch: [4]  [  40/2012]  eta: 0:07:22  lr: 0.000006  loss: 0.011738 (0.042274)  time: 0.217052  data: 0.179046  max mem: 211\n",
      "Epoch: [4]  [  60/2012]  eta: 0:07:08  lr: 0.000006  loss: 0.013222 (0.047060)  time: 0.209401  data: 0.171320  max mem: 211\n",
      "Epoch: [4]  [  80/2012]  eta: 0:06:53  lr: 0.000006  loss: 0.027045 (0.048275)  time: 0.197092  data: 0.159086  max mem: 211\n",
      "Epoch: [4]  [ 100/2012]  eta: 0:06:47  lr: 0.000006  loss: 0.012682 (0.042859)  time: 0.209686  data: 0.171688  max mem: 211\n",
      "Epoch: [4]  [ 120/2012]  eta: 0:06:38  lr: 0.000006  loss: 0.026790 (0.042647)  time: 0.197864  data: 0.159881  max mem: 211\n",
      "Epoch: [4]  [ 140/2012]  eta: 0:06:34  lr: 0.000006  loss: 0.017443 (0.043470)  time: 0.211809  data: 0.173813  max mem: 211\n",
      "Epoch: [4]  [ 160/2012]  eta: 0:06:29  lr: 0.000006  loss: 0.019650 (0.042316)  time: 0.207334  data: 0.169341  max mem: 211\n",
      "Epoch: [4]  [ 180/2012]  eta: 0:06:23  lr: 0.000006  loss: 0.019814 (0.043162)  time: 0.202435  data: 0.164453  max mem: 211\n",
      "Epoch: [4]  [ 200/2012]  eta: 0:06:20  lr: 0.000006  loss: 0.007162 (0.042310)  time: 0.216456  data: 0.178431  max mem: 211\n",
      "Epoch: [4]  [ 220/2012]  eta: 0:06:15  lr: 0.000006  loss: 0.026880 (0.042171)  time: 0.203699  data: 0.165691  max mem: 211\n",
      "Epoch: [4]  [ 240/2012]  eta: 0:06:11  lr: 0.000006  loss: 0.013960 (0.040691)  time: 0.210450  data: 0.172459  max mem: 211\n",
      "Epoch: [4]  [ 260/2012]  eta: 0:06:10  lr: 0.000006  loss: 0.016492 (0.039854)  time: 0.236393  data: 0.198365  max mem: 211\n",
      "Epoch: [4]  [ 280/2012]  eta: 0:06:06  lr: 0.000006  loss: 0.012707 (0.042135)  time: 0.207897  data: 0.169908  max mem: 211\n",
      "Epoch: [4]  [ 300/2012]  eta: 0:06:02  lr: 0.000006  loss: 0.023788 (0.042154)  time: 0.213061  data: 0.175065  max mem: 211\n",
      "Epoch: [4]  [ 320/2012]  eta: 0:05:58  lr: 0.000006  loss: 0.021847 (0.042018)  time: 0.216961  data: 0.178939  max mem: 211\n",
      "Epoch: [4]  [ 340/2012]  eta: 0:05:53  lr: 0.000006  loss: 0.010347 (0.041843)  time: 0.208288  data: 0.170262  max mem: 211\n",
      "Epoch: [4]  [ 360/2012]  eta: 0:05:49  lr: 0.000006  loss: 0.009834 (0.041319)  time: 0.214903  data: 0.176899  max mem: 211\n",
      "Epoch: [4]  [ 380/2012]  eta: 0:05:46  lr: 0.000006  loss: 0.009797 (0.041141)  time: 0.216152  data: 0.178139  max mem: 211\n",
      "Epoch: [4]  [ 400/2012]  eta: 0:05:42  lr: 0.000006  loss: 0.021810 (0.041487)  time: 0.216207  data: 0.178194  max mem: 211\n",
      "Epoch: [4]  [ 420/2012]  eta: 0:05:37  lr: 0.000006  loss: 0.005497 (0.040452)  time: 0.208692  data: 0.170706  max mem: 211\n",
      "Epoch: [4]  [ 440/2012]  eta: 0:05:32  lr: 0.000006  loss: 0.015283 (0.040590)  time: 0.199326  data: 0.161323  max mem: 211\n",
      "Epoch: [4]  [ 460/2012]  eta: 0:05:30  lr: 0.000006  loss: 0.010095 (0.040387)  time: 0.240821  data: 0.202825  max mem: 211\n",
      "Epoch: [4]  [ 480/2012]  eta: 0:05:25  lr: 0.000006  loss: 0.029423 (0.040800)  time: 0.211326  data: 0.173298  max mem: 211\n",
      "Epoch: [4]  [ 500/2012]  eta: 0:05:21  lr: 0.000006  loss: 0.037392 (0.042296)  time: 0.207804  data: 0.169798  max mem: 211\n",
      "Epoch: [4]  [ 520/2012]  eta: 0:05:16  lr: 0.000006  loss: 0.016433 (0.042444)  time: 0.196659  data: 0.158667  max mem: 211\n",
      "Epoch: [4]  [ 540/2012]  eta: 0:05:11  lr: 0.000006  loss: 0.017506 (0.042496)  time: 0.206599  data: 0.168590  max mem: 211\n",
      "Epoch: [4]  [ 560/2012]  eta: 0:05:07  lr: 0.000006  loss: 0.018042 (0.042218)  time: 0.218384  data: 0.180389  max mem: 211\n",
      "Epoch: [4]  [ 580/2012]  eta: 0:05:03  lr: 0.000006  loss: 0.032439 (0.042474)  time: 0.201891  data: 0.163873  max mem: 211\n",
      "Epoch: [4]  [ 600/2012]  eta: 0:04:57  lr: 0.000006  loss: 0.009702 (0.041819)  time: 0.188100  data: 0.150100  max mem: 211\n",
      "Epoch: [4]  [ 620/2012]  eta: 0:04:54  lr: 0.000006  loss: 0.010773 (0.042164)  time: 0.227430  data: 0.189432  max mem: 211\n",
      "Epoch: [4]  [ 640/2012]  eta: 0:04:49  lr: 0.000006  loss: 0.013834 (0.041849)  time: 0.191663  data: 0.153645  max mem: 211\n",
      "Epoch: [4]  [ 660/2012]  eta: 0:04:44  lr: 0.000006  loss: 0.006522 (0.041114)  time: 0.191644  data: 0.153656  max mem: 211\n",
      "Epoch: [4]  [ 680/2012]  eta: 0:04:39  lr: 0.000006  loss: 0.036839 (0.041497)  time: 0.194243  data: 0.156228  max mem: 211\n",
      "Epoch: [4]  [ 700/2012]  eta: 0:04:34  lr: 0.000006  loss: 0.032554 (0.042355)  time: 0.193192  data: 0.155199  max mem: 211\n",
      "Epoch: [4]  [ 720/2012]  eta: 0:04:29  lr: 0.000006  loss: 0.008553 (0.042864)  time: 0.198149  data: 0.160154  max mem: 211\n",
      "Epoch: [4]  [ 740/2012]  eta: 0:04:25  lr: 0.000006  loss: 0.009097 (0.042485)  time: 0.193213  data: 0.155221  max mem: 211\n",
      "Epoch: [4]  [ 760/2012]  eta: 0:04:20  lr: 0.000006  loss: 0.011649 (0.041987)  time: 0.200370  data: 0.162367  max mem: 211\n",
      "Epoch: [4]  [ 780/2012]  eta: 0:04:16  lr: 0.000006  loss: 0.007749 (0.041758)  time: 0.193422  data: 0.155365  max mem: 211\n",
      "Epoch: [4]  [ 800/2012]  eta: 0:04:11  lr: 0.000006  loss: 0.013580 (0.041813)  time: 0.192856  data: 0.154862  max mem: 211\n",
      "Epoch: [4]  [ 820/2012]  eta: 0:04:07  lr: 0.000006  loss: 0.017496 (0.041647)  time: 0.195741  data: 0.157740  max mem: 211\n",
      "Epoch: [4]  [ 840/2012]  eta: 0:04:02  lr: 0.000006  loss: 0.014982 (0.041522)  time: 0.210578  data: 0.172581  max mem: 211\n",
      "Epoch: [4]  [ 860/2012]  eta: 0:03:59  lr: 0.000006  loss: 0.018587 (0.041745)  time: 0.215789  data: 0.177750  max mem: 211\n",
      "Epoch: [4]  [ 880/2012]  eta: 0:03:54  lr: 0.000006  loss: 0.014667 (0.041702)  time: 0.194113  data: 0.156126  max mem: 211\n",
      "Epoch: [4]  [ 900/2012]  eta: 0:03:50  lr: 0.000006  loss: 0.014333 (0.041569)  time: 0.202991  data: 0.164960  max mem: 211\n",
      "Epoch: [4]  [ 920/2012]  eta: 0:03:45  lr: 0.000006  loss: 0.033294 (0.041790)  time: 0.198767  data: 0.160761  max mem: 211\n",
      "Epoch: [4]  [ 940/2012]  eta: 0:03:41  lr: 0.000006  loss: 0.015399 (0.041349)  time: 0.207801  data: 0.169795  max mem: 211\n",
      "Epoch: [4]  [ 960/2012]  eta: 0:03:37  lr: 0.000006  loss: 0.030681 (0.041798)  time: 0.201065  data: 0.163045  max mem: 211\n",
      "Epoch: [4]  [ 980/2012]  eta: 0:03:33  lr: 0.000006  loss: 0.019625 (0.041882)  time: 0.198087  data: 0.160088  max mem: 211\n",
      "Epoch: [4]  [1000/2012]  eta: 0:03:29  lr: 0.000006  loss: 0.010021 (0.041869)  time: 0.214031  data: 0.176017  max mem: 211\n",
      "Epoch: [4]  [1020/2012]  eta: 0:03:24  lr: 0.000006  loss: 0.023442 (0.042440)  time: 0.190525  data: 0.152555  max mem: 211\n",
      "Epoch: [4]  [1040/2012]  eta: 0:03:20  lr: 0.000006  loss: 0.013952 (0.042054)  time: 0.191316  data: 0.153314  max mem: 211\n",
      "Epoch: [4]  [1060/2012]  eta: 0:03:16  lr: 0.000006  loss: 0.024164 (0.041984)  time: 0.204889  data: 0.166904  max mem: 211\n",
      "Epoch: [4]  [1080/2012]  eta: 0:03:12  lr: 0.000006  loss: 0.004565 (0.041804)  time: 0.203237  data: 0.165213  max mem: 211\n",
      "Epoch: [4]  [1100/2012]  eta: 0:03:07  lr: 0.000006  loss: 0.014369 (0.041637)  time: 0.189202  data: 0.151227  max mem: 211\n",
      "Epoch: [4]  [1120/2012]  eta: 0:03:03  lr: 0.000006  loss: 0.009745 (0.042078)  time: 0.202358  data: 0.164360  max mem: 211\n",
      "Epoch: [4]  [1140/2012]  eta: 0:02:59  lr: 0.000006  loss: 0.019327 (0.042283)  time: 0.204470  data: 0.166476  max mem: 211\n",
      "Epoch: [4]  [1160/2012]  eta: 0:02:55  lr: 0.000006  loss: 0.020888 (0.042247)  time: 0.206786  data: 0.168792  max mem: 211\n",
      "Epoch: [4]  [1180/2012]  eta: 0:02:51  lr: 0.000006  loss: 0.011670 (0.041886)  time: 0.220914  data: 0.182896  max mem: 211\n",
      "Epoch: [4]  [1200/2012]  eta: 0:02:47  lr: 0.000006  loss: 0.040163 (0.042093)  time: 0.214952  data: 0.176955  max mem: 211\n",
      "Epoch: [4]  [1220/2012]  eta: 0:02:43  lr: 0.000006  loss: 0.006246 (0.042024)  time: 0.200467  data: 0.162463  max mem: 211\n",
      "Epoch: [4]  [1240/2012]  eta: 0:02:38  lr: 0.000006  loss: 0.016458 (0.041947)  time: 0.192542  data: 0.154554  max mem: 211\n",
      "Epoch: [4]  [1260/2012]  eta: 0:02:34  lr: 0.000006  loss: 0.007633 (0.041821)  time: 0.194624  data: 0.156633  max mem: 211\n",
      "Epoch: [4]  [1280/2012]  eta: 0:02:30  lr: 0.000006  loss: 0.006607 (0.041561)  time: 0.198382  data: 0.160389  max mem: 211\n",
      "Epoch: [4]  [1300/2012]  eta: 0:02:26  lr: 0.000006  loss: 0.007499 (0.041381)  time: 0.195490  data: 0.157492  max mem: 211\n",
      "Epoch: [4]  [1320/2012]  eta: 0:02:22  lr: 0.000006  loss: 0.017435 (0.041269)  time: 0.212363  data: 0.174363  max mem: 211\n",
      "Epoch: [4]  [1340/2012]  eta: 0:02:18  lr: 0.000006  loss: 0.012805 (0.041292)  time: 0.196189  data: 0.158191  max mem: 211\n",
      "Epoch: [4]  [1360/2012]  eta: 0:02:13  lr: 0.000006  loss: 0.013041 (0.041208)  time: 0.197032  data: 0.159013  max mem: 211\n",
      "Epoch: [4]  [1380/2012]  eta: 0:02:09  lr: 0.000006  loss: 0.004672 (0.041227)  time: 0.198008  data: 0.160020  max mem: 211\n",
      "Epoch: [4]  [1400/2012]  eta: 0:02:05  lr: 0.000006  loss: 0.005291 (0.041014)  time: 0.199516  data: 0.161512  max mem: 211\n",
      "Epoch: [4]  [1420/2012]  eta: 0:02:01  lr: 0.000006  loss: 0.027297 (0.041996)  time: 0.193755  data: 0.155758  max mem: 211\n",
      "Epoch: [4]  [1440/2012]  eta: 0:01:57  lr: 0.000006  loss: 0.009428 (0.041881)  time: 0.206679  data: 0.168662  max mem: 211\n",
      "Epoch: [4]  [1460/2012]  eta: 0:01:53  lr: 0.000006  loss: 0.011103 (0.041567)  time: 0.200096  data: 0.162105  max mem: 211\n",
      "Epoch: [4]  [1480/2012]  eta: 0:01:48  lr: 0.000006  loss: 0.018590 (0.041429)  time: 0.193024  data: 0.155012  max mem: 211\n",
      "Epoch: [4]  [1500/2012]  eta: 0:01:44  lr: 0.000006  loss: 0.010714 (0.041261)  time: 0.204086  data: 0.166086  max mem: 211\n",
      "Epoch: [4]  [1520/2012]  eta: 0:01:40  lr: 0.000006  loss: 0.018432 (0.041179)  time: 0.205173  data: 0.167170  max mem: 211\n",
      "Epoch: [4]  [1540/2012]  eta: 0:01:36  lr: 0.000006  loss: 0.019833 (0.041099)  time: 0.203011  data: 0.165017  max mem: 211\n",
      "Epoch: [4]  [1560/2012]  eta: 0:01:32  lr: 0.000006  loss: 0.024206 (0.041189)  time: 0.207028  data: 0.169019  max mem: 211\n",
      "Epoch: [4]  [1580/2012]  eta: 0:01:28  lr: 0.000006  loss: 0.023079 (0.041678)  time: 0.202345  data: 0.164327  max mem: 211\n",
      "Epoch: [4]  [1600/2012]  eta: 0:01:24  lr: 0.000006  loss: 0.020704 (0.041563)  time: 0.205781  data: 0.167791  max mem: 211\n",
      "Epoch: [4]  [1620/2012]  eta: 0:01:20  lr: 0.000006  loss: 0.018352 (0.041434)  time: 0.202728  data: 0.164736  max mem: 211\n",
      "Epoch: [4]  [1640/2012]  eta: 0:01:16  lr: 0.000006  loss: 0.013717 (0.041345)  time: 0.195284  data: 0.157325  max mem: 211\n",
      "Epoch: [4]  [1660/2012]  eta: 0:01:11  lr: 0.000006  loss: 0.016796 (0.041274)  time: 0.194860  data: 0.156863  max mem: 211\n",
      "Epoch: [4]  [1680/2012]  eta: 0:01:07  lr: 0.000006  loss: 0.017644 (0.041080)  time: 0.194908  data: 0.156948  max mem: 211\n",
      "Epoch: [4]  [1700/2012]  eta: 0:01:03  lr: 0.000006  loss: 0.020423 (0.041179)  time: 0.191494  data: 0.153469  max mem: 211\n",
      "Epoch: [4]  [1720/2012]  eta: 0:00:59  lr: 0.000006  loss: 0.012591 (0.041027)  time: 0.196615  data: 0.158620  max mem: 211\n",
      "Epoch: [4]  [1740/2012]  eta: 0:00:55  lr: 0.000006  loss: 0.019112 (0.041156)  time: 0.211772  data: 0.173772  max mem: 211\n",
      "Epoch: [4]  [1760/2012]  eta: 0:00:51  lr: 0.000006  loss: 0.017560 (0.041064)  time: 0.198928  data: 0.160907  max mem: 211\n",
      "Epoch: [4]  [1780/2012]  eta: 0:00:47  lr: 0.000006  loss: 0.019802 (0.041317)  time: 0.226443  data: 0.188433  max mem: 211\n",
      "Epoch: [4]  [1800/2012]  eta: 0:00:43  lr: 0.000006  loss: 0.006916 (0.041155)  time: 0.226959  data: 0.188844  max mem: 211\n",
      "Epoch: [4]  [1820/2012]  eta: 0:00:39  lr: 0.000006  loss: 0.029206 (0.041187)  time: 0.208515  data: 0.170485  max mem: 211\n",
      "Epoch: [4]  [1840/2012]  eta: 0:00:35  lr: 0.000006  loss: 0.024192 (0.041286)  time: 0.206238  data: 0.168237  max mem: 211\n",
      "Epoch: [4]  [1860/2012]  eta: 0:00:31  lr: 0.000006  loss: 0.022889 (0.041312)  time: 0.199992  data: 0.161972  max mem: 211\n",
      "Epoch: [4]  [1880/2012]  eta: 0:00:27  lr: 0.000006  loss: 0.018862 (0.041405)  time: 0.204082  data: 0.166063  max mem: 211\n",
      "Epoch: [4]  [1900/2012]  eta: 0:00:22  lr: 0.000006  loss: 0.014744 (0.041196)  time: 0.201642  data: 0.163647  max mem: 211\n",
      "Epoch: [4]  [1920/2012]  eta: 0:00:18  lr: 0.000006  loss: 0.018854 (0.041034)  time: 0.210580  data: 0.172556  max mem: 211\n",
      "Epoch: [4]  [1940/2012]  eta: 0:00:14  lr: 0.000006  loss: 0.010968 (0.040955)  time: 0.197809  data: 0.159806  max mem: 211\n",
      "Epoch: [4]  [1960/2012]  eta: 0:00:10  lr: 0.000006  loss: 0.012964 (0.041025)  time: 0.201738  data: 0.163716  max mem: 211\n",
      "Epoch: [4]  [1980/2012]  eta: 0:00:06  lr: 0.000006  loss: 0.012732 (0.041045)  time: 0.192466  data: 0.154466  max mem: 211\n",
      "Epoch: [4]  [2000/2012]  eta: 0:00:02  lr: 0.000006  loss: 0.011591 (0.040849)  time: 0.202627  data: 0.164623  max mem: 211\n",
      "Epoch: [4]  [2011/2012]  eta: 0:00:00  lr: 0.000006  loss: 0.012020 (0.040972)  time: 0.190354  data: 0.153708  max mem: 211\n",
      "Epoch: [4] Total time: 0:06:51 (0.204350 s / it)\n",
      "Averaged stats: lr: 0.000006  loss: 0.012020 (0.040972)\n",
      "Test:  [ 0/79]  eta: 0:00:15  loss: 0.003205 (0.003205)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.200919  data: 0.162933  max mem: 211\n",
      "Test:  [20/79]  eta: 0:00:12  loss: 0.065145 (0.115921)  acc1: 93.750000 (96.428571)  acc5: 100.000000 (100.000000)  time: 0.208667  data: 0.170655  max mem: 211\n",
      "Test:  [40/79]  eta: 0:00:08  loss: 0.018752 (0.092547)  acc1: 100.000000 (96.951220)  acc5: 100.000000 (99.847561)  time: 0.234076  data: 0.196060  max mem: 211\n",
      "Test:  [60/79]  eta: 0:00:04  loss: 0.037413 (0.095656)  acc1: 100.000000 (96.721311)  acc5: 100.000000 (99.795082)  time: 0.221817  data: 0.183825  max mem: 211\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 0.021900 (0.089623)  acc1: 100.000000 (97.200000)  acc5: 100.000000 (99.840000)  time: 0.205882  data: 0.169292  max mem: 211\n",
      "Test: Total time: 0:00:17 (0.217983 s / it)\n",
      "* Acc@1 97.200 Acc@5 99.840 loss 0.090\n",
      "Accuracy at epoch 4 of the network on the 79 test images: 97.2%\n",
      "Accuracy at epoch 4 of the network on the 1250 test images: 97.2%\n",
      "Max accuracy so far: 97.28%\n",
      "Training of the supervised linear classifier on frozen features completed.\n",
      "Top-1 test accuracy: 97.3\n",
      "Finished Training, saving to 25_classes/clean.pt\n"
     ]
    }
   ],
   "source": [
    "for attack, loaders in loader_dict.items():\n",
    "    if not attack == \"clean\":\n",
    "        continue\n",
    "        \n",
    "    # Tensorboard summary writer\n",
    "    TB_LOGS_PATH = Path(TB_LOGS_BASE_PATH, 'adversarial_classifier', version, attack)\n",
    "    writer = SummaryWriter(TB_LOGS_PATH)\n",
    "    np.set_printoptions(precision=4)\n",
    "    \n",
    "    # Initialise classifier\n",
    "    adv_linear_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                         num_labels=len(CLASS_SUBSET))\n",
    "    adv_linear_classifier = adv_linear_classifier.cuda()\n",
    "\n",
    "    # Metric logger path\n",
    "    LOG_PATH = Path(LOG_BASE_PATH, 'adv_classifier', version, attack)\n",
    "    if not os.path.isdir(LOG_PATH):\n",
    "        os.makedirs(LOG_PATH)\n",
    "    \n",
    "    # train\n",
    "    print(f\"Training classifier for {attack}\")\n",
    "    loggers = train(model, \n",
    "                    adv_linear_classifier, \n",
    "                    loaders[\"train\"], \n",
    "                    loaders[\"validation\"], \n",
    "                    log_dir=LOG_PATH, \n",
    "                    tensor_dir=None, \n",
    "                    optimizer=None, \n",
    "                    adversarial_attack=None, \n",
    "                    epochs=5, \n",
    "                    val_freq=1, \n",
    "                    batch_size=16,  \n",
    "                    lr=0.001, \n",
    "                    to_restore = {\"epoch\": 0, \"best_acc\": 0.}, \n",
    "                    n=4, \n",
    "                    avgpool_patchtokens=False, \n",
    "                    show_image=False, \n",
    "                    writer=writer\n",
    "                   )\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    # Save adversarial Classifier\n",
    "    save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version)\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    save_file = f\"{attack}.pt\"\n",
    "    torch.save(adv_linear_classifier.state_dict(), str(save_path) + \"/\" + save_file)\n",
    "    print(f'Finished Training, saving to {str(save_path.stem)}/{save_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on all adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## evaluating adv_classifier trained on pgd ##################################################\n",
      "-------------------------------------------------- pgd dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.071055 (0.071055)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.084277  data: 0.046243  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.116650 (0.146619)  acc1: 93.750000 (95.833333)  acc5: 100.000000 (100.000000)  time: 0.077803  data: 0.045770  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.025480 (0.119763)  acc1: 100.000000 (96.200000)  acc5: 100.000000 (100.000000)  time: 0.074005  data: 0.044119  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.076153 s / it)\n",
      "* Acc@1 96.200 Acc@5 100.000 loss 0.120\n",
      "-------------------------------------------------- cw dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.103200 (0.103200)  acc1: 93.750000 (93.750000)  acc5: 100.000000 (100.000000)  time: 0.075449  data: 0.044555  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.056992 (0.081956)  acc1: 100.000000 (96.726190)  acc5: 100.000000 (100.000000)  time: 0.075031  data: 0.044152  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.032778 (0.073914)  acc1: 100.000000 (97.000000)  acc5: 100.000000 (100.000000)  time: 0.072439  data: 0.042601  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.073478 s / it)\n",
      "* Acc@1 97.000 Acc@5 100.000 loss 0.074\n",
      "-------------------------------------------------- fgsm dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.077921 (0.077921)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.077381  data: 0.046619  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.136815 (0.172462)  acc1: 93.750000 (95.238095)  acc5: 100.000000 (99.404762)  time: 0.076882  data: 0.045991  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.076273 (0.147212)  acc1: 100.000000 (95.400000)  acc5: 100.000000 (99.600000)  time: 0.074478  data: 0.044590  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.075333 s / it)\n",
      "* Acc@1 95.400 Acc@5 99.600 loss 0.147\n",
      "-------------------------------------------------- clean dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:04  loss: 0.048554 (0.048554)  acc1: 93.750000 (93.750000)  acc5: 100.000000 (100.000000)  time: 0.150954  data: 0.120108  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:02  loss: 0.030302 (0.055625)  acc1: 100.000000 (98.214286)  acc5: 100.000000 (100.000000)  time: 0.181885  data: 0.147449  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.015333 (0.055477)  acc1: 100.000000 (97.800000)  acc5: 100.000000 (100.000000)  time: 0.187909  data: 0.151181  max mem: 210\n",
      "Test: Total time: 0:00:05 (0.179996 s / it)\n",
      "* Acc@1 97.800 Acc@5 100.000 loss 0.055\n",
      "################################################## evaluating adv_classifier trained on cw ##################################################\n",
      "-------------------------------------------------- pgd dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.293546 (0.293546)  acc1: 93.750000 (93.750000)  acc5: 100.000000 (100.000000)  time: 0.090787  data: 0.052718  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.340758 (0.350815)  acc1: 87.500000 (89.880952)  acc5: 100.000000 (99.107143)  time: 0.077522  data: 0.044525  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.253744 (0.344048)  acc1: 87.500000 (89.200000)  acc5: 100.000000 (98.800000)  time: 0.073408  data: 0.043453  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.076030 s / it)\n",
      "* Acc@1 89.200 Acc@5 98.800 loss 0.344\n",
      "-------------------------------------------------- cw dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.008331 (0.008331)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.075333  data: 0.044494  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.006340 (0.042553)  acc1: 100.000000 (98.511905)  acc5: 100.000000 (100.000000)  time: 0.075036  data: 0.044153  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.005765 (0.036177)  acc1: 100.000000 (98.800000)  acc5: 100.000000 (100.000000)  time: 0.072839  data: 0.042961  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.073902 s / it)\n",
      "* Acc@1 98.800 Acc@5 100.000 loss 0.036\n",
      "-------------------------------------------------- fgsm dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.329130 (0.329130)  acc1: 93.750000 (93.750000)  acc5: 100.000000 (100.000000)  time: 0.076973  data: 0.046097  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.292435 (0.284882)  acc1: 87.500000 (90.773810)  acc5: 100.000000 (99.702381)  time: 0.076282  data: 0.045377  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.175427 (0.282836)  acc1: 87.500000 (91.400000)  acc5: 100.000000 (99.600000)  time: 0.073473  data: 0.043634  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.074512 s / it)\n",
      "* Acc@1 91.400 Acc@5 99.600 loss 0.283\n",
      "-------------------------------------------------- clean dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:04  loss: 0.002522 (0.002522)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.149918  data: 0.119128  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:02  loss: 0.005166 (0.042606)  acc1: 100.000000 (98.214286)  acc5: 100.000000 (100.000000)  time: 0.180815  data: 0.146091  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.003175 (0.034004)  acc1: 100.000000 (98.800000)  acc5: 100.000000 (100.000000)  time: 0.188224  data: 0.151607  max mem: 210\n",
      "Test: Total time: 0:00:05 (0.179418 s / it)\n",
      "* Acc@1 98.800 Acc@5 100.000 loss 0.034\n",
      "################################################## evaluating adv_classifier trained on fgsm ##################################################\n",
      "-------------------------------------------------- pgd dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.022597 (0.022597)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.082840  data: 0.044836  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.120698 (0.196613)  acc1: 93.750000 (92.857143)  acc5: 100.000000 (99.404762)  time: 0.078938  data: 0.045563  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.080945 (0.159094)  acc1: 93.750000 (94.400000)  acc5: 100.000000 (99.600000)  time: 0.073832  data: 0.043955  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.076510 s / it)\n",
      "* Acc@1 94.400 Acc@5 99.600 loss 0.159\n",
      "-------------------------------------------------- cw dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.003324 (0.003324)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.082255  data: 0.051244  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.006645 (0.058137)  acc1: 100.000000 (97.619048)  acc5: 100.000000 (100.000000)  time: 0.075784  data: 0.044874  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.006645 (0.044611)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (100.000000)  time: 0.072862  data: 0.042994  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.074071 s / it)\n",
      "* Acc@1 98.000 Acc@5 100.000 loss 0.045\n",
      "-------------------------------------------------- fgsm dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.064689 (0.064689)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.076843  data: 0.045950  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.106439 (0.155034)  acc1: 93.750000 (94.642857)  acc5: 100.000000 (99.702381)  time: 0.076493  data: 0.045608  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.057308 (0.127367)  acc1: 100.000000 (95.400000)  acc5: 100.000000 (99.800000)  time: 0.073193  data: 0.043269  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.074713 s / it)\n",
      "* Acc@1 95.400 Acc@5 99.800 loss 0.127\n",
      "-------------------------------------------------- clean dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:05  loss: 0.001721 (0.001721)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.160493  data: 0.129679  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:02  loss: 0.006587 (0.052068)  acc1: 100.000000 (98.214286)  acc5: 100.000000 (100.000000)  time: 0.179954  data: 0.145125  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.003975 (0.042037)  acc1: 100.000000 (98.600000)  acc5: 100.000000 (100.000000)  time: 0.187840  data: 0.151197  max mem: 210\n",
      "Test: Total time: 0:00:05 (0.179235 s / it)\n",
      "* Acc@1 98.600 Acc@5 100.000 loss 0.042\n",
      "################################################## evaluating adv_classifier trained on clean ##################################################\n",
      "-------------------------------------------------- pgd dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.146107 (0.146107)  acc1: 93.750000 (93.750000)  acc5: 100.000000 (100.000000)  time: 0.083623  data: 0.045637  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.395486 (0.414238)  acc1: 81.250000 (86.011905)  acc5: 100.000000 (98.214286)  time: 0.077276  data: 0.044997  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.305276 (0.384768)  acc1: 87.500000 (87.600000)  acc5: 100.000000 (98.600000)  time: 0.073187  data: 0.043372  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.075388 s / it)\n",
      "* Acc@1 87.600 Acc@5 98.600 loss 0.385\n",
      "-------------------------------------------------- cw dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.002326 (0.002326)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.075002  data: 0.044173  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.009360 (0.054829)  acc1: 100.000000 (97.619048)  acc5: 100.000000 (100.000000)  time: 0.075662  data: 0.044768  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.007733 (0.044564)  acc1: 100.000000 (98.200000)  acc5: 100.000000 (100.000000)  time: 0.073440  data: 0.043547  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.074153 s / it)\n",
      "* Acc@1 98.200 Acc@5 100.000 loss 0.045\n",
      "-------------------------------------------------- fgsm dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.278169 (0.278169)  acc1: 87.500000 (87.500000)  acc5: 100.000000 (100.000000)  time: 0.076789  data: 0.046072  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.359974 (0.355525)  acc1: 87.500000 (88.690476)  acc5: 100.000000 (98.511905)  time: 0.076386  data: 0.045499  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.234210 (0.330447)  acc1: 87.500000 (89.400000)  acc5: 100.000000 (99.000000)  time: 0.073771  data: 0.043889  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.074823 s / it)\n",
      "* Acc@1 89.400 Acc@5 99.000 loss 0.330\n",
      "-------------------------------------------------- clean dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:04  loss: 0.000532 (0.000532)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.151387  data: 0.120537  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:02  loss: 0.005250 (0.041903)  acc1: 100.000000 (99.107143)  acc5: 100.000000 (100.000000)  time: 0.181749  data: 0.147300  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.003331 (0.031725)  acc1: 100.000000 (99.200000)  acc5: 100.000000 (100.000000)  time: 0.188872  data: 0.152186  max mem: 210\n",
      "Test: Total time: 0:00:05 (0.180406 s / it)\n",
      "* Acc@1 99.200 Acc@5 100.000 loss 0.032\n"
     ]
    }
   ],
   "source": [
    "attacks = [x for x in loader_dict.keys()]\n",
    "\n",
    "for attack in attacks:\n",
    "    \n",
    "    print(\"#\"*50 + f\" evaluating adv_classifier trained on {attack} \" + \"#\"*50)\n",
    "    adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                             num_labels=len(CLASS_SUBSET))\n",
    "    adv_classifier.to(device)\n",
    "    \n",
    "    save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version)\n",
    "    save_file = f\"{attack}.pt\"\n",
    "    adv_classifier.load_state_dict(torch.load(str(save_path) + \"/\" + save_file))\n",
    "    \n",
    "    for applied_attack in attacks:\n",
    "        \n",
    "#         if applied_attack == attack:\n",
    "#             continue\n",
    "        \n",
    "        print(\"-\"*50 + f\" {applied_attack} dataset \" + \"-\"*50)\n",
    "        logger_dict, logger = validate_network(model, \n",
    "                                               adv_classifier, \n",
    "                                               loader_dict[applied_attack][\"validation\"], \n",
    "                                               tensor_dir=None, \n",
    "                                               adversarial_attack=None, \n",
    "                                               n=4, \n",
    "                                               avgpool=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on newly generated attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = [x for x in loader_dict.keys()]\n",
    "for attack in attacks:\n",
    "    print(\"#\"*50 + f\" evaluating adv_classifier trained on {attack} \" + \"#\"*50)\n",
    "    adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                             num_labels=len(CLASS_SUBSET))\n",
    "    adv_classifier.to(device)\n",
    "    \n",
    "    save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version)\n",
    "    save_file = f\"{attack}.pt\"\n",
    "    adv_classifier.load_state_dict(torch.load(str(save_path) + \"/\" + save_file))\n",
    "    \n",
    "    vits = ViTWrapper(model, adv_classifier, transform=None)\n",
    "\n",
    "    for applied_attack in attacks:\n",
    "        \n",
    "        if applied_attack == \"pgd\":\n",
    "            ev_attack = PGD(vits, eps=0.3, alpha=6/255, steps=15)\n",
    "        elif applied_attack == \"cw\":\n",
    "            ev_attack = CW(vits, c=10, lr=0.003, steps=30)\n",
    "        elif applied_attack == \"fgsm\":\n",
    "            ev_attack = FGSM(vits, eps=0.03)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        print(\"-\"*50 + f\" applying attack: {ev_attack} \" + \"-\"*50)\n",
    "        logger_dict, logger = validate_network(model, \n",
    "                                               adv_classifier, \n",
    "                                               loader_dict[\"clean\"][\"validation\"], \n",
    "                                               tensor_dir=None, \n",
    "                                               adversarial_attack=ev_attack,\n",
    "                                               n=4, \n",
    "                                               avgpool=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on full pipeline with post-hoc as multiplexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load posthoc\n",
    "name=\"cw\" # cw, fgsm, pgd\n",
    "posthoc = LinearBC(1536)\n",
    "posthoc.to(device)\n",
    "posthoc.load_state_dict(torch.load(f\"/cluster/scratch/mmathys/dl_data/posthoc-models/{name}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best adv_classifier\n",
    "name=\"cw\" # cw, fgsm, pgd\n",
    "adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                  num_labels=len(CLASS_SUBSET))\n",
    "adv_classifier.to(device)\n",
    "adv_classifier.load_state_dict(torch.load(f\"/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/{version}/{name}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load clean_classifier\n",
    "name=\"clean\"\n",
    "clean_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                    num_labels=len(CLASS_SUBSET))\n",
    "clean_classifier.to(device)\n",
    "clean_classifier.load_state_dict(torch.load(f\"/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/{version}/{name}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## Validating on pgd ##################################################\n",
      "Test:  [ 0/32]  eta: 0:00:03  loss: 0.293546 (0.293546)  acc1: 93.750000 (93.750000)  acc5: 100.000000 (100.000000)  time: 0.094445  data: 0.045961  max mem: 210\n",
      "Test:  [ 5/32]  eta: 0:00:02  loss: 0.293546 (0.300796)  acc1: 93.750000 (92.708333)  acc5: 100.000000 (98.958333)  time: 0.089142  data: 0.047695  max mem: 210\n",
      "Test:  [10/32]  eta: 0:00:01  loss: 0.340758 (0.394105)  acc1: 87.500000 (89.204545)  acc5: 100.000000 (98.295455)  time: 0.084818  data: 0.046690  max mem: 210\n",
      "Test:  [15/32]  eta: 0:00:01  loss: 0.340758 (0.366558)  acc1: 87.500000 (89.453125)  acc5: 100.000000 (98.828125)  time: 0.082831  data: 0.046439  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.340758 (0.350821)  acc1: 87.500000 (89.880952)  acc5: 100.000000 (99.107143)  time: 0.081223  data: 0.046395  max mem: 210\n",
      "Test:  [25/32]  eta: 0:00:00  loss: 0.350162 (0.347260)  acc1: 87.500000 (89.182692)  acc5: 100.000000 (99.278846)  time: 0.079296  data: 0.046327  max mem: 210\n",
      "Test:  [30/32]  eta: 0:00:00  loss: 0.260566 (0.348918)  acc1: 87.500000 (89.112903)  acc5: 100.000000 (98.790323)  time: 0.079136  data: 0.046575  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.253744 (0.344008)  acc1: 87.500000 (89.200000)  acc5: 100.000000 (98.800000)  time: 0.076424  data: 0.044914  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.079541 s / it)\n",
      "* Acc@1 89.200 Acc@5 98.800 loss 0.344\n",
      "################################################## Validating on cw ##################################################\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.008331 (0.008331)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.076863  data: 0.044297  max mem: 210\n",
      "Test:  [ 5/32]  eta: 0:00:02  loss: 0.007401 (0.061974)  acc1: 100.000000 (97.916667)  acc5: 100.000000 (100.000000)  time: 0.077484  data: 0.044928  max mem: 210\n",
      "Test:  [10/32]  eta: 0:00:01  loss: 0.008331 (0.061961)  acc1: 100.000000 (97.727273)  acc5: 100.000000 (100.000000)  time: 0.078580  data: 0.045966  max mem: 210\n",
      "Test:  [15/32]  eta: 0:00:01  loss: 0.005765 (0.051089)  acc1: 100.000000 (98.046875)  acc5: 100.000000 (100.000000)  time: 0.078286  data: 0.045684  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.006332 (0.042544)  acc1: 100.000000 (98.511905)  acc5: 100.000000 (100.000000)  time: 0.078723  data: 0.046127  max mem: 210\n",
      "Test:  [25/32]  eta: 0:00:00  loss: 0.006332 (0.036801)  acc1: 100.000000 (98.798077)  acc5: 100.000000 (100.000000)  time: 0.078702  data: 0.046077  max mem: 210\n",
      "Test:  [30/32]  eta: 0:00:00  loss: 0.006332 (0.037214)  acc1: 100.000000 (98.790323)  acc5: 100.000000 (100.000000)  time: 0.078057  data: 0.045437  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.005765 (0.036127)  acc1: 100.000000 (98.800000)  acc5: 100.000000 (100.000000)  time: 0.075341  data: 0.043793  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.076687 s / it)\n",
      "* Acc@1 98.800 Acc@5 100.000 loss 0.036\n",
      "################################################## Validating on fgsm ##################################################\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.328904 (0.328904)  acc1: 93.750000 (93.750000)  acc5: 100.000000 (100.000000)  time: 0.078955  data: 0.046333  max mem: 210\n",
      "Test:  [ 5/32]  eta: 0:00:02  loss: 0.187655 (0.238639)  acc1: 93.750000 (93.750000)  acc5: 100.000000 (98.958333)  time: 0.078993  data: 0.046230  max mem: 210\n",
      "Test:  [10/32]  eta: 0:00:01  loss: 0.328904 (0.331967)  acc1: 93.750000 (90.340909)  acc5: 100.000000 (99.431818)  time: 0.079443  data: 0.046715  max mem: 210\n",
      "Test:  [15/32]  eta: 0:00:01  loss: 0.292435 (0.287733)  acc1: 93.750000 (91.406250)  acc5: 100.000000 (99.609375)  time: 0.079375  data: 0.046646  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.292435 (0.283795)  acc1: 87.500000 (90.773810)  acc5: 100.000000 (99.702381)  time: 0.079271  data: 0.046552  max mem: 210\n",
      "Test:  [25/32]  eta: 0:00:00  loss: 0.171930 (0.253462)  acc1: 87.500000 (92.067308)  acc5: 100.000000 (99.759615)  time: 0.079145  data: 0.046454  max mem: 210\n",
      "Test:  [30/32]  eta: 0:00:00  loss: 0.171930 (0.266301)  acc1: 87.500000 (91.532258)  acc5: 100.000000 (99.596774)  time: 0.078862  data: 0.046153  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.171930 (0.282068)  acc1: 87.500000 (91.400000)  acc5: 100.000000 (99.600000)  time: 0.076084  data: 0.044468  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.077492 s / it)\n",
      "* Acc@1 91.400 Acc@5 99.600 loss 0.282\n",
      "################################################## Validating on clean ##################################################\n",
      "Test:  [ 0/32]  eta: 0:00:04  loss: 0.000532 (0.000532)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.152390  data: 0.119861  max mem: 210\n",
      "Test:  [ 5/32]  eta: 0:00:04  loss: 0.005199 (0.032065)  acc1: 100.000000 (98.958333)  acc5: 100.000000 (100.000000)  time: 0.149750  data: 0.117079  max mem: 210\n",
      "Test:  [10/32]  eta: 0:00:03  loss: 0.011935 (0.059865)  acc1: 100.000000 (98.295455)  acc5: 100.000000 (100.000000)  time: 0.158273  data: 0.125209  max mem: 210\n",
      "Test:  [15/32]  eta: 0:00:02  loss: 0.005250 (0.053141)  acc1: 100.000000 (98.437500)  acc5: 100.000000 (100.000000)  time: 0.175093  data: 0.139804  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:02  loss: 0.005250 (0.050448)  acc1: 100.000000 (98.511905)  acc5: 100.000000 (100.000000)  time: 0.184551  data: 0.147927  max mem: 210\n",
      "Test:  [25/32]  eta: 0:00:01  loss: 0.004268 (0.042116)  acc1: 100.000000 (98.798077)  acc5: 100.000000 (100.000000)  time: 0.203971  data: 0.165509  max mem: 210\n",
      "Test:  [30/32]  eta: 0:00:00  loss: 0.004038 (0.038527)  acc1: 100.000000 (98.790323)  acc5: 100.000000 (100.000000)  time: 0.206184  data: 0.166087  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.003798 (0.037411)  acc1: 100.000000 (98.800000)  acc5: 100.000000 (100.000000)  time: 0.193887  data: 0.155182  max mem: 210\n",
      "Test: Total time: 0:00:05 (0.184745 s / it)\n",
      "* Acc@1 98.800 Acc@5 100.000 loss 0.037\n"
     ]
    }
   ],
   "source": [
    "# Perform validation on clean dataset\n",
    "for attack, loaders in loader_dict.items():\n",
    "    print(\"#\"*50 + f\" Validating on {attack} \" + \"#\"*50)\n",
    "    log_dict, logger = validate_multihead_network(model, \n",
    "                                                  posthoc,\n",
    "                                                  adv_classifier,\n",
    "                                                  clean_classifier,\n",
    "                                                  loader_dict[attack][\"validation\"], \n",
    "                                                  tensor_dir=None, \n",
    "                                                  adversarial_attack=None, \n",
    "                                                  n=4, \n",
    "                                                  avgpool=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black Box on Multihead model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
