{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "# from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino, ViTWrapper\n",
    "from src.model.data import *\n",
    "from src.model.train import *\n",
    "from src.model.multihead_model import *\n",
    "\n",
    "from torchattacks import *\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "MAX_PATH = Path('/','cluster', 'scratch', 'mmathys', 'dl_data')\n",
    "\n",
    "LOG_BASE_PATH = Path(MAX_PATH, 'logs')\n",
    "\n",
    "# DamageNet\n",
    "DN_PATH = Path(DATA_PATH, 'damageNet')\n",
    "DN_LABEL_PATH = Path(DN_PATH, 'val_damagenet.txt')\n",
    "DN_IMAGES_PATH = Path(DN_PATH, 'images')\n",
    "\n",
    "# Image Net\n",
    "ORI_PATH = Path(DATA_PATH, 'ori_data')\n",
    "CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "\n",
    "VAL_PATH = Path(ORI_PATH, 'validation')\n",
    "VAL_IMAGES_PATH = Path(VAL_PATH,'images')\n",
    "VAL_LABEL_PATH = Path(VAL_PATH, 'correct_labels.txt')\n",
    "\n",
    "TRAIN_PATH = Path(ORI_PATH, 'train')\n",
    "TRAIN_IMAGES_PATH = Path(TRAIN_PATH,'images')\n",
    "TRAIN_LABEL_PATH = Path(TRAIN_PATH, 'correct_labels.txt')\n",
    "\n",
    "# Adversarial Data\n",
    "# PGD\n",
    "PGD_TRAIN_PATH = Path(MAX_PATH, 'adversarial_data', 'pgd_06', 'train')\n",
    "PGD_TRAIN_IMAGES_PATH = Path(PGD_TRAIN_PATH,'images')\n",
    "PGD_TRAIN_LABEL_PATH = Path(PGD_TRAIN_PATH, 'labels.txt')\n",
    "\n",
    "PGD_VAL_PATH = Path(MAX_PATH, 'adversarial_data', 'pgd_06', 'validation')\n",
    "PGD_VAL_IMAGES_PATH = Path(PGD_VAL_PATH,'images')\n",
    "PGD_VAL_LABEL_PATH = Path(PGD_VAL_PATH, 'labels.txt')\n",
    "\n",
    "# CW\n",
    "CW_TRAIN_PATH = Path(MAX_PATH, 'adversarial_data', 'cw', 'train')\n",
    "CW_TRAIN_IMAGES_PATH = Path(CW_TRAIN_PATH,'images')\n",
    "CW_TRAIN_LABEL_PATH = Path(CW_TRAIN_PATH, 'labels.txt')\n",
    "\n",
    "CW_VAL_PATH = Path(MAX_PATH, 'adversarial_data', 'cw', 'validation')\n",
    "CW_VAL_IMAGES_PATH = Path(CW_VAL_PATH,'images')\n",
    "CW_VAL_LABEL_PATH = Path(CW_VAL_PATH, 'labels.txt')\n",
    "\n",
    "# FGSM\n",
    "FGSM_TRAIN_PATH = Path(MAX_PATH, 'adversarial_data', 'fgsm_06', 'train')\n",
    "FGSM_TRAIN_IMAGES_PATH = Path(FGSM_TRAIN_PATH,'images')\n",
    "FGSM_TRAIN_LABEL_PATH = Path(FGSM_TRAIN_PATH, 'labels.txt')\n",
    "\n",
    "FGSM_VAL_PATH = Path(MAX_PATH, 'adversarial_data', 'fgsm_06', 'validation')\n",
    "FGSM_VAL_IMAGES_PATH = Path(FGSM_VAL_PATH,'images')\n",
    "FGSM_VAL_LABEL_PATH = Path(FGSM_VAL_PATH, 'labels.txt')\n",
    "\n",
    "\n",
    "# TB LOG\n",
    "TB_LOGS_BASE_PATH = Path(LOG_BASE_PATH, 'tb_logs')\n",
    "\n",
    "\n",
    "# Model save path\n",
    "ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH = Path(MAX_PATH, 'adversarial_data', 'adv_classifiers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If CLASS_SUBSET is specified, INDEX_SUBSET will be ignored. Set CLASS_SUBSET=None if you want to use indexes.\n",
    "# INDEX_SUBSET = get_random_indexes(number_of_images = 50000, n_samples=1000)\n",
    "# CLASS_SUBSET = get_random_classes(number_of_classes = 25, min_rand_class = 1, max_rand_class = 1001)\n",
    "\n",
    "\n",
    "CLASS_SUBSET = np.load(CLASS_SUBSET_PATH)\n",
    "\n",
    "# CLASS_SUBSET = CLASS_SUBSET[:25] \n",
    "\n",
    "NUM_WORKERS= 0\n",
    "PIN_MEMORY=True\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import DINO\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights.\n"
     ]
    }
   ],
   "source": [
    "model, linear_classifier = get_dino()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to set the correct transformation\n",
    "# encoder\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit([i for i in CLASS_SUBSET])\n",
    "\n",
    "\n",
    "# PGD\n",
    "pgd_train_dataset = AdvTrainingImageDataset(PGD_TRAIN_IMAGES_PATH, \n",
    "                                            PGD_TRAIN_LABEL_PATH, \n",
    "                                            ORIGINAL_TRANSFORM, \n",
    "                                            CLASS_SUBSET, \n",
    "                                            index_subset=None,\n",
    "                                            label_encoder=label_encoder\n",
    "                                           )\n",
    "\n",
    "pgd_val_dataset = AdvTrainingImageDataset(PGD_VAL_IMAGES_PATH, \n",
    "                                          PGD_VAL_LABEL_PATH, \n",
    "                                          ORIGINAL_TRANSFORM, \n",
    "                                          CLASS_SUBSET, \n",
    "                                          index_subset=None,\n",
    "                                          label_encoder=label_encoder\n",
    "                                         )\n",
    "\n",
    "\n",
    "pgd_train_loader = DataLoader(pgd_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "pgd_val_loader = DataLoader(pgd_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY, \n",
    "                            shuffle=False)\n",
    "\n",
    "# CW\n",
    "cw_train_dataset = AdvTrainingImageDataset(CW_TRAIN_IMAGES_PATH, \n",
    "                                            CW_TRAIN_LABEL_PATH, \n",
    "                                            ORIGINAL_TRANSFORM, \n",
    "                                            CLASS_SUBSET, \n",
    "                                            index_subset=None,\n",
    "                                           label_encoder=label_encoder\n",
    "                                          )\n",
    "\n",
    "cw_val_dataset = AdvTrainingImageDataset(CW_VAL_IMAGES_PATH, \n",
    "                                          CW_VAL_LABEL_PATH, \n",
    "                                          ORIGINAL_TRANSFORM, \n",
    "                                          CLASS_SUBSET, \n",
    "                                          index_subset=None,\n",
    "                                         label_encoder=label_encoder\n",
    "                                        )\n",
    "\n",
    "cw_train_loader = DataLoader(cw_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "cw_val_loader = DataLoader(cw_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY, \n",
    "                            shuffle=False)\n",
    "\n",
    "# FGSM\n",
    "fgsm_train_dataset = AdvTrainingImageDataset(FGSM_TRAIN_IMAGES_PATH, \n",
    "                                            FGSM_TRAIN_LABEL_PATH, \n",
    "                                            ORIGINAL_TRANSFORM, \n",
    "                                            CLASS_SUBSET, \n",
    "                                            index_subset=None,\n",
    "                                            label_encoder=label_encoder\n",
    "                                            )\n",
    "\n",
    "fgsm_val_dataset = AdvTrainingImageDataset(FGSM_VAL_IMAGES_PATH, \n",
    "                                          FGSM_VAL_LABEL_PATH, \n",
    "                                          ORIGINAL_TRANSFORM, \n",
    "                                          CLASS_SUBSET, \n",
    "                                          index_subset=None,\n",
    "                                          label_encoder=label_encoder\n",
    "                                          )\n",
    "\n",
    "fgsm_train_loader = DataLoader(fgsm_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "fgsm_val_loader = DataLoader(fgsm_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY, \n",
    "                            shuffle=False)\n",
    "\n",
    "# Clean\n",
    "clean_train_dataset = ImageDataset(TRAIN_IMAGES_PATH, \n",
    "                                  TRAIN_LABEL_PATH, \n",
    "                                  ORIGINAL_TRANSFORM,\n",
    "                                  CLASS_SUBSET, \n",
    "                                  index_subset=None, \n",
    "                                  label_encoder=label_encoder)\n",
    "\n",
    "clean_val_dataset = ImageDataset(VAL_IMAGES_PATH, \n",
    "                                  VAL_LABEL_PATH, \n",
    "                                  ORIGINAL_TRANSFORM,\n",
    "                                  CLASS_SUBSET, \n",
    "                                  index_subset=None, \n",
    "                                  label_encoder=label_encoder)\n",
    "\n",
    "clean_train_loader = DataLoader(clean_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "clean_val_loader = DataLoader(clean_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY,\n",
    "                            shuffle=False)\n",
    "\n",
    "\n",
    "loader_dict = {\n",
    "    \"pgd\" : {\n",
    "        \"train\" : pgd_train_loader,\n",
    "        \"validation\" : pgd_val_loader,\n",
    "    },\n",
    "    \"cw\" : {\n",
    "        \"train\" : cw_train_loader,\n",
    "        \"validation\" : cw_val_loader,\n",
    "    }, \n",
    "    \"fgsm\" : {\n",
    "        \"train\" : fgsm_train_loader,\n",
    "        \"validation\" : fgsm_val_loader,\n",
    "    },\n",
    "    \"clean\" : {\n",
    "        \"train\" : clean_train_loader,\n",
    "        \"validation\" : clean_val_loader,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2b05dece0910>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_dict[\"pgd\"][\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '25_classes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(dim, num_labels)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # linear layer\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Binary Classifier\n",
    "class LinearBC(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(LinearBC,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train various classifiers on all adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier for clean\n",
      "Epoch: [0]  [   0/2012]  eta: 0:07:42  lr: 0.000063  loss: 4.070327 (4.070327)  time: 0.229926  data: 0.190067  max mem: 210\n",
      "Epoch: [0]  [  20/2012]  eta: 0:07:45  lr: 0.000063  loss: 3.364912 (3.396890)  time: 0.233918  data: 0.195904  max mem: 211\n",
      "Epoch: [0]  [  40/2012]  eta: 0:07:25  lr: 0.000063  loss: 1.061000 (2.291339)  time: 0.217225  data: 0.179242  max mem: 211\n",
      "Epoch: [0]  [  60/2012]  eta: 0:07:07  lr: 0.000063  loss: 0.637545 (1.764758)  time: 0.206038  data: 0.168053  max mem: 211\n",
      "Epoch: [0]  [  80/2012]  eta: 0:06:56  lr: 0.000063  loss: 0.350218 (1.438930)  time: 0.204567  data: 0.166585  max mem: 211\n",
      "Epoch: [0]  [ 100/2012]  eta: 0:06:53  lr: 0.000063  loss: 0.228549 (1.213146)  time: 0.219306  data: 0.181325  max mem: 211\n",
      "Epoch: [0]  [ 120/2012]  eta: 0:06:44  lr: 0.000063  loss: 0.336601 (1.068877)  time: 0.201224  data: 0.163227  max mem: 211\n",
      "Epoch: [0]  [ 140/2012]  eta: 0:06:38  lr: 0.000063  loss: 0.299253 (0.958927)  time: 0.205310  data: 0.167319  max mem: 211\n",
      "Epoch: [0]  [ 160/2012]  eta: 0:06:32  lr: 0.000063  loss: 0.195812 (0.869668)  time: 0.206228  data: 0.168224  max mem: 211\n",
      "Epoch: [0]  [ 180/2012]  eta: 0:06:26  lr: 0.000063  loss: 0.226854 (0.799860)  time: 0.202910  data: 0.164928  max mem: 211\n",
      "Epoch: [0]  [ 200/2012]  eta: 0:06:22  lr: 0.000063  loss: 0.139051 (0.740949)  time: 0.214776  data: 0.176781  max mem: 211\n",
      "Epoch: [0]  [ 220/2012]  eta: 0:06:17  lr: 0.000063  loss: 0.162165 (0.691239)  time: 0.206873  data: 0.168886  max mem: 211\n",
      "Epoch: [0]  [ 240/2012]  eta: 0:06:14  lr: 0.000063  loss: 0.167063 (0.649869)  time: 0.214100  data: 0.176095  max mem: 211\n",
      "Epoch: [0]  [ 260/2012]  eta: 0:06:12  lr: 0.000063  loss: 0.118953 (0.612220)  time: 0.230956  data: 0.192966  max mem: 211\n",
      "Epoch: [0]  [ 280/2012]  eta: 0:06:09  lr: 0.000063  loss: 0.127655 (0.582747)  time: 0.222709  data: 0.184724  max mem: 211\n",
      "Epoch: [0]  [ 300/2012]  eta: 0:06:04  lr: 0.000063  loss: 0.129688 (0.556522)  time: 0.204795  data: 0.166793  max mem: 211\n",
      "Epoch: [0]  [ 320/2012]  eta: 0:06:00  lr: 0.000063  loss: 0.135158 (0.532325)  time: 0.216414  data: 0.178387  max mem: 211\n",
      "Epoch: [0]  [ 340/2012]  eta: 0:05:55  lr: 0.000063  loss: 0.118780 (0.510678)  time: 0.207661  data: 0.169679  max mem: 211\n",
      "Epoch: [0]  [ 360/2012]  eta: 0:05:52  lr: 0.000063  loss: 0.091949 (0.490932)  time: 0.224432  data: 0.186434  max mem: 211\n",
      "Epoch: [0]  [ 380/2012]  eta: 0:05:47  lr: 0.000063  loss: 0.078607 (0.471724)  time: 0.208532  data: 0.170555  max mem: 211\n",
      "Epoch: [0]  [ 400/2012]  eta: 0:05:44  lr: 0.000063  loss: 0.108062 (0.456755)  time: 0.220157  data: 0.182157  max mem: 211\n",
      "Epoch: [0]  [ 420/2012]  eta: 0:05:39  lr: 0.000063  loss: 0.067917 (0.439278)  time: 0.215441  data: 0.177434  max mem: 211\n",
      "Epoch: [0]  [ 440/2012]  eta: 0:05:35  lr: 0.000063  loss: 0.099343 (0.426036)  time: 0.204762  data: 0.166798  max mem: 211\n",
      "Epoch: [0]  [ 460/2012]  eta: 0:05:32  lr: 0.000063  loss: 0.078168 (0.412705)  time: 0.232913  data: 0.194899  max mem: 211\n",
      "Epoch: [0]  [ 480/2012]  eta: 0:05:27  lr: 0.000063  loss: 0.150450 (0.402988)  time: 0.207671  data: 0.169696  max mem: 211\n",
      "Epoch: [0]  [ 500/2012]  eta: 0:05:22  lr: 0.000063  loss: 0.151450 (0.395237)  time: 0.210866  data: 0.172888  max mem: 211\n",
      "Epoch: [0]  [ 520/2012]  eta: 0:05:18  lr: 0.000063  loss: 0.128976 (0.385937)  time: 0.206888  data: 0.168919  max mem: 211\n",
      "Epoch: [0]  [ 540/2012]  eta: 0:05:14  lr: 0.000063  loss: 0.124808 (0.377958)  time: 0.214089  data: 0.176113  max mem: 211\n",
      "Epoch: [0]  [ 560/2012]  eta: 0:05:09  lr: 0.000063  loss: 0.138064 (0.369559)  time: 0.209944  data: 0.171979  max mem: 211\n",
      "Epoch: [0]  [ 580/2012]  eta: 0:05:05  lr: 0.000063  loss: 0.132235 (0.362447)  time: 0.214133  data: 0.176139  max mem: 211\n",
      "Epoch: [0]  [ 600/2012]  eta: 0:05:00  lr: 0.000063  loss: 0.093887 (0.353493)  time: 0.200961  data: 0.162980  max mem: 211\n",
      "Epoch: [0]  [ 620/2012]  eta: 0:04:56  lr: 0.000063  loss: 0.043076 (0.346282)  time: 0.219013  data: 0.181041  max mem: 211\n",
      "Epoch: [0]  [ 640/2012]  eta: 0:04:51  lr: 0.000063  loss: 0.073704 (0.339323)  time: 0.202091  data: 0.164110  max mem: 211\n",
      "Epoch: [0]  [ 660/2012]  eta: 0:04:47  lr: 0.000063  loss: 0.040423 (0.331416)  time: 0.201593  data: 0.163577  max mem: 211\n",
      "Epoch: [0]  [ 680/2012]  eta: 0:04:42  lr: 0.000063  loss: 0.101072 (0.326596)  time: 0.201799  data: 0.163805  max mem: 211\n",
      "Epoch: [0]  [ 700/2012]  eta: 0:04:37  lr: 0.000063  loss: 0.131953 (0.322795)  time: 0.199083  data: 0.161112  max mem: 211\n",
      "Epoch: [0]  [ 720/2012]  eta: 0:04:33  lr: 0.000063  loss: 0.058156 (0.317496)  time: 0.203884  data: 0.165875  max mem: 211\n",
      "Epoch: [0]  [ 740/2012]  eta: 0:04:28  lr: 0.000063  loss: 0.060551 (0.311783)  time: 0.208844  data: 0.170831  max mem: 211\n",
      "Epoch: [0]  [ 760/2012]  eta: 0:04:24  lr: 0.000063  loss: 0.057030 (0.305628)  time: 0.211424  data: 0.173440  max mem: 211\n",
      "Epoch: [0]  [ 780/2012]  eta: 0:04:20  lr: 0.000063  loss: 0.075977 (0.300429)  time: 0.200342  data: 0.162357  max mem: 211\n",
      "Epoch: [0]  [ 800/2012]  eta: 0:04:15  lr: 0.000063  loss: 0.060701 (0.296487)  time: 0.199489  data: 0.161500  max mem: 211\n",
      "Epoch: [0]  [ 820/2012]  eta: 0:04:11  lr: 0.000063  loss: 0.082563 (0.292057)  time: 0.203666  data: 0.165680  max mem: 211\n",
      "Epoch: [0]  [ 840/2012]  eta: 0:04:07  lr: 0.000063  loss: 0.075978 (0.287932)  time: 0.215629  data: 0.177627  max mem: 211\n",
      "Epoch: [0]  [ 860/2012]  eta: 0:04:02  lr: 0.000063  loss: 0.076062 (0.284201)  time: 0.211518  data: 0.173535  max mem: 211\n",
      "Epoch: [0]  [ 880/2012]  eta: 0:03:58  lr: 0.000063  loss: 0.064339 (0.280630)  time: 0.202424  data: 0.164406  max mem: 211\n",
      "Epoch: [0]  [ 900/2012]  eta: 0:03:54  lr: 0.000063  loss: 0.072014 (0.276993)  time: 0.210984  data: 0.172980  max mem: 211\n",
      "Epoch: [0]  [ 920/2012]  eta: 0:03:49  lr: 0.000063  loss: 0.172770 (0.274769)  time: 0.208230  data: 0.170252  max mem: 211\n",
      "Epoch: [0]  [ 940/2012]  eta: 0:03:45  lr: 0.000063  loss: 0.036555 (0.270481)  time: 0.217014  data: 0.179012  max mem: 211\n",
      "Epoch: [0]  [ 960/2012]  eta: 0:03:41  lr: 0.000063  loss: 0.090589 (0.267478)  time: 0.208742  data: 0.170716  max mem: 211\n",
      "Epoch: [0]  [ 980/2012]  eta: 0:03:37  lr: 0.000063  loss: 0.060420 (0.264369)  time: 0.205102  data: 0.167119  max mem: 211\n",
      "Epoch: [0]  [1000/2012]  eta: 0:03:33  lr: 0.000063  loss: 0.058771 (0.261550)  time: 0.221639  data: 0.183654  max mem: 211\n",
      "Epoch: [0]  [1020/2012]  eta: 0:03:28  lr: 0.000063  loss: 0.074138 (0.259679)  time: 0.201045  data: 0.163070  max mem: 211\n",
      "Epoch: [0]  [1040/2012]  eta: 0:03:24  lr: 0.000063  loss: 0.057566 (0.256100)  time: 0.206973  data: 0.168978  max mem: 211\n",
      "Epoch: [0]  [1060/2012]  eta: 0:03:20  lr: 0.000063  loss: 0.055645 (0.253725)  time: 0.209791  data: 0.171794  max mem: 211\n",
      "Epoch: [0]  [1080/2012]  eta: 0:03:16  lr: 0.000063  loss: 0.018430 (0.250309)  time: 0.205425  data: 0.167449  max mem: 211\n",
      "Epoch: [0]  [1100/2012]  eta: 0:03:11  lr: 0.000063  loss: 0.075526 (0.247530)  time: 0.204919  data: 0.166935  max mem: 211\n",
      "Epoch: [0]  [1120/2012]  eta: 0:03:07  lr: 0.000063  loss: 0.030650 (0.245465)  time: 0.204751  data: 0.166752  max mem: 211\n",
      "Epoch: [0]  [1140/2012]  eta: 0:03:03  lr: 0.000063  loss: 0.116762 (0.243616)  time: 0.212199  data: 0.174200  max mem: 211\n",
      "Epoch: [0]  [1160/2012]  eta: 0:02:59  lr: 0.000063  loss: 0.067476 (0.241258)  time: 0.218021  data: 0.180032  max mem: 211\n"
     ]
    }
   ],
   "source": [
    "for attack, loaders in loader_dict.items():\n",
    "    if not attack == \"clean\":\n",
    "        continue\n",
    "        \n",
    "    # Tensorboard summary writer\n",
    "    TB_LOGS_PATH = Path(TB_LOGS_BASE_PATH, 'adversarial_classifier', version, attack)\n",
    "    writer = SummaryWriter(TB_LOGS_PATH)\n",
    "    np.set_printoptions(precision=4)\n",
    "    \n",
    "    # Initialise classifier\n",
    "    adv_linear_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                         num_labels=len(CLASS_SUBSET))\n",
    "    adv_linear_classifier = adv_linear_classifier.cuda()\n",
    "\n",
    "    # Metric logger path\n",
    "    LOG_PATH = Path(LOG_BASE_PATH, 'adv_classifier', version, attack)\n",
    "    if not os.path.isdir(LOG_PATH):\n",
    "        os.makedirs(LOG_PATH)\n",
    "    \n",
    "    # train\n",
    "    print(f\"Training classifier for {attack}\")\n",
    "    loggers = train(model, \n",
    "                    adv_linear_classifier, \n",
    "                    loaders[\"train\"], \n",
    "                    loaders[\"validation\"], \n",
    "                    log_dir=LOG_PATH, \n",
    "                    tensor_dir=None, \n",
    "                    optimizer=None, \n",
    "                    adversarial_attack=None, \n",
    "                    epochs=5, \n",
    "                    val_freq=1, \n",
    "                    batch_size=16,  \n",
    "                    lr=0.001, \n",
    "                    to_restore = {\"epoch\": 0, \"best_acc\": 0.}, \n",
    "                    n=4, \n",
    "                    avgpool_patchtokens=False, \n",
    "                    show_image=False, \n",
    "                    writer=writer\n",
    "                   )\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    # Save adversarial Classifier\n",
    "    save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version)\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    save_file = f\"{attack}.pt\"\n",
    "    torch.save(adv_linear_classifier.state_dict(), str(save_path) + \"/\" + save_file)\n",
    "    print(f'Finished Training, saving to {str(save_path.stem)}/{save_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on all adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## evaluating adv_classifier trained on pgd ##################################################\n",
      "-------------------------------------------------- pgd dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.071055 (0.071055)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.084277  data: 0.046243  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.116650 (0.146619)  acc1: 93.750000 (95.833333)  acc5: 100.000000 (100.000000)  time: 0.077803  data: 0.045770  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.025480 (0.119763)  acc1: 100.000000 (96.200000)  acc5: 100.000000 (100.000000)  time: 0.074005  data: 0.044119  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.076153 s / it)\n",
      "* Acc@1 96.200 Acc@5 100.000 loss 0.120\n",
      "-------------------------------------------------- cw dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.103200 (0.103200)  acc1: 93.750000 (93.750000)  acc5: 100.000000 (100.000000)  time: 0.075449  data: 0.044555  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.056992 (0.081956)  acc1: 100.000000 (96.726190)  acc5: 100.000000 (100.000000)  time: 0.075031  data: 0.044152  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.032778 (0.073914)  acc1: 100.000000 (97.000000)  acc5: 100.000000 (100.000000)  time: 0.072439  data: 0.042601  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.073478 s / it)\n",
      "* Acc@1 97.000 Acc@5 100.000 loss 0.074\n",
      "-------------------------------------------------- fgsm dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.077921 (0.077921)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.077381  data: 0.046619  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.136815 (0.172462)  acc1: 93.750000 (95.238095)  acc5: 100.000000 (99.404762)  time: 0.076882  data: 0.045991  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.076273 (0.147212)  acc1: 100.000000 (95.400000)  acc5: 100.000000 (99.600000)  time: 0.074478  data: 0.044590  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.075333 s / it)\n",
      "* Acc@1 95.400 Acc@5 99.600 loss 0.147\n",
      "-------------------------------------------------- clean dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:04  loss: 0.048554 (0.048554)  acc1: 93.750000 (93.750000)  acc5: 100.000000 (100.000000)  time: 0.150954  data: 0.120108  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:02  loss: 0.030302 (0.055625)  acc1: 100.000000 (98.214286)  acc5: 100.000000 (100.000000)  time: 0.181885  data: 0.147449  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.015333 (0.055477)  acc1: 100.000000 (97.800000)  acc5: 100.000000 (100.000000)  time: 0.187909  data: 0.151181  max mem: 210\n",
      "Test: Total time: 0:00:05 (0.179996 s / it)\n",
      "* Acc@1 97.800 Acc@5 100.000 loss 0.055\n",
      "################################################## evaluating adv_classifier trained on cw ##################################################\n",
      "-------------------------------------------------- pgd dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.293546 (0.293546)  acc1: 93.750000 (93.750000)  acc5: 100.000000 (100.000000)  time: 0.090787  data: 0.052718  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.340758 (0.350815)  acc1: 87.500000 (89.880952)  acc5: 100.000000 (99.107143)  time: 0.077522  data: 0.044525  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.253744 (0.344048)  acc1: 87.500000 (89.200000)  acc5: 100.000000 (98.800000)  time: 0.073408  data: 0.043453  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.076030 s / it)\n",
      "* Acc@1 89.200 Acc@5 98.800 loss 0.344\n",
      "-------------------------------------------------- cw dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.008331 (0.008331)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.075333  data: 0.044494  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.006340 (0.042553)  acc1: 100.000000 (98.511905)  acc5: 100.000000 (100.000000)  time: 0.075036  data: 0.044153  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.005765 (0.036177)  acc1: 100.000000 (98.800000)  acc5: 100.000000 (100.000000)  time: 0.072839  data: 0.042961  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.073902 s / it)\n",
      "* Acc@1 98.800 Acc@5 100.000 loss 0.036\n",
      "-------------------------------------------------- fgsm dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.329130 (0.329130)  acc1: 93.750000 (93.750000)  acc5: 100.000000 (100.000000)  time: 0.076973  data: 0.046097  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.292435 (0.284882)  acc1: 87.500000 (90.773810)  acc5: 100.000000 (99.702381)  time: 0.076282  data: 0.045377  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.175427 (0.282836)  acc1: 87.500000 (91.400000)  acc5: 100.000000 (99.600000)  time: 0.073473  data: 0.043634  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.074512 s / it)\n",
      "* Acc@1 91.400 Acc@5 99.600 loss 0.283\n",
      "-------------------------------------------------- clean dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:04  loss: 0.002522 (0.002522)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.149918  data: 0.119128  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:02  loss: 0.005166 (0.042606)  acc1: 100.000000 (98.214286)  acc5: 100.000000 (100.000000)  time: 0.180815  data: 0.146091  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.003175 (0.034004)  acc1: 100.000000 (98.800000)  acc5: 100.000000 (100.000000)  time: 0.188224  data: 0.151607  max mem: 210\n",
      "Test: Total time: 0:00:05 (0.179418 s / it)\n",
      "* Acc@1 98.800 Acc@5 100.000 loss 0.034\n",
      "################################################## evaluating adv_classifier trained on fgsm ##################################################\n",
      "-------------------------------------------------- pgd dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.022597 (0.022597)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.082840  data: 0.044836  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.120698 (0.196613)  acc1: 93.750000 (92.857143)  acc5: 100.000000 (99.404762)  time: 0.078938  data: 0.045563  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.080945 (0.159094)  acc1: 93.750000 (94.400000)  acc5: 100.000000 (99.600000)  time: 0.073832  data: 0.043955  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.076510 s / it)\n",
      "* Acc@1 94.400 Acc@5 99.600 loss 0.159\n",
      "-------------------------------------------------- cw dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.003324 (0.003324)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.082255  data: 0.051244  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.006645 (0.058137)  acc1: 100.000000 (97.619048)  acc5: 100.000000 (100.000000)  time: 0.075784  data: 0.044874  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.006645 (0.044611)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (100.000000)  time: 0.072862  data: 0.042994  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.074071 s / it)\n",
      "* Acc@1 98.000 Acc@5 100.000 loss 0.045\n",
      "-------------------------------------------------- fgsm dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.064689 (0.064689)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.076843  data: 0.045950  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.106439 (0.155034)  acc1: 93.750000 (94.642857)  acc5: 100.000000 (99.702381)  time: 0.076493  data: 0.045608  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.057308 (0.127367)  acc1: 100.000000 (95.400000)  acc5: 100.000000 (99.800000)  time: 0.073193  data: 0.043269  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.074713 s / it)\n",
      "* Acc@1 95.400 Acc@5 99.800 loss 0.127\n",
      "-------------------------------------------------- clean dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:05  loss: 0.001721 (0.001721)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.160493  data: 0.129679  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:02  loss: 0.006587 (0.052068)  acc1: 100.000000 (98.214286)  acc5: 100.000000 (100.000000)  time: 0.179954  data: 0.145125  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.003975 (0.042037)  acc1: 100.000000 (98.600000)  acc5: 100.000000 (100.000000)  time: 0.187840  data: 0.151197  max mem: 210\n",
      "Test: Total time: 0:00:05 (0.179235 s / it)\n",
      "* Acc@1 98.600 Acc@5 100.000 loss 0.042\n",
      "################################################## evaluating adv_classifier trained on clean ##################################################\n",
      "-------------------------------------------------- pgd dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.146107 (0.146107)  acc1: 93.750000 (93.750000)  acc5: 100.000000 (100.000000)  time: 0.083623  data: 0.045637  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.395486 (0.414238)  acc1: 81.250000 (86.011905)  acc5: 100.000000 (98.214286)  time: 0.077276  data: 0.044997  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.305276 (0.384768)  acc1: 87.500000 (87.600000)  acc5: 100.000000 (98.600000)  time: 0.073187  data: 0.043372  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.075388 s / it)\n",
      "* Acc@1 87.600 Acc@5 98.600 loss 0.385\n",
      "-------------------------------------------------- cw dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.002326 (0.002326)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.075002  data: 0.044173  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.009360 (0.054829)  acc1: 100.000000 (97.619048)  acc5: 100.000000 (100.000000)  time: 0.075662  data: 0.044768  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.007733 (0.044564)  acc1: 100.000000 (98.200000)  acc5: 100.000000 (100.000000)  time: 0.073440  data: 0.043547  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.074153 s / it)\n",
      "* Acc@1 98.200 Acc@5 100.000 loss 0.045\n",
      "-------------------------------------------------- fgsm dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.278169 (0.278169)  acc1: 87.500000 (87.500000)  acc5: 100.000000 (100.000000)  time: 0.076789  data: 0.046072  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.359974 (0.355525)  acc1: 87.500000 (88.690476)  acc5: 100.000000 (98.511905)  time: 0.076386  data: 0.045499  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.234210 (0.330447)  acc1: 87.500000 (89.400000)  acc5: 100.000000 (99.000000)  time: 0.073771  data: 0.043889  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.074823 s / it)\n",
      "* Acc@1 89.400 Acc@5 99.000 loss 0.330\n",
      "-------------------------------------------------- clean dataset --------------------------------------------------\n",
      "Test:  [ 0/32]  eta: 0:00:04  loss: 0.000532 (0.000532)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.151387  data: 0.120537  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:02  loss: 0.005250 (0.041903)  acc1: 100.000000 (99.107143)  acc5: 100.000000 (100.000000)  time: 0.181749  data: 0.147300  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.003331 (0.031725)  acc1: 100.000000 (99.200000)  acc5: 100.000000 (100.000000)  time: 0.188872  data: 0.152186  max mem: 210\n",
      "Test: Total time: 0:00:05 (0.180406 s / it)\n",
      "* Acc@1 99.200 Acc@5 100.000 loss 0.032\n"
     ]
    }
   ],
   "source": [
    "attacks = [x for x in loader_dict.keys()]\n",
    "\n",
    "for attack in attacks:\n",
    "    \n",
    "    print(\"#\"*50 + f\" evaluating adv_classifier trained on {attack} \" + \"#\"*50)\n",
    "    adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                             num_labels=len(CLASS_SUBSET))\n",
    "    adv_classifier.to(device)\n",
    "    \n",
    "    save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version)\n",
    "    save_file = f\"{attack}.pt\"\n",
    "    adv_classifier.load_state_dict(torch.load(str(save_path) + \"/\" + save_file))\n",
    "    \n",
    "    for applied_attack in attacks:\n",
    "        \n",
    "#         if applied_attack == attack:\n",
    "#             continue\n",
    "        \n",
    "        print(\"-\"*50 + f\" {applied_attack} dataset \" + \"-\"*50)\n",
    "        logger_dict, logger = validate_network(model, \n",
    "                                               adv_classifier, \n",
    "                                               loader_dict[applied_attack][\"validation\"], \n",
    "                                               tensor_dir=None, \n",
    "                                               adversarial_attack=None, \n",
    "                                               n=4, \n",
    "                                               avgpool=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on newly generated attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = [x for x in loader_dict.keys()]\n",
    "for attack in attacks:\n",
    "    print(\"#\"*50 + f\" evaluating adv_classifier trained on {attack} \" + \"#\"*50)\n",
    "    adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                             num_labels=len(CLASS_SUBSET))\n",
    "    adv_classifier.to(device)\n",
    "    \n",
    "    save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version)\n",
    "    save_file = f\"{attack}.pt\"\n",
    "    adv_classifier.load_state_dict(torch.load(str(save_path) + \"/\" + save_file))\n",
    "    \n",
    "    vits = ViTWrapper(model, adv_classifier, transform=None)\n",
    "\n",
    "    for applied_attack in attacks:\n",
    "        \n",
    "        if applied_attack == \"pgd\":\n",
    "            ev_attack = PGD(vits, eps=0.3, alpha=6/255, steps=15)\n",
    "        elif applied_attack == \"cw\":\n",
    "            ev_attack = CW(vits, c=10, lr=0.003, steps=30)\n",
    "        elif applied_attack == \"fgsm\":\n",
    "            ev_attack = FGSM(vits, eps=0.03)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        print(\"-\"*50 + f\" applying attack: {ev_attack} \" + \"-\"*50)\n",
    "        logger_dict, logger = validate_network(model, \n",
    "                                               adv_classifier, \n",
    "                                               loader_dict[\"clean\"][\"validation\"], \n",
    "                                               tensor_dir=None, \n",
    "                                               adversarial_attack=ev_attack,\n",
    "                                               n=4, \n",
    "                                               avgpool=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on full pipeline with post-hoc as multiplexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load posthoc\n",
    "name=\"cw\" # cw, fgsm, pgd\n",
    "posthoc = LinearBC(1536)\n",
    "posthoc.to(device)\n",
    "posthoc.load_state_dict(torch.load(f\"/cluster/scratch/mmathys/dl_data/posthoc-models/{name}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best adv_classifier\n",
    "name=\"cw\" # cw, fgsm, pgd\n",
    "adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                  num_labels=len(CLASS_SUBSET))\n",
    "adv_classifier.to(device)\n",
    "adv_classifier.load_state_dict(torch.load(f\"/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/{version}/{name}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load clean_classifier\n",
    "name=\"clean\"\n",
    "clean_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                    num_labels=len(CLASS_SUBSET))\n",
    "clean_classifier.to(device)\n",
    "clean_classifier.load_state_dict(torch.load(f\"/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/{version}/{name}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## Validating on pgd ##################################################\n",
      "Test:  [ 0/32]  eta: 0:00:03  loss: 0.293546 (0.293546)  acc1: 93.750000 (93.750000)  acc5: 100.000000 (100.000000)  time: 0.094445  data: 0.045961  max mem: 210\n",
      "Test:  [ 5/32]  eta: 0:00:02  loss: 0.293546 (0.300796)  acc1: 93.750000 (92.708333)  acc5: 100.000000 (98.958333)  time: 0.089142  data: 0.047695  max mem: 210\n",
      "Test:  [10/32]  eta: 0:00:01  loss: 0.340758 (0.394105)  acc1: 87.500000 (89.204545)  acc5: 100.000000 (98.295455)  time: 0.084818  data: 0.046690  max mem: 210\n",
      "Test:  [15/32]  eta: 0:00:01  loss: 0.340758 (0.366558)  acc1: 87.500000 (89.453125)  acc5: 100.000000 (98.828125)  time: 0.082831  data: 0.046439  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.340758 (0.350821)  acc1: 87.500000 (89.880952)  acc5: 100.000000 (99.107143)  time: 0.081223  data: 0.046395  max mem: 210\n",
      "Test:  [25/32]  eta: 0:00:00  loss: 0.350162 (0.347260)  acc1: 87.500000 (89.182692)  acc5: 100.000000 (99.278846)  time: 0.079296  data: 0.046327  max mem: 210\n",
      "Test:  [30/32]  eta: 0:00:00  loss: 0.260566 (0.348918)  acc1: 87.500000 (89.112903)  acc5: 100.000000 (98.790323)  time: 0.079136  data: 0.046575  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.253744 (0.344008)  acc1: 87.500000 (89.200000)  acc5: 100.000000 (98.800000)  time: 0.076424  data: 0.044914  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.079541 s / it)\n",
      "* Acc@1 89.200 Acc@5 98.800 loss 0.344\n",
      "################################################## Validating on cw ##################################################\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.008331 (0.008331)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.076863  data: 0.044297  max mem: 210\n",
      "Test:  [ 5/32]  eta: 0:00:02  loss: 0.007401 (0.061974)  acc1: 100.000000 (97.916667)  acc5: 100.000000 (100.000000)  time: 0.077484  data: 0.044928  max mem: 210\n",
      "Test:  [10/32]  eta: 0:00:01  loss: 0.008331 (0.061961)  acc1: 100.000000 (97.727273)  acc5: 100.000000 (100.000000)  time: 0.078580  data: 0.045966  max mem: 210\n",
      "Test:  [15/32]  eta: 0:00:01  loss: 0.005765 (0.051089)  acc1: 100.000000 (98.046875)  acc5: 100.000000 (100.000000)  time: 0.078286  data: 0.045684  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.006332 (0.042544)  acc1: 100.000000 (98.511905)  acc5: 100.000000 (100.000000)  time: 0.078723  data: 0.046127  max mem: 210\n",
      "Test:  [25/32]  eta: 0:00:00  loss: 0.006332 (0.036801)  acc1: 100.000000 (98.798077)  acc5: 100.000000 (100.000000)  time: 0.078702  data: 0.046077  max mem: 210\n",
      "Test:  [30/32]  eta: 0:00:00  loss: 0.006332 (0.037214)  acc1: 100.000000 (98.790323)  acc5: 100.000000 (100.000000)  time: 0.078057  data: 0.045437  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.005765 (0.036127)  acc1: 100.000000 (98.800000)  acc5: 100.000000 (100.000000)  time: 0.075341  data: 0.043793  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.076687 s / it)\n",
      "* Acc@1 98.800 Acc@5 100.000 loss 0.036\n",
      "################################################## Validating on fgsm ##################################################\n",
      "Test:  [ 0/32]  eta: 0:00:02  loss: 0.328904 (0.328904)  acc1: 93.750000 (93.750000)  acc5: 100.000000 (100.000000)  time: 0.078955  data: 0.046333  max mem: 210\n",
      "Test:  [ 5/32]  eta: 0:00:02  loss: 0.187655 (0.238639)  acc1: 93.750000 (93.750000)  acc5: 100.000000 (98.958333)  time: 0.078993  data: 0.046230  max mem: 210\n",
      "Test:  [10/32]  eta: 0:00:01  loss: 0.328904 (0.331967)  acc1: 93.750000 (90.340909)  acc5: 100.000000 (99.431818)  time: 0.079443  data: 0.046715  max mem: 210\n",
      "Test:  [15/32]  eta: 0:00:01  loss: 0.292435 (0.287733)  acc1: 93.750000 (91.406250)  acc5: 100.000000 (99.609375)  time: 0.079375  data: 0.046646  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:00  loss: 0.292435 (0.283795)  acc1: 87.500000 (90.773810)  acc5: 100.000000 (99.702381)  time: 0.079271  data: 0.046552  max mem: 210\n",
      "Test:  [25/32]  eta: 0:00:00  loss: 0.171930 (0.253462)  acc1: 87.500000 (92.067308)  acc5: 100.000000 (99.759615)  time: 0.079145  data: 0.046454  max mem: 210\n",
      "Test:  [30/32]  eta: 0:00:00  loss: 0.171930 (0.266301)  acc1: 87.500000 (91.532258)  acc5: 100.000000 (99.596774)  time: 0.078862  data: 0.046153  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.171930 (0.282068)  acc1: 87.500000 (91.400000)  acc5: 100.000000 (99.600000)  time: 0.076084  data: 0.044468  max mem: 210\n",
      "Test: Total time: 0:00:02 (0.077492 s / it)\n",
      "* Acc@1 91.400 Acc@5 99.600 loss 0.282\n",
      "################################################## Validating on clean ##################################################\n",
      "Test:  [ 0/32]  eta: 0:00:04  loss: 0.000532 (0.000532)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.152390  data: 0.119861  max mem: 210\n",
      "Test:  [ 5/32]  eta: 0:00:04  loss: 0.005199 (0.032065)  acc1: 100.000000 (98.958333)  acc5: 100.000000 (100.000000)  time: 0.149750  data: 0.117079  max mem: 210\n",
      "Test:  [10/32]  eta: 0:00:03  loss: 0.011935 (0.059865)  acc1: 100.000000 (98.295455)  acc5: 100.000000 (100.000000)  time: 0.158273  data: 0.125209  max mem: 210\n",
      "Test:  [15/32]  eta: 0:00:02  loss: 0.005250 (0.053141)  acc1: 100.000000 (98.437500)  acc5: 100.000000 (100.000000)  time: 0.175093  data: 0.139804  max mem: 210\n",
      "Test:  [20/32]  eta: 0:00:02  loss: 0.005250 (0.050448)  acc1: 100.000000 (98.511905)  acc5: 100.000000 (100.000000)  time: 0.184551  data: 0.147927  max mem: 210\n",
      "Test:  [25/32]  eta: 0:00:01  loss: 0.004268 (0.042116)  acc1: 100.000000 (98.798077)  acc5: 100.000000 (100.000000)  time: 0.203971  data: 0.165509  max mem: 210\n",
      "Test:  [30/32]  eta: 0:00:00  loss: 0.004038 (0.038527)  acc1: 100.000000 (98.790323)  acc5: 100.000000 (100.000000)  time: 0.206184  data: 0.166087  max mem: 210\n",
      "Test:  [31/32]  eta: 0:00:00  loss: 0.003798 (0.037411)  acc1: 100.000000 (98.800000)  acc5: 100.000000 (100.000000)  time: 0.193887  data: 0.155182  max mem: 210\n",
      "Test: Total time: 0:00:05 (0.184745 s / it)\n",
      "* Acc@1 98.800 Acc@5 100.000 loss 0.037\n"
     ]
    }
   ],
   "source": [
    "# Perform validation on clean dataset\n",
    "for attack, loaders in loader_dict.items():\n",
    "    print(\"#\"*50 + f\" Validating on {attack} \" + \"#\"*50)\n",
    "    log_dict, logger = validate_multihead_network(model, \n",
    "                                                  posthoc,\n",
    "                                                  adv_classifier,\n",
    "                                                  clean_classifier,\n",
    "                                                  loader_dict[attack][\"validation\"], \n",
    "                                                  tensor_dir=None, \n",
    "                                                  adversarial_attack=None, \n",
    "                                                  n=4, \n",
    "                                                  avgpool=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black Box on Multihead model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
