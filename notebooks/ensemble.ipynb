{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "# from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino, ViTWrapper\n",
    "from src.model.data import *\n",
    "from src.model.train import *\n",
    "from src.model.multihead_model import *\n",
    "\n",
    "from torchattacks import *\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "MAX_PATH = Path('/','cluster', 'scratch', 'mmathys', 'dl_data')\n",
    "\n",
    "LOG_BASE_PATH = Path(MAX_PATH, 'logs')\n",
    "\n",
    "# DamageNet\n",
    "DN_PATH = Path(DATA_PATH, 'damageNet')\n",
    "DN_LABEL_PATH = Path(DN_PATH, 'val_damagenet.txt')\n",
    "DN_IMAGES_PATH = Path(DN_PATH, 'images')\n",
    "\n",
    "# Image Net\n",
    "ORI_PATH = Path(DATA_PATH, 'ori_data')\n",
    "CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "\n",
    "VAL_PATH = Path(ORI_PATH, 'validation')\n",
    "VAL_IMAGES_PATH = Path(VAL_PATH,'images')\n",
    "VAL_LABEL_PATH = Path(VAL_PATH, 'correct_labels.txt')\n",
    "\n",
    "TRAIN_PATH = Path(ORI_PATH, 'train')\n",
    "TRAIN_IMAGES_PATH = Path(TRAIN_PATH,'images')\n",
    "TRAIN_LABEL_PATH = Path(TRAIN_PATH, 'correct_labels.txt')\n",
    "\n",
    "# Adversarial Data\n",
    "# PGD\n",
    "PGD_TRAIN_PATH = Path(MAX_PATH, 'adversarial_data', 'pgd_06', 'train')\n",
    "PGD_TRAIN_IMAGES_PATH = Path(PGD_TRAIN_PATH,'images')\n",
    "PGD_TRAIN_LABEL_PATH = Path(PGD_TRAIN_PATH, 'labels.txt')\n",
    "\n",
    "PGD_VAL_PATH = Path(MAX_PATH, 'adversarial_data', 'pgd_06', 'validation')\n",
    "PGD_VAL_IMAGES_PATH = Path(PGD_VAL_PATH,'images')\n",
    "PGD_VAL_LABEL_PATH = Path(PGD_VAL_PATH, 'labels.txt')\n",
    "\n",
    "# CW\n",
    "CW_TRAIN_PATH = Path(MAX_PATH, 'adversarial_data', 'cw', 'train')\n",
    "CW_TRAIN_IMAGES_PATH = Path(CW_TRAIN_PATH,'images')\n",
    "CW_TRAIN_LABEL_PATH = Path(CW_TRAIN_PATH, 'labels.txt')\n",
    "\n",
    "CW_VAL_PATH = Path(MAX_PATH, 'adversarial_data', 'cw', 'validation')\n",
    "CW_VAL_IMAGES_PATH = Path(CW_VAL_PATH,'images')\n",
    "CW_VAL_LABEL_PATH = Path(CW_VAL_PATH, 'labels.txt')\n",
    "\n",
    "# FGSM\n",
    "FGSM_TRAIN_PATH = Path(MAX_PATH, 'adversarial_data', 'fgsm_06', 'train')\n",
    "FGSM_TRAIN_IMAGES_PATH = Path(FGSM_TRAIN_PATH,'images')\n",
    "FGSM_TRAIN_LABEL_PATH = Path(FGSM_TRAIN_PATH, 'labels.txt')\n",
    "\n",
    "FGSM_VAL_PATH = Path(MAX_PATH, 'adversarial_data', 'fgsm_06', 'validation')\n",
    "FGSM_VAL_IMAGES_PATH = Path(FGSM_VAL_PATH,'images')\n",
    "FGSM_VAL_LABEL_PATH = Path(FGSM_VAL_PATH, 'labels.txt')\n",
    "\n",
    "\n",
    "# TB LOG\n",
    "TB_LOGS_BASE_PATH = Path(LOG_BASE_PATH, 'tb_logs')\n",
    "\n",
    "\n",
    "# Model save path\n",
    "ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH = Path(MAX_PATH, 'adversarial_data', 'adv_classifiers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If CLASS_SUBSET is specified, INDEX_SUBSET will be ignored. Set CLASS_SUBSET=None if you want to use indexes.\n",
    "# INDEX_SUBSET = get_random_indexes(number_of_images = 50000, n_samples=1000)\n",
    "# CLASS_SUBSET = get_random_classes(number_of_classes = 25, min_rand_class = 1, max_rand_class = 1001)\n",
    "\n",
    "\n",
    "CLASS_SUBSET = np.load(CLASS_SUBSET_PATH)\n",
    "\n",
    "# CLASS_SUBSET = CLASS_SUBSET[:25] \n",
    "\n",
    "NUM_WORKERS= 0\n",
    "PIN_MEMORY=True\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import DINO\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights.\n"
     ]
    }
   ],
   "source": [
    "model, linear_classifier = get_dino()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to set the correct transformation\n",
    "# encoder\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit([i for i in CLASS_SUBSET])\n",
    "\n",
    "\n",
    "# PGD\n",
    "pgd_train_dataset = AdvTrainingImageDataset(PGD_TRAIN_IMAGES_PATH, \n",
    "                                            PGD_TRAIN_LABEL_PATH, \n",
    "                                            ORIGINAL_TRANSFORM, \n",
    "                                            CLASS_SUBSET, \n",
    "                                            index_subset=None,\n",
    "                                            label_encoder=label_encoder\n",
    "                                           )\n",
    "\n",
    "pgd_val_dataset = AdvTrainingImageDataset(PGD_VAL_IMAGES_PATH, \n",
    "                                          PGD_VAL_LABEL_PATH, \n",
    "                                          ORIGINAL_TRANSFORM, \n",
    "                                          CLASS_SUBSET, \n",
    "                                          index_subset=None,\n",
    "                                          label_encoder=label_encoder\n",
    "                                         )\n",
    "\n",
    "\n",
    "pgd_train_loader = DataLoader(pgd_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "pgd_val_loader = DataLoader(pgd_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY, \n",
    "                            shuffle=False)\n",
    "\n",
    "# CW\n",
    "cw_train_dataset = AdvTrainingImageDataset(CW_TRAIN_IMAGES_PATH, \n",
    "                                            CW_TRAIN_LABEL_PATH, \n",
    "                                            ORIGINAL_TRANSFORM, \n",
    "                                            CLASS_SUBSET, \n",
    "                                            index_subset=None,\n",
    "                                           label_encoder=label_encoder\n",
    "                                          )\n",
    "\n",
    "cw_val_dataset = AdvTrainingImageDataset(CW_VAL_IMAGES_PATH, \n",
    "                                          CW_VAL_LABEL_PATH, \n",
    "                                          ORIGINAL_TRANSFORM, \n",
    "                                          CLASS_SUBSET, \n",
    "                                          index_subset=None,\n",
    "                                         label_encoder=label_encoder\n",
    "                                        )\n",
    "\n",
    "cw_train_loader = DataLoader(cw_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "cw_val_loader = DataLoader(cw_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY, \n",
    "                            shuffle=False)\n",
    "\n",
    "# FGSM\n",
    "fgsm_train_dataset = AdvTrainingImageDataset(FGSM_TRAIN_IMAGES_PATH, \n",
    "                                            FGSM_TRAIN_LABEL_PATH, \n",
    "                                            ORIGINAL_TRANSFORM, \n",
    "                                            CLASS_SUBSET, \n",
    "                                            index_subset=None,\n",
    "                                            label_encoder=label_encoder\n",
    "                                            )\n",
    "\n",
    "fgsm_val_dataset = AdvTrainingImageDataset(FGSM_VAL_IMAGES_PATH, \n",
    "                                          FGSM_VAL_LABEL_PATH, \n",
    "                                          ORIGINAL_TRANSFORM, \n",
    "                                          CLASS_SUBSET, \n",
    "                                          index_subset=None,\n",
    "                                          label_encoder=label_encoder\n",
    "                                          )\n",
    "\n",
    "fgsm_train_loader = DataLoader(fgsm_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "fgsm_val_loader = DataLoader(fgsm_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY, \n",
    "                            shuffle=False)\n",
    "\n",
    "# Clean\n",
    "clean_train_dataset = ImageDataset(TRAIN_IMAGES_PATH, \n",
    "                                  TRAIN_LABEL_PATH, \n",
    "                                  ORIGINAL_TRANSFORM,\n",
    "                                  CLASS_SUBSET, \n",
    "                                  index_subset=None, \n",
    "                                  label_encoder=label_encoder)\n",
    "\n",
    "clean_val_dataset = ImageDataset(VAL_IMAGES_PATH, \n",
    "                                  VAL_LABEL_PATH, \n",
    "                                  ORIGINAL_TRANSFORM,\n",
    "                                  CLASS_SUBSET, \n",
    "                                  index_subset=None, \n",
    "                                  label_encoder=label_encoder)\n",
    "\n",
    "clean_train_loader = DataLoader(clean_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n",
    "\n",
    "clean_val_loader = DataLoader(clean_val_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_WORKERS, \n",
    "                            pin_memory=PIN_MEMORY,\n",
    "                            shuffle=False)\n",
    "\n",
    "\n",
    "loader_dict = {\n",
    "    \"pgd\" : {\n",
    "        \"train\" : pgd_train_loader,\n",
    "        \"validation\" : pgd_val_loader,\n",
    "    },\n",
    "    \"cw\" : {\n",
    "        \"train\" : cw_train_loader,\n",
    "        \"validation\" : cw_val_loader,\n",
    "    }, \n",
    "    \"fgsm\" : {\n",
    "        \"train\" : fgsm_train_loader,\n",
    "        \"validation\" : fgsm_val_loader,\n",
    "    },\n",
    "    \"clean\" : {\n",
    "        \"train\" : clean_train_loader,\n",
    "        \"validation\" : clean_val_loader,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2b52a5790a30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_dict[\"pgd\"][\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '25_classes_full_v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(dim, num_labels)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # linear layer\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Binary Classifier\n",
    "class LinearBC(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(LinearBC,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train various classifiers on all adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################################################################################################\n",
      "##################################################Training classifier for pgd##################################################\n",
      "###############################################################################################################################\n",
      "Found checkpoint at /cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/pgd/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/pgd/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      "=> loaded 'optimizer' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/pgd/checkpoint.pth.tar'\n",
      "=> loaded 'scheduler' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/pgd/checkpoint.pth.tar'\n",
      "Epoch: [1]  [ 0/82]  eta: 0:00:30  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.377378  data: 0.339144  max mem: 211\n",
      "Epoch: [1]  [20/82]  eta: 0:00:22  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.354450  data: 0.316195  max mem: 211\n",
      "Epoch: [1]  [40/82]  eta: 0:00:14  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.352559  data: 0.314284  max mem: 211\n",
      "Epoch: [1]  [60/82]  eta: 0:00:07  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.350409  data: 0.312152  max mem: 211\n",
      "Epoch: [1]  [80/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.350542  data: 0.312306  max mem: 211\n",
      "Epoch: [1]  [81/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.337431  data: 0.300519  max mem: 211\n",
      "Epoch: [1] Total time: 0:00:28 (0.349175 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:01  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.358613  data: 0.320376  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.277177  data: 0.245552  max mem: 211\n",
      "Test: Total time: 0:00:01 (0.277506 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 1 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 1 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [2]  [ 0/82]  eta: 0:00:19  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236319  data: 0.197987  max mem: 211\n",
      "Epoch: [2]  [20/82]  eta: 0:00:14  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236264  data: 0.197998  max mem: 211\n",
      "Epoch: [2]  [40/82]  eta: 0:00:09  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.235630  data: 0.197380  max mem: 211\n",
      "Epoch: [2]  [60/82]  eta: 0:00:05  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236204  data: 0.197931  max mem: 211\n",
      "Epoch: [2]  [80/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236336  data: 0.198089  max mem: 211\n",
      "Epoch: [2]  [81/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.227662  data: 0.190698  max mem: 211\n",
      "Epoch: [2] Total time: 0:00:19 (0.234068 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.242759  data: 0.204627  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.187176  data: 0.155723  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.187511 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 2 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 2 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [3]  [ 0/82]  eta: 0:00:19  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.243459  data: 0.205405  max mem: 211\n",
      "Epoch: [3]  [20/82]  eta: 0:00:14  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.236944  data: 0.198821  max mem: 211\n",
      "Epoch: [3]  [40/82]  eta: 0:00:09  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.235865  data: 0.197741  max mem: 211\n",
      "Epoch: [3]  [60/82]  eta: 0:00:05  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.237145  data: 0.198999  max mem: 211\n",
      "Epoch: [3]  [80/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.236505  data: 0.198364  max mem: 211\n",
      "Epoch: [3]  [81/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.227764  data: 0.190937  max mem: 211\n",
      "Epoch: [3] Total time: 0:00:19 (0.234638 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.243639  data: 0.205340  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.187040  data: 0.155561  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.187371 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 3 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 3 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [4]  [ 0/82]  eta: 0:00:19  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.237304  data: 0.199155  max mem: 211\n",
      "Epoch: [4]  [20/82]  eta: 0:00:14  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236897  data: 0.198766  max mem: 211\n",
      "Epoch: [4]  [40/82]  eta: 0:00:09  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236708  data: 0.198561  max mem: 211\n",
      "Epoch: [4]  [60/82]  eta: 0:00:05  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.235746  data: 0.197592  max mem: 211\n",
      "Epoch: [4]  [80/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236444  data: 0.198347  max mem: 211\n",
      "Epoch: [4]  [81/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.227742  data: 0.190964  max mem: 211\n",
      "Epoch: [4] Total time: 0:00:19 (0.234395 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.241399  data: 0.203145  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.187272  data: 0.155722  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.187605 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 4 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 4 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Training of the supervised linear classifier on frozen features completed.\n",
      "Top-1 test accuracy: 100.0\n",
      "Finished Training, saving model to /cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes_full_v1/pgd.pt and log to /cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes_full_v1/pgd.pt\n",
      "##############################################################################################################################\n",
      "##################################################Training classifier for cw##################################################\n",
      "##############################################################################################################################\n",
      "Found checkpoint at /cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/cw/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/cw/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      "=> loaded 'optimizer' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/cw/checkpoint.pth.tar'\n",
      "=> loaded 'scheduler' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/cw/checkpoint.pth.tar'\n",
      "Epoch: [1]  [ 0/82]  eta: 0:00:30  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.366465  data: 0.328399  max mem: 211\n",
      "Epoch: [1]  [20/82]  eta: 0:00:23  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.385364  data: 0.347210  max mem: 211\n",
      "Epoch: [1]  [40/82]  eta: 0:00:15  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.375541  data: 0.337434  max mem: 211\n",
      "Epoch: [1]  [60/82]  eta: 0:00:08  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.343131  data: 0.305093  max mem: 211\n",
      "Epoch: [1]  [80/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.338256  data: 0.300229  max mem: 211\n",
      "Epoch: [1]  [81/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.325455  data: 0.288810  max mem: 211\n",
      "Epoch: [1] Total time: 0:00:29 (0.357354 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:01  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.347097  data: 0.309057  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.348742  data: 0.317586  max mem: 211\n",
      "Test: Total time: 0:00:01 (0.349043 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 1 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 1 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [2]  [ 0/82]  eta: 0:00:19  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.233743  data: 0.195713  max mem: 211\n",
      "Epoch: [2]  [20/82]  eta: 0:00:14  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.226078  data: 0.188059  max mem: 211\n",
      "Epoch: [2]  [40/82]  eta: 0:00:09  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.233324  data: 0.195268  max mem: 211\n",
      "Epoch: [2]  [60/82]  eta: 0:00:05  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.238494  data: 0.200393  max mem: 211\n",
      "Epoch: [2]  [80/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.237795  data: 0.199652  max mem: 211\n",
      "Epoch: [2]  [81/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.229055  data: 0.192177  max mem: 211\n",
      "Epoch: [2] Total time: 0:00:19 (0.231958 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:01  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.253195  data: 0.215042  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.191940  data: 0.160324  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.192286 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 2 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 2 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [3]  [ 0/82]  eta: 0:00:19  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.242012  data: 0.203958  max mem: 211\n",
      "Epoch: [3]  [20/82]  eta: 0:00:14  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.236564  data: 0.198460  max mem: 211\n",
      "Epoch: [3]  [40/82]  eta: 0:00:09  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.234945  data: 0.196857  max mem: 211\n",
      "Epoch: [3]  [60/82]  eta: 0:00:05  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.235822  data: 0.197740  max mem: 211\n",
      "Epoch: [3]  [80/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.235935  data: 0.197810  max mem: 211\n",
      "Epoch: [3]  [81/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.227086  data: 0.190303  max mem: 211\n",
      "Epoch: [3] Total time: 0:00:19 (0.233841 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.242429  data: 0.204274  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.186432  data: 0.155138  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.186758 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 3 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 3 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [4]  [ 0/82]  eta: 0:00:19  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.241511  data: 0.203505  max mem: 211\n",
      "Epoch: [4]  [20/82]  eta: 0:00:14  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.234986  data: 0.196831  max mem: 211\n",
      "Epoch: [4]  [40/82]  eta: 0:00:09  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.235451  data: 0.197245  max mem: 211\n",
      "Epoch: [4]  [60/82]  eta: 0:00:05  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236556  data: 0.198440  max mem: 211\n",
      "Epoch: [4]  [80/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.235119  data: 0.197026  max mem: 211\n",
      "Epoch: [4]  [81/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.226421  data: 0.189643  max mem: 211\n",
      "Epoch: [4] Total time: 0:00:19 (0.233559 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.241777  data: 0.203600  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.187539  data: 0.155816  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.187884 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 4 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 4 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Training of the supervised linear classifier on frozen features completed.\n",
      "Top-1 test accuracy: 100.0\n",
      "Finished Training, saving model to /cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes_full_v1/cw.pt and log to /cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes_full_v1/cw.pt\n",
      "################################################################################################################################\n",
      "##################################################Training classifier for fgsm##################################################\n",
      "################################################################################################################################\n",
      "Found checkpoint at /cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/fgsm/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/fgsm/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      "=> loaded 'optimizer' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/fgsm/checkpoint.pth.tar'\n",
      "=> loaded 'scheduler' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/fgsm/checkpoint.pth.tar'\n",
      "Epoch: [1]  [ 0/82]  eta: 0:00:30  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.369044  data: 0.330962  max mem: 211\n",
      "Epoch: [1]  [20/82]  eta: 0:00:24  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.391692  data: 0.353577  max mem: 211\n",
      "Epoch: [1]  [40/82]  eta: 0:00:15  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.354034  data: 0.315903  max mem: 211\n",
      "Epoch: [1]  [60/82]  eta: 0:00:08  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.377947  data: 0.339829  max mem: 211\n",
      "Epoch: [1]  [80/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.354370  data: 0.316231  max mem: 211\n",
      "Epoch: [1]  [81/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.341205  data: 0.304371  max mem: 211\n",
      "Epoch: [1] Total time: 0:00:30 (0.366204 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:01  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.353285  data: 0.315068  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.277448  data: 0.245949  max mem: 211\n",
      "Test: Total time: 0:00:01 (0.277772 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 1 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 1 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [2]  [ 0/82]  eta: 0:00:19  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.243253  data: 0.205139  max mem: 211\n",
      "Epoch: [2]  [20/82]  eta: 0:00:14  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.236905  data: 0.198773  max mem: 211\n",
      "Epoch: [2]  [40/82]  eta: 0:00:09  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.237417  data: 0.199229  max mem: 211\n",
      "Epoch: [2]  [60/82]  eta: 0:00:05  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.237800  data: 0.199664  max mem: 211\n",
      "Epoch: [2]  [80/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.238117  data: 0.199989  max mem: 211\n",
      "Epoch: [2]  [81/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.229282  data: 0.192448  max mem: 211\n",
      "Epoch: [2] Total time: 0:00:19 (0.235574 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.247326  data: 0.209194  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.190469  data: 0.158956  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.190884 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 2 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 2 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [3]  [ 0/82]  eta: 0:00:19  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.242464  data: 0.204284  max mem: 211\n",
      "Epoch: [3]  [20/82]  eta: 0:00:14  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.237374  data: 0.199240  max mem: 211\n",
      "Epoch: [3]  [40/82]  eta: 0:00:10  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.238866  data: 0.200749  max mem: 211\n",
      "Epoch: [3]  [60/82]  eta: 0:00:05  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.237324  data: 0.199200  max mem: 211\n",
      "Epoch: [3]  [80/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.238038  data: 0.199859  max mem: 211\n",
      "Epoch: [3]  [81/82]  eta: 0:00:00  lr: 0.000000  loss: 0.000000 (0.000000)  time: 0.229211  data: 0.192389  max mem: 211\n",
      "Epoch: [3] Total time: 0:00:19 (0.235890 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.243826  data: 0.205690  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.187973  data: 0.156508  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.188302 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 3 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 3 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Epoch: [4]  [ 0/82]  eta: 0:00:19  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.243892  data: 0.205813  max mem: 211\n",
      "Epoch: [4]  [20/82]  eta: 0:00:14  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.237372  data: 0.199194  max mem: 211\n",
      "Epoch: [4]  [40/82]  eta: 0:00:09  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.237349  data: 0.199198  max mem: 211\n",
      "Epoch: [4]  [60/82]  eta: 0:00:05  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.237702  data: 0.199592  max mem: 211\n",
      "Epoch: [4]  [80/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.237794  data: 0.199693  max mem: 211\n",
      "Epoch: [4]  [81/82]  eta: 0:00:00  lr: 0.000063  loss: 0.000000 (0.000000)  time: 0.229052  data: 0.192263  max mem: 211\n",
      "Epoch: [4] Total time: 0:00:19 (0.235571 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 0.000000 (0.000000)\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.243511  data: 0.205322  max mem: 211\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.000000)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.188655  data: 0.157124  max mem: 211\n",
      "Test: Total time: 0:00:00 (0.188982 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.000\n",
      "Accuracy at epoch 4 of the network on the 4 test images: 100.0%\n",
      "Accuracy at epoch 4 of the network on the 50 test images: 100.0%\n",
      "Max accuracy so far: 100.00%\n",
      "Training of the supervised linear classifier on frozen features completed.\n",
      "Top-1 test accuracy: 100.0\n",
      "Finished Training, saving model to /cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes_full_v1/fgsm.pt and log to /cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes_full_v1/fgsm.pt\n",
      "#################################################################################################################################\n",
      "##################################################Training classifier for clean##################################################\n",
      "#################################################################################################################################\n",
      "Found checkpoint at /cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/clean/checkpoint.pth.tar\n",
      "=> loaded 'state_dict' from checkpoint '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/clean/checkpoint.pth.tar' with msg <All keys matched successfully>\n",
      "=> loaded 'optimizer' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/clean/checkpoint.pth.tar'\n",
      "=> loaded 'scheduler' from checkpoint: '/cluster/scratch/mmathys/dl_data/logs/adv_classifier/25_classes_full_v1/clean/checkpoint.pth.tar'\n",
      "Epoch: [1]  [   0/2012]  eta: 0:16:45  lr: 0.000000  loss: 0.005330 (0.005330)  time: 0.499512  data: 0.461465  max mem: 211\n",
      "Epoch: [1]  [  20/2012]  eta: 0:17:38  lr: 0.000000  loss: 0.011244 (0.051113)  time: 0.533071  data: 0.494918  max mem: 211\n",
      "Epoch: [1]  [  40/2012]  eta: 0:16:37  lr: 0.000000  loss: 0.053833 (0.071367)  time: 0.479311  data: 0.441185  max mem: 211\n",
      "Epoch: [1]  [  60/2012]  eta: 0:16:12  lr: 0.000000  loss: 0.027650 (0.078470)  time: 0.482569  data: 0.444440  max mem: 211\n",
      "Epoch: [1]  [  80/2012]  eta: 0:15:51  lr: 0.000000  loss: 0.070722 (0.084037)  time: 0.475324  data: 0.437212  max mem: 211\n",
      "Epoch: [1]  [ 100/2012]  eta: 0:15:51  lr: 0.000000  loss: 0.019478 (0.075089)  time: 0.516705  data: 0.478581  max mem: 211\n",
      "Epoch: [1]  [ 120/2012]  eta: 0:15:33  lr: 0.000000  loss: 0.034886 (0.075078)  time: 0.474049  data: 0.435946  max mem: 211\n",
      "Epoch: [1]  [ 140/2012]  eta: 0:15:44  lr: 0.000000  loss: 0.051577 (0.077865)  time: 0.569886  data: 0.531821  max mem: 211\n",
      "Epoch: [1]  [ 160/2012]  eta: 0:15:23  lr: 0.000000  loss: 0.041574 (0.075853)  time: 0.457652  data: 0.419659  max mem: 211\n",
      "Epoch: [1]  [ 180/2012]  eta: 0:15:12  lr: 0.000000  loss: 0.031845 (0.077014)  time: 0.496048  data: 0.457939  max mem: 211\n",
      "Epoch: [1]  [ 200/2012]  eta: 0:15:09  lr: 0.000000  loss: 0.013812 (0.074935)  time: 0.534538  data: 0.496385  max mem: 211\n",
      "Epoch: [1]  [ 220/2012]  eta: 0:14:54  lr: 0.000000  loss: 0.052742 (0.076821)  time: 0.474526  data: 0.436374  max mem: 211\n",
      "Epoch: [1]  [ 240/2012]  eta: 0:14:44  lr: 0.000000  loss: 0.027114 (0.075238)  time: 0.495445  data: 0.457301  max mem: 211\n",
      "Epoch: [1]  [ 260/2012]  eta: 0:14:33  lr: 0.000000  loss: 0.031278 (0.073430)  time: 0.492553  data: 0.454438  max mem: 211\n",
      "Epoch: [1]  [ 280/2012]  eta: 0:14:23  lr: 0.000000  loss: 0.049985 (0.075734)  time: 0.495358  data: 0.457229  max mem: 211\n",
      "Epoch: [1]  [ 300/2012]  eta: 0:14:14  lr: 0.000000  loss: 0.082680 (0.076830)  time: 0.507792  data: 0.469681  max mem: 211\n",
      "Epoch: [1]  [ 320/2012]  eta: 0:14:02  lr: 0.000000  loss: 0.028380 (0.075247)  time: 0.483601  data: 0.445479  max mem: 211\n",
      "Epoch: [1]  [ 340/2012]  eta: 0:13:52  lr: 0.000000  loss: 0.038142 (0.075684)  time: 0.495681  data: 0.457569  max mem: 211\n",
      "Epoch: [1]  [ 360/2012]  eta: 0:13:45  lr: 0.000000  loss: 0.032275 (0.076189)  time: 0.526778  data: 0.488626  max mem: 211\n",
      "Epoch: [1]  [ 380/2012]  eta: 0:13:34  lr: 0.000000  loss: 0.018004 (0.075316)  time: 0.496122  data: 0.458001  max mem: 211\n",
      "Epoch: [1]  [ 400/2012]  eta: 0:13:25  lr: 0.000000  loss: 0.036643 (0.076430)  time: 0.500914  data: 0.462815  max mem: 211\n",
      "Epoch: [1]  [ 420/2012]  eta: 0:13:13  lr: 0.000000  loss: 0.011514 (0.074363)  time: 0.476698  data: 0.438552  max mem: 211\n",
      "Epoch: [1]  [ 440/2012]  eta: 0:13:01  lr: 0.000000  loss: 0.064203 (0.074604)  time: 0.470230  data: 0.432101  max mem: 211\n",
      "Epoch: [1]  [ 460/2012]  eta: 0:12:51  lr: 0.000000  loss: 0.017533 (0.073647)  time: 0.490987  data: 0.452852  max mem: 211\n",
      "Epoch: [1]  [ 480/2012]  eta: 0:12:39  lr: 0.000000  loss: 0.046839 (0.074022)  time: 0.467338  data: 0.429224  max mem: 211\n",
      "Epoch: [1]  [ 500/2012]  eta: 0:12:28  lr: 0.000000  loss: 0.084284 (0.076365)  time: 0.476285  data: 0.438157  max mem: 211\n",
      "Epoch: [1]  [ 520/2012]  eta: 0:12:16  lr: 0.000000  loss: 0.022889 (0.076581)  time: 0.470263  data: 0.432125  max mem: 211\n",
      "Epoch: [1]  [ 540/2012]  eta: 0:12:06  lr: 0.000000  loss: 0.049436 (0.076635)  time: 0.478737  data: 0.440596  max mem: 211\n",
      "Epoch: [1]  [ 560/2012]  eta: 0:11:55  lr: 0.000000  loss: 0.048562 (0.076729)  time: 0.474662  data: 0.436527  max mem: 211\n",
      "Epoch: [1]  [ 580/2012]  eta: 0:11:44  lr: 0.000000  loss: 0.043978 (0.077596)  time: 0.473206  data: 0.435080  max mem: 211\n",
      "Epoch: [1]  [ 600/2012]  eta: 0:11:32  lr: 0.000000  loss: 0.027769 (0.076507)  time: 0.453324  data: 0.415262  max mem: 211\n",
      "Epoch: [1]  [ 620/2012]  eta: 0:11:22  lr: 0.000000  loss: 0.014957 (0.076705)  time: 0.470661  data: 0.432603  max mem: 211\n",
      "Epoch: [1]  [ 640/2012]  eta: 0:11:11  lr: 0.000000  loss: 0.012808 (0.076122)  time: 0.468050  data: 0.430005  max mem: 211\n",
      "Epoch: [1]  [ 660/2012]  eta: 0:11:00  lr: 0.000000  loss: 0.020078 (0.074974)  time: 0.468444  data: 0.430297  max mem: 211\n",
      "Epoch: [1]  [ 680/2012]  eta: 0:10:50  lr: 0.000000  loss: 0.072095 (0.075940)  time: 0.464289  data: 0.426146  max mem: 211\n",
      "Epoch: [1]  [ 700/2012]  eta: 0:10:39  lr: 0.000000  loss: 0.091372 (0.077354)  time: 0.464567  data: 0.426456  max mem: 211\n",
      "Epoch: [1]  [ 720/2012]  eta: 0:10:28  lr: 0.000000  loss: 0.028162 (0.077480)  time: 0.469942  data: 0.431810  max mem: 211\n",
      "Epoch: [1]  [ 740/2012]  eta: 0:10:18  lr: 0.000000  loss: 0.057996 (0.077324)  time: 0.465674  data: 0.427539  max mem: 211\n",
      "Epoch: [1]  [ 760/2012]  eta: 0:10:08  lr: 0.000000  loss: 0.026980 (0.076474)  time: 0.472900  data: 0.434784  max mem: 211\n",
      "Epoch: [1]  [ 780/2012]  eta: 0:09:57  lr: 0.000000  loss: 0.024923 (0.076175)  time: 0.461499  data: 0.423393  max mem: 211\n",
      "Epoch: [1]  [ 800/2012]  eta: 0:09:47  lr: 0.000000  loss: 0.030287 (0.076533)  time: 0.459599  data: 0.421476  max mem: 211\n",
      "Epoch: [1]  [ 820/2012]  eta: 0:09:37  lr: 0.000000  loss: 0.067242 (0.076408)  time: 0.467226  data: 0.429083  max mem: 211\n",
      "Epoch: [1]  [ 840/2012]  eta: 0:09:27  lr: 0.000000  loss: 0.032722 (0.076614)  time: 0.485193  data: 0.447076  max mem: 211\n",
      "Epoch: [1]  [ 860/2012]  eta: 0:09:18  lr: 0.000000  loss: 0.028994 (0.076656)  time: 0.522404  data: 0.484277  max mem: 211\n",
      "Epoch: [1]  [ 880/2012]  eta: 0:09:10  lr: 0.000000  loss: 0.039631 (0.076865)  time: 0.525979  data: 0.487823  max mem: 211\n",
      "Epoch: [1]  [ 900/2012]  eta: 0:09:01  lr: 0.000000  loss: 0.036611 (0.076398)  time: 0.536935  data: 0.498807  max mem: 211\n",
      "Epoch: [1]  [ 920/2012]  eta: 0:08:52  lr: 0.000000  loss: 0.054139 (0.076988)  time: 0.488143  data: 0.449989  max mem: 211\n",
      "Epoch: [1]  [ 940/2012]  eta: 0:08:42  lr: 0.000000  loss: 0.026851 (0.076389)  time: 0.479873  data: 0.441726  max mem: 211\n",
      "Epoch: [1]  [ 960/2012]  eta: 0:08:32  lr: 0.000000  loss: 0.072742 (0.077106)  time: 0.489240  data: 0.451088  max mem: 211\n",
      "Epoch: [1]  [ 980/2012]  eta: 0:08:22  lr: 0.000000  loss: 0.055265 (0.077408)  time: 0.487031  data: 0.448882  max mem: 211\n",
      "Epoch: [1]  [1000/2012]  eta: 0:08:13  lr: 0.000000  loss: 0.043211 (0.077559)  time: 0.508327  data: 0.470206  max mem: 211\n",
      "Epoch: [1]  [1020/2012]  eta: 0:08:03  lr: 0.000000  loss: 0.035788 (0.078257)  time: 0.489984  data: 0.451847  max mem: 211\n",
      "Epoch: [1]  [1040/2012]  eta: 0:07:53  lr: 0.000000  loss: 0.027212 (0.077775)  time: 0.465540  data: 0.427389  max mem: 211\n",
      "Epoch: [1]  [1060/2012]  eta: 0:07:44  lr: 0.000000  loss: 0.056084 (0.077934)  time: 0.505489  data: 0.467355  max mem: 211\n",
      "Epoch: [1]  [1080/2012]  eta: 0:07:33  lr: 0.000000  loss: 0.009794 (0.077663)  time: 0.457698  data: 0.419643  max mem: 211\n",
      "Epoch: [1]  [1100/2012]  eta: 0:07:24  lr: 0.000000  loss: 0.029961 (0.077447)  time: 0.504119  data: 0.466104  max mem: 211\n",
      "Epoch: [1]  [1120/2012]  eta: 0:07:14  lr: 0.000000  loss: 0.024783 (0.077802)  time: 0.495445  data: 0.457326  max mem: 211\n",
      "Epoch: [1]  [1140/2012]  eta: 0:07:05  lr: 0.000000  loss: 0.057797 (0.078059)  time: 0.490419  data: 0.452276  max mem: 211\n",
      "Epoch: [1]  [1160/2012]  eta: 0:06:55  lr: 0.000000  loss: 0.025613 (0.077871)  time: 0.502233  data: 0.464139  max mem: 211\n",
      "Epoch: [1]  [1180/2012]  eta: 0:06:46  lr: 0.000000  loss: 0.022035 (0.077293)  time: 0.539294  data: 0.501186  max mem: 211\n",
      "Epoch: [1]  [1200/2012]  eta: 0:06:37  lr: 0.000000  loss: 0.079833 (0.077926)  time: 0.536213  data: 0.498103  max mem: 211\n",
      "Epoch: [1]  [1220/2012]  eta: 0:06:27  lr: 0.000000  loss: 0.020836 (0.077931)  time: 0.513274  data: 0.475132  max mem: 211\n",
      "Epoch: [1]  [1240/2012]  eta: 0:06:17  lr: 0.000000  loss: 0.020617 (0.077950)  time: 0.480064  data: 0.441922  max mem: 211\n",
      "Epoch: [1]  [1260/2012]  eta: 0:06:08  lr: 0.000000  loss: 0.018655 (0.077876)  time: 0.479584  data: 0.441459  max mem: 211\n",
      "Epoch: [1]  [1280/2012]  eta: 0:05:58  lr: 0.000000  loss: 0.026419 (0.077606)  time: 0.471583  data: 0.433416  max mem: 211\n",
      "Epoch: [1]  [1300/2012]  eta: 0:05:48  lr: 0.000000  loss: 0.015994 (0.077119)  time: 0.465282  data: 0.427172  max mem: 211\n",
      "Epoch: [1]  [1320/2012]  eta: 0:05:38  lr: 0.000000  loss: 0.036624 (0.077074)  time: 0.527704  data: 0.489554  max mem: 211\n",
      "Epoch: [1]  [1340/2012]  eta: 0:05:28  lr: 0.000000  loss: 0.047064 (0.076954)  time: 0.468755  data: 0.430618  max mem: 211\n",
      "Epoch: [1]  [1360/2012]  eta: 0:05:18  lr: 0.000000  loss: 0.031782 (0.076795)  time: 0.468204  data: 0.430078  max mem: 211\n",
      "Epoch: [1]  [1380/2012]  eta: 0:05:08  lr: 0.000000  loss: 0.012382 (0.076786)  time: 0.470069  data: 0.431926  max mem: 211\n",
      "Epoch: [1]  [1400/2012]  eta: 0:04:58  lr: 0.000000  loss: 0.014999 (0.076370)  time: 0.466150  data: 0.428018  max mem: 211\n",
      "Epoch: [1]  [1420/2012]  eta: 0:04:48  lr: 0.000000  loss: 0.046547 (0.077104)  time: 0.462353  data: 0.424212  max mem: 211\n",
      "Epoch: [1]  [1440/2012]  eta: 0:04:38  lr: 0.000000  loss: 0.025886 (0.076740)  time: 0.475199  data: 0.437050  max mem: 211\n",
      "Epoch: [1]  [1460/2012]  eta: 0:04:29  lr: 0.000000  loss: 0.030186 (0.076399)  time: 0.471511  data: 0.433382  max mem: 211\n",
      "Epoch: [1]  [1480/2012]  eta: 0:04:19  lr: 0.000000  loss: 0.033346 (0.076269)  time: 0.466734  data: 0.428640  max mem: 211\n",
      "Epoch: [1]  [1500/2012]  eta: 0:04:09  lr: 0.000000  loss: 0.048453 (0.075978)  time: 0.473828  data: 0.435703  max mem: 211\n",
      "Epoch: [1]  [1520/2012]  eta: 0:03:59  lr: 0.000000  loss: 0.027140 (0.075860)  time: 0.477579  data: 0.439458  max mem: 211\n",
      "Epoch: [1]  [1540/2012]  eta: 0:03:49  lr: 0.000000  loss: 0.044412 (0.075787)  time: 0.465358  data: 0.427314  max mem: 211\n",
      "Epoch: [1]  [1560/2012]  eta: 0:03:39  lr: 0.000000  loss: 0.026552 (0.075848)  time: 0.462834  data: 0.424822  max mem: 211\n",
      "Epoch: [1]  [1580/2012]  eta: 0:03:30  lr: 0.000000  loss: 0.041059 (0.076395)  time: 0.476395  data: 0.438330  max mem: 211\n",
      "Epoch: [1]  [1600/2012]  eta: 0:03:20  lr: 0.000000  loss: 0.030246 (0.076227)  time: 0.500448  data: 0.462270  max mem: 211\n",
      "Epoch: [1]  [1620/2012]  eta: 0:03:10  lr: 0.000000  loss: 0.036634 (0.076097)  time: 0.524348  data: 0.486232  max mem: 211\n",
      "Epoch: [1]  [1640/2012]  eta: 0:03:01  lr: 0.000000  loss: 0.057421 (0.076271)  time: 0.523289  data: 0.485171  max mem: 211\n",
      "Epoch: [1]  [1660/2012]  eta: 0:02:51  lr: 0.000000  loss: 0.044258 (0.076283)  time: 0.541265  data: 0.503144  max mem: 211\n",
      "Epoch: [1]  [1680/2012]  eta: 0:02:42  lr: 0.000000  loss: 0.026157 (0.076041)  time: 0.587744  data: 0.549617  max mem: 211\n",
      "Epoch: [1]  [1700/2012]  eta: 0:02:32  lr: 0.000000  loss: 0.039392 (0.076022)  time: 0.555425  data: 0.517258  max mem: 211\n",
      "Epoch: [1]  [1720/2012]  eta: 0:02:23  lr: 0.000000  loss: 0.036162 (0.075790)  time: 0.510644  data: 0.472463  max mem: 211\n",
      "Epoch: [1]  [1740/2012]  eta: 0:02:13  lr: 0.000000  loss: 0.040149 (0.075956)  time: 0.515032  data: 0.476923  max mem: 211\n",
      "Epoch: [1]  [1760/2012]  eta: 0:02:03  lr: 0.000000  loss: 0.046009 (0.076032)  time: 0.514229  data: 0.476139  max mem: 211\n",
      "Epoch: [1]  [1780/2012]  eta: 0:01:54  lr: 0.000000  loss: 0.029062 (0.076271)  time: 0.616757  data: 0.578642  max mem: 211\n",
      "Epoch: [1]  [1800/2012]  eta: 0:01:44  lr: 0.000000  loss: 0.020641 (0.075976)  time: 0.610260  data: 0.572089  max mem: 211\n",
      "Epoch: [1]  [1820/2012]  eta: 0:01:34  lr: 0.000000  loss: 0.018579 (0.075854)  time: 0.506266  data: 0.468139  max mem: 211\n",
      "Epoch: [1]  [1840/2012]  eta: 0:01:24  lr: 0.000000  loss: 0.034909 (0.075934)  time: 0.505565  data: 0.467402  max mem: 211\n",
      "Epoch: [1]  [1860/2012]  eta: 0:01:15  lr: 0.000000  loss: 0.046678 (0.076177)  time: 0.519024  data: 0.480893  max mem: 211\n",
      "Epoch: [1]  [1880/2012]  eta: 0:01:05  lr: 0.000000  loss: 0.049599 (0.076225)  time: 0.518161  data: 0.480037  max mem: 211\n",
      "Epoch: [1]  [1900/2012]  eta: 0:00:55  lr: 0.000000  loss: 0.032178 (0.075862)  time: 0.474464  data: 0.436313  max mem: 211\n",
      "Epoch: [1]  [1920/2012]  eta: 0:00:45  lr: 0.000000  loss: 0.035471 (0.075578)  time: 0.481592  data: 0.443470  max mem: 211\n",
      "Epoch: [1]  [1940/2012]  eta: 0:00:35  lr: 0.000000  loss: 0.020236 (0.075240)  time: 0.505226  data: 0.467086  max mem: 211\n",
      "Epoch: [1]  [1960/2012]  eta: 0:00:25  lr: 0.000000  loss: 0.023877 (0.075541)  time: 0.495802  data: 0.457709  max mem: 211\n",
      "Epoch: [1]  [1980/2012]  eta: 0:00:15  lr: 0.000000  loss: 0.040097 (0.075366)  time: 0.465226  data: 0.427181  max mem: 211\n",
      "Epoch: [1]  [2000/2012]  eta: 0:00:05  lr: 0.000000  loss: 0.017719 (0.074966)  time: 0.469580  data: 0.431517  max mem: 211\n",
      "Epoch: [1]  [2011/2012]  eta: 0:00:00  lr: 0.000000  loss: 0.042188 (0.074969)  time: 0.454249  data: 0.417486  max mem: 211\n",
      "Epoch: [1] Total time: 0:16:32 (0.493205 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.042188 (0.074969)\n",
      "Test:  [ 0/79]  eta: 0:00:38  loss: 0.014553 (0.014553)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.493172  data: 0.455018  max mem: 211\n",
      "Test:  [20/79]  eta: 0:00:28  loss: 0.065969 (0.117765)  acc1: 93.750000 (96.428571)  acc5: 100.000000 (100.000000)  time: 0.488015  data: 0.449838  max mem: 211\n",
      "Test:  [40/79]  eta: 0:00:19  loss: 0.064802 (0.099378)  acc1: 100.000000 (96.798780)  acc5: 100.000000 (100.000000)  time: 0.518520  data: 0.480362  max mem: 211\n",
      "Test:  [60/79]  eta: 0:00:09  loss: 0.085115 (0.103508)  acc1: 93.750000 (96.516393)  acc5: 100.000000 (99.897541)  time: 0.542683  data: 0.504556  max mem: 211\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 0.024753 (0.100158)  acc1: 100.000000 (97.200000)  acc5: 100.000000 (99.920000)  time: 0.516772  data: 0.479912  max mem: 211\n",
      "Test: Total time: 0:00:40 (0.517174 s / it)\n",
      "* Acc@1 97.200 Acc@5 99.920 loss 0.100\n",
      "Accuracy at epoch 1 of the network on the 79 test images: 97.2%\n",
      "Accuracy at epoch 1 of the network on the 1250 test images: 97.2%\n",
      "Max accuracy so far: 97.20%\n",
      "Epoch: [2]  [   0/2012]  eta: 0:15:28  lr: 0.000063  loss: 0.005330 (0.005330)  time: 0.461684  data: 0.423612  max mem: 211\n",
      "Epoch: [2]  [  20/2012]  eta: 0:19:08  lr: 0.000063  loss: 0.011232 (0.049991)  time: 0.582087  data: 0.543944  max mem: 211\n",
      "Epoch: [2]  [  40/2012]  eta: 0:17:26  lr: 0.000063  loss: 0.040048 (0.068912)  time: 0.482205  data: 0.444092  max mem: 211\n",
      "Epoch: [2]  [  60/2012]  eta: 0:16:46  lr: 0.000063  loss: 0.034136 (0.080130)  time: 0.485972  data: 0.447847  max mem: 211\n",
      "Epoch: [2]  [  80/2012]  eta: 0:16:12  lr: 0.000063  loss: 0.059056 (0.080550)  time: 0.464718  data: 0.426619  max mem: 211\n",
      "Epoch: [2]  [ 100/2012]  eta: 0:16:02  lr: 0.000063  loss: 0.016719 (0.071608)  time: 0.503762  data: 0.465653  max mem: 211\n",
      "Epoch: [2]  [ 120/2012]  eta: 0:16:07  lr: 0.000063  loss: 0.033637 (0.073345)  time: 0.551544  data: 0.513315  max mem: 211\n",
      "Epoch: [2]  [ 140/2012]  eta: 0:15:56  lr: 0.000063  loss: 0.056505 (0.076377)  time: 0.507732  data: 0.469605  max mem: 211\n",
      "Epoch: [2]  [ 160/2012]  eta: 0:16:02  lr: 0.000063  loss: 0.035610 (0.074340)  time: 0.582563  data: 0.544438  max mem: 211\n",
      "Epoch: [2]  [ 180/2012]  eta: 0:15:42  lr: 0.000063  loss: 0.026446 (0.074191)  time: 0.469931  data: 0.431815  max mem: 211\n",
      "Epoch: [2]  [ 200/2012]  eta: 0:15:25  lr: 0.000063  loss: 0.018662 (0.072253)  time: 0.477220  data: 0.439084  max mem: 211\n",
      "Epoch: [2]  [ 220/2012]  eta: 0:15:17  lr: 0.000063  loss: 0.053119 (0.072730)  time: 0.524496  data: 0.486384  max mem: 211\n",
      "Epoch: [2]  [ 240/2012]  eta: 0:15:04  lr: 0.000063  loss: 0.028623 (0.071049)  time: 0.498048  data: 0.459925  max mem: 211\n",
      "Epoch: [2]  [ 260/2012]  eta: 0:14:57  lr: 0.000063  loss: 0.048758 (0.069422)  time: 0.532254  data: 0.494134  max mem: 211\n",
      "Epoch: [2]  [ 280/2012]  eta: 0:14:48  lr: 0.000063  loss: 0.034510 (0.070606)  time: 0.525718  data: 0.487540  max mem: 211\n",
      "Epoch: [2]  [ 300/2012]  eta: 0:14:42  lr: 0.000063  loss: 0.036258 (0.070129)  time: 0.542394  data: 0.504262  max mem: 211\n",
      "Epoch: [2]  [ 320/2012]  eta: 0:14:37  lr: 0.000063  loss: 0.021990 (0.069451)  time: 0.570177  data: 0.532064  max mem: 211\n",
      "Epoch: [2]  [ 340/2012]  eta: 0:14:25  lr: 0.000063  loss: 0.025445 (0.069650)  time: 0.497724  data: 0.459662  max mem: 211\n",
      "Epoch: [2]  [ 360/2012]  eta: 0:14:18  lr: 0.000063  loss: 0.027199 (0.070140)  time: 0.555658  data: 0.517621  max mem: 211\n",
      "Epoch: [2]  [ 380/2012]  eta: 0:14:05  lr: 0.000063  loss: 0.012806 (0.069742)  time: 0.492697  data: 0.454531  max mem: 211\n",
      "Epoch: [2]  [ 400/2012]  eta: 0:13:55  lr: 0.000063  loss: 0.029276 (0.070466)  time: 0.517090  data: 0.478962  max mem: 211\n",
      "Epoch: [2]  [ 420/2012]  eta: 0:13:42  lr: 0.000063  loss: 0.010286 (0.068517)  time: 0.493473  data: 0.455319  max mem: 211\n",
      "Epoch: [2]  [ 440/2012]  eta: 0:13:31  lr: 0.000063  loss: 0.059503 (0.068469)  time: 0.498167  data: 0.460024  max mem: 211\n",
      "Epoch: [2]  [ 460/2012]  eta: 0:13:20  lr: 0.000063  loss: 0.013551 (0.068006)  time: 0.508078  data: 0.469933  max mem: 211\n",
      "Epoch: [2]  [ 480/2012]  eta: 0:13:08  lr: 0.000063  loss: 0.062677 (0.068631)  time: 0.487558  data: 0.449415  max mem: 211\n",
      "Epoch: [2]  [ 500/2012]  eta: 0:12:57  lr: 0.000063  loss: 0.042332 (0.070876)  time: 0.510627  data: 0.472509  max mem: 211\n",
      "Epoch: [2]  [ 520/2012]  eta: 0:12:46  lr: 0.000063  loss: 0.030824 (0.071282)  time: 0.494052  data: 0.455903  max mem: 211\n",
      "Epoch: [2]  [ 540/2012]  eta: 0:12:33  lr: 0.000063  loss: 0.033984 (0.071479)  time: 0.476527  data: 0.438383  max mem: 211\n",
      "Epoch: [2]  [ 560/2012]  eta: 0:12:21  lr: 0.000063  loss: 0.046995 (0.071873)  time: 0.472945  data: 0.434790  max mem: 211\n",
      "Epoch: [2]  [ 580/2012]  eta: 0:12:09  lr: 0.000063  loss: 0.052934 (0.072372)  time: 0.469609  data: 0.431496  max mem: 211\n",
      "Epoch: [2]  [ 600/2012]  eta: 0:11:56  lr: 0.000063  loss: 0.017810 (0.071314)  time: 0.458975  data: 0.420831  max mem: 211\n",
      "Epoch: [2]  [ 620/2012]  eta: 0:11:45  lr: 0.000063  loss: 0.036237 (0.071650)  time: 0.480462  data: 0.442337  max mem: 211\n",
      "Epoch: [2]  [ 640/2012]  eta: 0:11:33  lr: 0.000063  loss: 0.021361 (0.071409)  time: 0.456873  data: 0.418738  max mem: 211\n",
      "Epoch: [2]  [ 660/2012]  eta: 0:11:21  lr: 0.000063  loss: 0.011988 (0.070150)  time: 0.459927  data: 0.421805  max mem: 211\n",
      "Epoch: [2]  [ 680/2012]  eta: 0:11:09  lr: 0.000063  loss: 0.058832 (0.071337)  time: 0.462602  data: 0.424486  max mem: 211\n",
      "Epoch: [2]  [ 700/2012]  eta: 0:10:58  lr: 0.000063  loss: 0.057940 (0.072770)  time: 0.463538  data: 0.425418  max mem: 211\n",
      "Epoch: [2]  [ 720/2012]  eta: 0:10:46  lr: 0.000063  loss: 0.031155 (0.072884)  time: 0.467830  data: 0.429704  max mem: 211\n",
      "Epoch: [2]  [ 740/2012]  eta: 0:10:35  lr: 0.000063  loss: 0.037619 (0.072458)  time: 0.464011  data: 0.425884  max mem: 211\n",
      "Epoch: [2]  [ 760/2012]  eta: 0:10:24  lr: 0.000063  loss: 0.020229 (0.071500)  time: 0.471624  data: 0.433522  max mem: 211\n",
      "Epoch: [2]  [ 780/2012]  eta: 0:10:14  lr: 0.000063  loss: 0.021169 (0.071190)  time: 0.481989  data: 0.443865  max mem: 211\n",
      "Epoch: [2]  [ 800/2012]  eta: 0:10:02  lr: 0.000063  loss: 0.026221 (0.071476)  time: 0.448653  data: 0.410606  max mem: 211\n",
      "Epoch: [2]  [ 820/2012]  eta: 0:09:51  lr: 0.000063  loss: 0.042556 (0.071334)  time: 0.469429  data: 0.431391  max mem: 211\n",
      "Epoch: [2]  [ 840/2012]  eta: 0:09:41  lr: 0.000063  loss: 0.029400 (0.071516)  time: 0.486100  data: 0.448020  max mem: 211\n",
      "Epoch: [2]  [ 860/2012]  eta: 0:09:31  lr: 0.000063  loss: 0.027855 (0.071900)  time: 0.473283  data: 0.435138  max mem: 211\n",
      "Epoch: [2]  [ 880/2012]  eta: 0:09:20  lr: 0.000063  loss: 0.037516 (0.071962)  time: 0.473051  data: 0.434931  max mem: 211\n",
      "Epoch: [2]  [ 900/2012]  eta: 0:09:10  lr: 0.000063  loss: 0.036583 (0.071513)  time: 0.473606  data: 0.435427  max mem: 211\n",
      "Epoch: [2]  [ 920/2012]  eta: 0:08:59  lr: 0.000063  loss: 0.049218 (0.072046)  time: 0.480191  data: 0.442057  max mem: 211\n",
      "Epoch: [2]  [ 940/2012]  eta: 0:08:49  lr: 0.000063  loss: 0.019372 (0.071332)  time: 0.489098  data: 0.450961  max mem: 211\n",
      "Epoch: [2]  [ 960/2012]  eta: 0:08:39  lr: 0.000063  loss: 0.049971 (0.071580)  time: 0.470359  data: 0.432259  max mem: 211\n",
      "Epoch: [2]  [ 980/2012]  eta: 0:08:29  lr: 0.000063  loss: 0.020712 (0.071232)  time: 0.464866  data: 0.426719  max mem: 211\n",
      "Epoch: [2]  [1000/2012]  eta: 0:08:19  lr: 0.000063  loss: 0.032101 (0.071528)  time: 0.496290  data: 0.458176  max mem: 211\n",
      "Epoch: [2]  [1020/2012]  eta: 0:08:08  lr: 0.000063  loss: 0.030765 (0.072492)  time: 0.463409  data: 0.425248  max mem: 211\n",
      "Epoch: [2]  [1040/2012]  eta: 0:07:58  lr: 0.000063  loss: 0.022046 (0.072028)  time: 0.465283  data: 0.427149  max mem: 211\n",
      "Epoch: [2]  [1060/2012]  eta: 0:07:48  lr: 0.000063  loss: 0.039600 (0.071888)  time: 0.473125  data: 0.435010  max mem: 211\n",
      "Epoch: [2]  [1080/2012]  eta: 0:07:38  lr: 0.000063  loss: 0.007381 (0.071632)  time: 0.478295  data: 0.440162  max mem: 211\n",
      "Epoch: [2]  [1100/2012]  eta: 0:07:27  lr: 0.000063  loss: 0.021214 (0.071511)  time: 0.465617  data: 0.427455  max mem: 211\n",
      "Epoch: [2]  [1120/2012]  eta: 0:07:18  lr: 0.000063  loss: 0.018610 (0.071977)  time: 0.514680  data: 0.476545  max mem: 211\n",
      "Epoch: [2]  [1140/2012]  eta: 0:07:08  lr: 0.000063  loss: 0.047304 (0.072188)  time: 0.481394  data: 0.443263  max mem: 211\n",
      "Epoch: [2]  [1160/2012]  eta: 0:06:58  lr: 0.000063  loss: 0.019383 (0.071988)  time: 0.479906  data: 0.441791  max mem: 211\n",
      "Epoch: [2]  [1180/2012]  eta: 0:06:48  lr: 0.000063  loss: 0.017545 (0.071248)  time: 0.487190  data: 0.449074  max mem: 211\n",
      "Epoch: [2]  [1200/2012]  eta: 0:06:38  lr: 0.000063  loss: 0.054178 (0.071701)  time: 0.487335  data: 0.449227  max mem: 211\n",
      "Epoch: [2]  [1220/2012]  eta: 0:06:29  lr: 0.000063  loss: 0.013325 (0.071586)  time: 0.500304  data: 0.462127  max mem: 211\n",
      "Epoch: [2]  [1240/2012]  eta: 0:06:19  lr: 0.000063  loss: 0.018722 (0.071645)  time: 0.524259  data: 0.486110  max mem: 211\n",
      "Epoch: [2]  [1260/2012]  eta: 0:06:09  lr: 0.000063  loss: 0.014971 (0.071615)  time: 0.462469  data: 0.424368  max mem: 211\n",
      "Epoch: [2]  [1280/2012]  eta: 0:05:59  lr: 0.000063  loss: 0.016886 (0.071190)  time: 0.467855  data: 0.429806  max mem: 211\n",
      "Epoch: [2]  [1300/2012]  eta: 0:05:49  lr: 0.000063  loss: 0.012625 (0.070724)  time: 0.455569  data: 0.417570  max mem: 211\n",
      "Epoch: [2]  [1320/2012]  eta: 0:05:39  lr: 0.000063  loss: 0.025025 (0.070540)  time: 0.496277  data: 0.458166  max mem: 211\n",
      "Epoch: [2]  [1340/2012]  eta: 0:05:29  lr: 0.000063  loss: 0.027733 (0.070470)  time: 0.531857  data: 0.493733  max mem: 211\n",
      "Epoch: [2]  [1360/2012]  eta: 0:05:20  lr: 0.000063  loss: 0.018313 (0.070229)  time: 0.525020  data: 0.486863  max mem: 211\n",
      "Epoch: [2]  [1380/2012]  eta: 0:05:10  lr: 0.000063  loss: 0.007769 (0.070234)  time: 0.504997  data: 0.466884  max mem: 211\n",
      "Epoch: [2]  [1400/2012]  eta: 0:05:01  lr: 0.000063  loss: 0.012247 (0.069895)  time: 0.556108  data: 0.517859  max mem: 211\n",
      "Epoch: [2]  [1420/2012]  eta: 0:04:51  lr: 0.000063  loss: 0.045171 (0.070942)  time: 0.509601  data: 0.471473  max mem: 211\n",
      "Epoch: [2]  [1440/2012]  eta: 0:04:42  lr: 0.000063  loss: 0.019995 (0.070601)  time: 0.509975  data: 0.471840  max mem: 211\n",
      "Epoch: [2]  [1460/2012]  eta: 0:04:32  lr: 0.000063  loss: 0.030798 (0.070163)  time: 0.499550  data: 0.461444  max mem: 211\n",
      "Epoch: [2]  [1480/2012]  eta: 0:04:22  lr: 0.000063  loss: 0.028319 (0.069952)  time: 0.467639  data: 0.429497  max mem: 211\n",
      "Epoch: [2]  [1500/2012]  eta: 0:04:12  lr: 0.000063  loss: 0.011941 (0.069560)  time: 0.472132  data: 0.433991  max mem: 211\n",
      "Epoch: [2]  [1520/2012]  eta: 0:04:02  lr: 0.000063  loss: 0.021357 (0.069536)  time: 0.477636  data: 0.439506  max mem: 211\n",
      "Epoch: [2]  [1540/2012]  eta: 0:03:52  lr: 0.000063  loss: 0.024866 (0.069462)  time: 0.476835  data: 0.438713  max mem: 211\n",
      "Epoch: [2]  [1560/2012]  eta: 0:03:42  lr: 0.000063  loss: 0.034800 (0.069659)  time: 0.474800  data: 0.436664  max mem: 211\n",
      "Epoch: [2]  [1580/2012]  eta: 0:03:32  lr: 0.000063  loss: 0.025376 (0.070220)  time: 0.484517  data: 0.446356  max mem: 211\n",
      "Epoch: [2]  [1600/2012]  eta: 0:03:22  lr: 0.000063  loss: 0.027032 (0.069947)  time: 0.476725  data: 0.438595  max mem: 211\n",
      "Epoch: [2]  [1620/2012]  eta: 0:03:12  lr: 0.000063  loss: 0.023113 (0.069758)  time: 0.475564  data: 0.437437  max mem: 211\n",
      "Epoch: [2]  [1640/2012]  eta: 0:03:03  lr: 0.000063  loss: 0.042453 (0.069823)  time: 0.566013  data: 0.527892  max mem: 211\n",
      "Epoch: [2]  [1660/2012]  eta: 0:02:53  lr: 0.000063  loss: 0.034084 (0.069711)  time: 0.539145  data: 0.501035  max mem: 211\n",
      "Epoch: [2]  [1680/2012]  eta: 0:02:43  lr: 0.000063  loss: 0.008973 (0.069426)  time: 0.480144  data: 0.442023  max mem: 211\n",
      "Epoch: [2]  [1700/2012]  eta: 0:02:33  lr: 0.000063  loss: 0.045585 (0.069549)  time: 0.467702  data: 0.429570  max mem: 211\n",
      "Epoch: [2]  [1720/2012]  eta: 0:02:23  lr: 0.000063  loss: 0.028024 (0.069396)  time: 0.469830  data: 0.431732  max mem: 211\n",
      "Epoch: [2]  [1740/2012]  eta: 0:02:13  lr: 0.000063  loss: 0.031484 (0.069616)  time: 0.506894  data: 0.468842  max mem: 211\n",
      "Epoch: [2]  [1760/2012]  eta: 0:02:04  lr: 0.000063  loss: 0.019868 (0.069622)  time: 0.467625  data: 0.429603  max mem: 211\n",
      "Epoch: [2]  [1780/2012]  eta: 0:01:54  lr: 0.000063  loss: 0.032058 (0.069921)  time: 0.533776  data: 0.495622  max mem: 211\n",
      "Epoch: [2]  [1800/2012]  eta: 0:01:44  lr: 0.000063  loss: 0.012463 (0.069582)  time: 0.486881  data: 0.448756  max mem: 211\n",
      "Epoch: [2]  [1820/2012]  eta: 0:01:34  lr: 0.000063  loss: 0.034465 (0.069673)  time: 0.478028  data: 0.439872  max mem: 211\n",
      "Epoch: [2]  [1840/2012]  eta: 0:01:24  lr: 0.000063  loss: 0.030474 (0.069838)  time: 0.474387  data: 0.436284  max mem: 211\n",
      "Epoch: [2]  [1860/2012]  eta: 0:01:14  lr: 0.000063  loss: 0.037637 (0.070027)  time: 0.485641  data: 0.447516  max mem: 211\n",
      "Epoch: [2]  [1880/2012]  eta: 0:01:04  lr: 0.000063  loss: 0.030390 (0.070037)  time: 0.496936  data: 0.458808  max mem: 211\n",
      "Epoch: [2]  [1900/2012]  eta: 0:00:55  lr: 0.000063  loss: 0.014511 (0.069720)  time: 0.474809  data: 0.436708  max mem: 211\n",
      "Epoch: [2]  [1920/2012]  eta: 0:00:45  lr: 0.000063  loss: 0.028199 (0.069500)  time: 0.498999  data: 0.460877  max mem: 211\n",
      "Epoch: [2]  [1940/2012]  eta: 0:00:35  lr: 0.000063  loss: 0.019109 (0.069221)  time: 0.482665  data: 0.444523  max mem: 211\n",
      "Epoch: [2]  [1960/2012]  eta: 0:00:25  lr: 0.000063  loss: 0.013329 (0.069583)  time: 0.493394  data: 0.455281  max mem: 211\n",
      "Epoch: [2]  [1980/2012]  eta: 0:00:15  lr: 0.000063  loss: 0.031219 (0.069615)  time: 0.474704  data: 0.436601  max mem: 211\n",
      "Epoch: [2]  [2000/2012]  eta: 0:00:05  lr: 0.000063  loss: 0.019140 (0.069292)  time: 0.497489  data: 0.459383  max mem: 211\n",
      "Epoch: [2]  [2011/2012]  eta: 0:00:00  lr: 0.000063  loss: 0.032366 (0.069309)  time: 0.464966  data: 0.428192  max mem: 211\n",
      "Epoch: [2] Total time: 0:16:29 (0.491777 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 0.032366 (0.069309)\n",
      "Test:  [ 0/79]  eta: 0:00:37  loss: 0.006180 (0.006180)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.480270  data: 0.442019  max mem: 211\n",
      "Test:  [20/79]  eta: 0:00:30  loss: 0.064749 (0.105674)  acc1: 93.750000 (96.726190)  acc5: 100.000000 (100.000000)  time: 0.510449  data: 0.472301  max mem: 211\n",
      "Test:  [40/79]  eta: 0:00:20  loss: 0.057951 (0.084190)  acc1: 100.000000 (97.256098)  acc5: 100.000000 (100.000000)  time: 0.528801  data: 0.490662  max mem: 211\n",
      "Test:  [60/79]  eta: 0:00:09  loss: 0.068573 (0.089204)  acc1: 100.000000 (97.028689)  acc5: 100.000000 (100.000000)  time: 0.501176  data: 0.463043  max mem: 211\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 0.017033 (0.085295)  acc1: 100.000000 (97.600000)  acc5: 100.000000 (100.000000)  time: 0.496465  data: 0.459642  max mem: 211\n",
      "Test: Total time: 0:00:40 (0.509758 s / it)\n",
      "* Acc@1 97.600 Acc@5 100.000 loss 0.085\n",
      "Accuracy at epoch 2 of the network on the 79 test images: 97.6%\n",
      "Accuracy at epoch 2 of the network on the 1250 test images: 97.6%\n",
      "Max accuracy so far: 97.60%\n",
      "Epoch: [3]  [   0/2012]  eta: 0:15:35  lr: 0.000000  loss: 0.003600 (0.003600)  time: 0.465183  data: 0.427027  max mem: 211\n",
      "Epoch: [3]  [  20/2012]  eta: 0:16:14  lr: 0.000000  loss: 0.009508 (0.038205)  time: 0.490659  data: 0.452530  max mem: 211\n",
      "Epoch: [3]  [  40/2012]  eta: 0:16:45  lr: 0.000000  loss: 0.030892 (0.053350)  time: 0.531678  data: 0.493560  max mem: 211\n",
      "Epoch: [3]  [  60/2012]  eta: 0:16:07  lr: 0.000000  loss: 0.015574 (0.059352)  time: 0.466366  data: 0.428265  max mem: 211\n",
      "Epoch: [3]  [  80/2012]  eta: 0:15:44  lr: 0.000000  loss: 0.046274 (0.064307)  time: 0.467105  data: 0.428950  max mem: 211\n",
      "Epoch: [3]  [ 100/2012]  eta: 0:15:28  lr: 0.000000  loss: 0.019046 (0.057295)  time: 0.472556  data: 0.434447  max mem: 211\n",
      "Epoch: [3]  [ 120/2012]  eta: 0:15:07  lr: 0.000000  loss: 0.026059 (0.056625)  time: 0.450477  data: 0.412418  max mem: 211\n",
      "Epoch: [3]  [ 140/2012]  eta: 0:14:55  lr: 0.000000  loss: 0.034326 (0.058339)  time: 0.468633  data: 0.430597  max mem: 211\n",
      "Epoch: [3]  [ 160/2012]  eta: 0:14:44  lr: 0.000000  loss: 0.029218 (0.056910)  time: 0.475475  data: 0.437287  max mem: 211\n",
      "Epoch: [3]  [ 180/2012]  eta: 0:14:32  lr: 0.000000  loss: 0.023541 (0.058028)  time: 0.464835  data: 0.426707  max mem: 211\n",
      "Epoch: [3]  [ 200/2012]  eta: 0:14:22  lr: 0.000000  loss: 0.006622 (0.055576)  time: 0.473286  data: 0.435179  max mem: 211\n",
      "Epoch: [3]  [ 220/2012]  eta: 0:14:12  lr: 0.000000  loss: 0.028221 (0.056738)  time: 0.473441  data: 0.435306  max mem: 211\n",
      "Epoch: [3]  [ 240/2012]  eta: 0:14:08  lr: 0.000000  loss: 0.021684 (0.055589)  time: 0.511244  data: 0.473035  max mem: 211\n",
      "Epoch: [3]  [ 260/2012]  eta: 0:14:06  lr: 0.000000  loss: 0.018865 (0.054218)  time: 0.532909  data: 0.494799  max mem: 211\n",
      "Epoch: [3]  [ 280/2012]  eta: 0:13:58  lr: 0.000000  loss: 0.026977 (0.056421)  time: 0.501063  data: 0.462950  max mem: 211\n",
      "Epoch: [3]  [ 300/2012]  eta: 0:13:53  lr: 0.000000  loss: 0.047479 (0.057051)  time: 0.525526  data: 0.487372  max mem: 211\n",
      "Epoch: [3]  [ 320/2012]  eta: 0:13:51  lr: 0.000000  loss: 0.021277 (0.056149)  time: 0.558329  data: 0.520212  max mem: 211\n",
      "Epoch: [3]  [ 340/2012]  eta: 0:13:42  lr: 0.000000  loss: 0.027217 (0.056580)  time: 0.499959  data: 0.461842  max mem: 211\n",
      "Epoch: [3]  [ 360/2012]  eta: 0:13:36  lr: 0.000000  loss: 0.021522 (0.056750)  time: 0.535678  data: 0.497550  max mem: 211\n",
      "Epoch: [3]  [ 380/2012]  eta: 0:13:26  lr: 0.000000  loss: 0.010315 (0.056142)  time: 0.490327  data: 0.452197  max mem: 211\n",
      "Epoch: [3]  [ 400/2012]  eta: 0:13:15  lr: 0.000000  loss: 0.027218 (0.056917)  time: 0.481844  data: 0.443712  max mem: 211\n",
      "Epoch: [3]  [ 420/2012]  eta: 0:13:04  lr: 0.000000  loss: 0.007350 (0.055340)  time: 0.475830  data: 0.437690  max mem: 211\n",
      "Epoch: [3]  [ 440/2012]  eta: 0:12:52  lr: 0.000000  loss: 0.033942 (0.055380)  time: 0.469105  data: 0.430973  max mem: 211\n",
      "Epoch: [3]  [ 460/2012]  eta: 0:12:43  lr: 0.000000  loss: 0.010317 (0.054829)  time: 0.495944  data: 0.457836  max mem: 211\n",
      "Epoch: [3]  [ 480/2012]  eta: 0:12:33  lr: 0.000000  loss: 0.022922 (0.054989)  time: 0.493474  data: 0.455350  max mem: 211\n",
      "Epoch: [3]  [ 500/2012]  eta: 0:12:23  lr: 0.000000  loss: 0.066588 (0.056650)  time: 0.486945  data: 0.448796  max mem: 211\n",
      "Epoch: [3]  [ 520/2012]  eta: 0:12:13  lr: 0.000000  loss: 0.018139 (0.056882)  time: 0.482788  data: 0.444658  max mem: 211\n",
      "Epoch: [3]  [ 540/2012]  eta: 0:12:06  lr: 0.000000  loss: 0.029950 (0.056836)  time: 0.544125  data: 0.505957  max mem: 211\n",
      "Epoch: [3]  [ 560/2012]  eta: 0:11:58  lr: 0.000000  loss: 0.020898 (0.056736)  time: 0.532191  data: 0.494082  max mem: 211\n",
      "Epoch: [3]  [ 580/2012]  eta: 0:11:46  lr: 0.000000  loss: 0.036453 (0.057247)  time: 0.460001  data: 0.421975  max mem: 211\n",
      "Epoch: [3]  [ 600/2012]  eta: 0:11:35  lr: 0.000000  loss: 0.013033 (0.056416)  time: 0.472787  data: 0.434727  max mem: 211\n",
      "Epoch: [3]  [ 620/2012]  eta: 0:11:25  lr: 0.000000  loss: 0.014908 (0.056580)  time: 0.492280  data: 0.454142  max mem: 211\n",
      "Epoch: [3]  [ 640/2012]  eta: 0:11:15  lr: 0.000000  loss: 0.010955 (0.056227)  time: 0.481630  data: 0.443514  max mem: 211\n",
      "Epoch: [3]  [ 660/2012]  eta: 0:11:05  lr: 0.000000  loss: 0.012592 (0.055304)  time: 0.495142  data: 0.456981  max mem: 211\n",
      "Epoch: [3]  [ 680/2012]  eta: 0:10:55  lr: 0.000000  loss: 0.054012 (0.056015)  time: 0.478050  data: 0.439923  max mem: 211\n",
      "Epoch: [3]  [ 700/2012]  eta: 0:10:44  lr: 0.000000  loss: 0.067475 (0.057059)  time: 0.468603  data: 0.430468  max mem: 211\n",
      "Epoch: [3]  [ 720/2012]  eta: 0:10:34  lr: 0.000000  loss: 0.020208 (0.057309)  time: 0.486572  data: 0.448435  max mem: 211\n",
      "Epoch: [3]  [ 740/2012]  eta: 0:10:24  lr: 0.000000  loss: 0.031100 (0.057090)  time: 0.481854  data: 0.443700  max mem: 211\n",
      "Epoch: [3]  [ 760/2012]  eta: 0:10:15  lr: 0.000000  loss: 0.017383 (0.056447)  time: 0.520957  data: 0.482842  max mem: 211\n",
      "Epoch: [3]  [ 780/2012]  eta: 0:10:05  lr: 0.000000  loss: 0.023912 (0.056289)  time: 0.493378  data: 0.455251  max mem: 211\n",
      "Epoch: [3]  [ 800/2012]  eta: 0:09:55  lr: 0.000000  loss: 0.017862 (0.056529)  time: 0.481905  data: 0.443767  max mem: 211\n",
      "Epoch: [3]  [ 820/2012]  eta: 0:09:45  lr: 0.000000  loss: 0.050289 (0.056430)  time: 0.489566  data: 0.451429  max mem: 211\n",
      "Epoch: [3]  [ 840/2012]  eta: 0:09:36  lr: 0.000000  loss: 0.021370 (0.056560)  time: 0.501692  data: 0.463580  max mem: 211\n",
      "Epoch: [3]  [ 860/2012]  eta: 0:09:27  lr: 0.000000  loss: 0.023749 (0.056641)  time: 0.521589  data: 0.483470  max mem: 211\n",
      "Epoch: [3]  [ 880/2012]  eta: 0:09:17  lr: 0.000000  loss: 0.021738 (0.056740)  time: 0.493061  data: 0.454954  max mem: 211\n",
      "Epoch: [3]  [ 900/2012]  eta: 0:09:08  lr: 0.000000  loss: 0.020590 (0.056353)  time: 0.516601  data: 0.478471  max mem: 211\n",
      "Epoch: [3]  [ 920/2012]  eta: 0:08:58  lr: 0.000000  loss: 0.036567 (0.056661)  time: 0.487136  data: 0.449004  max mem: 211\n",
      "Epoch: [3]  [ 940/2012]  eta: 0:08:49  lr: 0.000000  loss: 0.020678 (0.056174)  time: 0.531707  data: 0.493611  max mem: 211\n",
      "Epoch: [3]  [ 960/2012]  eta: 0:08:38  lr: 0.000000  loss: 0.055749 (0.056865)  time: 0.470245  data: 0.432096  max mem: 211\n",
      "Epoch: [3]  [ 980/2012]  eta: 0:08:28  lr: 0.000000  loss: 0.040700 (0.057096)  time: 0.487578  data: 0.449443  max mem: 211\n",
      "Epoch: [3]  [1000/2012]  eta: 0:08:20  lr: 0.000000  loss: 0.025287 (0.057140)  time: 0.565441  data: 0.527308  max mem: 211\n",
      "Epoch: [3]  [1020/2012]  eta: 0:08:10  lr: 0.000000  loss: 0.036310 (0.057662)  time: 0.464563  data: 0.426509  max mem: 211\n",
      "Epoch: [3]  [1040/2012]  eta: 0:07:59  lr: 0.000000  loss: 0.014531 (0.057241)  time: 0.452833  data: 0.414763  max mem: 211\n",
      "Epoch: [3]  [1060/2012]  eta: 0:07:49  lr: 0.000000  loss: 0.040839 (0.057269)  time: 0.502075  data: 0.464012  max mem: 211\n",
      "Epoch: [3]  [1080/2012]  eta: 0:07:39  lr: 0.000000  loss: 0.005158 (0.057187)  time: 0.473683  data: 0.435542  max mem: 211\n",
      "Epoch: [3]  [1100/2012]  eta: 0:07:29  lr: 0.000000  loss: 0.014396 (0.057011)  time: 0.481601  data: 0.443489  max mem: 211\n",
      "Epoch: [3]  [1120/2012]  eta: 0:07:19  lr: 0.000000  loss: 0.011872 (0.057394)  time: 0.467606  data: 0.429474  max mem: 211\n",
      "Epoch: [3]  [1140/2012]  eta: 0:07:09  lr: 0.000000  loss: 0.036419 (0.057518)  time: 0.491467  data: 0.453296  max mem: 211\n",
      "Epoch: [3]  [1160/2012]  eta: 0:07:00  lr: 0.000000  loss: 0.019895 (0.057384)  time: 0.554222  data: 0.516092  max mem: 211\n",
      "Epoch: [3]  [1180/2012]  eta: 0:06:50  lr: 0.000000  loss: 0.012028 (0.056959)  time: 0.522108  data: 0.483996  max mem: 211\n",
      "Epoch: [3]  [1200/2012]  eta: 0:06:41  lr: 0.000000  loss: 0.058910 (0.057431)  time: 0.525749  data: 0.487622  max mem: 211\n",
      "Epoch: [3]  [1220/2012]  eta: 0:06:31  lr: 0.000000  loss: 0.015148 (0.057453)  time: 0.487047  data: 0.448915  max mem: 211\n",
      "Epoch: [3]  [1240/2012]  eta: 0:06:22  lr: 0.000000  loss: 0.015378 (0.057388)  time: 0.540472  data: 0.502347  max mem: 211\n",
      "Epoch: [3]  [1260/2012]  eta: 0:06:12  lr: 0.000000  loss: 0.010592 (0.057357)  time: 0.477414  data: 0.439314  max mem: 211\n",
      "Epoch: [3]  [1280/2012]  eta: 0:06:02  lr: 0.000000  loss: 0.016861 (0.057168)  time: 0.517397  data: 0.479290  max mem: 211\n",
      "Epoch: [3]  [1300/2012]  eta: 0:05:52  lr: 0.000000  loss: 0.009491 (0.056845)  time: 0.483400  data: 0.445272  max mem: 211\n",
      "Epoch: [3]  [1320/2012]  eta: 0:05:42  lr: 0.000000  loss: 0.026272 (0.056814)  time: 0.491488  data: 0.453362  max mem: 211\n",
      "Epoch: [3]  [1340/2012]  eta: 0:05:32  lr: 0.000000  loss: 0.025679 (0.056649)  time: 0.515898  data: 0.477777  max mem: 211\n",
      "Epoch: [3]  [1360/2012]  eta: 0:05:23  lr: 0.000000  loss: 0.024966 (0.056505)  time: 0.549213  data: 0.511085  max mem: 211\n",
      "Epoch: [3]  [1380/2012]  eta: 0:05:13  lr: 0.000000  loss: 0.009605 (0.056499)  time: 0.496015  data: 0.457901  max mem: 211\n",
      "Epoch: [3]  [1400/2012]  eta: 0:05:03  lr: 0.000000  loss: 0.007985 (0.056194)  time: 0.475873  data: 0.437766  max mem: 211\n",
      "Epoch: [3]  [1420/2012]  eta: 0:04:53  lr: 0.000000  loss: 0.030790 (0.056965)  time: 0.496044  data: 0.457929  max mem: 211\n",
      "Epoch: [3]  [1440/2012]  eta: 0:04:43  lr: 0.000000  loss: 0.015318 (0.056683)  time: 0.481896  data: 0.443729  max mem: 211\n",
      "Epoch: [3]  [1460/2012]  eta: 0:04:33  lr: 0.000000  loss: 0.019720 (0.056383)  time: 0.533903  data: 0.495798  max mem: 211\n",
      "Epoch: [3]  [1480/2012]  eta: 0:04:23  lr: 0.000000  loss: 0.024277 (0.056284)  time: 0.452448  data: 0.414386  max mem: 211\n",
      "Epoch: [3]  [1500/2012]  eta: 0:04:13  lr: 0.000000  loss: 0.029838 (0.056079)  time: 0.461863  data: 0.423823  max mem: 211\n",
      "Epoch: [3]  [1520/2012]  eta: 0:04:03  lr: 0.000000  loss: 0.016986 (0.055970)  time: 0.481652  data: 0.443534  max mem: 211\n",
      "Epoch: [3]  [1540/2012]  eta: 0:03:53  lr: 0.000000  loss: 0.032077 (0.055872)  time: 0.479930  data: 0.441784  max mem: 211\n",
      "Epoch: [3]  [1560/2012]  eta: 0:03:43  lr: 0.000000  loss: 0.019866 (0.055941)  time: 0.553900  data: 0.515786  max mem: 211\n",
      "Epoch: [3]  [1580/2012]  eta: 0:03:34  lr: 0.000000  loss: 0.026858 (0.056402)  time: 0.519036  data: 0.480929  max mem: 211\n",
      "Epoch: [3]  [1600/2012]  eta: 0:03:24  lr: 0.000000  loss: 0.023661 (0.056265)  time: 0.529105  data: 0.490963  max mem: 211\n",
      "Epoch: [3]  [1620/2012]  eta: 0:03:14  lr: 0.000000  loss: 0.028841 (0.056115)  time: 0.509634  data: 0.471506  max mem: 211\n",
      "Epoch: [3]  [1640/2012]  eta: 0:03:04  lr: 0.000000  loss: 0.029731 (0.056238)  time: 0.486424  data: 0.448319  max mem: 211\n",
      "Epoch: [3]  [1660/2012]  eta: 0:02:54  lr: 0.000000  loss: 0.032154 (0.056262)  time: 0.475841  data: 0.437740  max mem: 211\n",
      "Epoch: [3]  [1680/2012]  eta: 0:02:44  lr: 0.000000  loss: 0.016911 (0.056048)  time: 0.467086  data: 0.428943  max mem: 211\n",
      "Epoch: [3]  [1700/2012]  eta: 0:02:34  lr: 0.000000  loss: 0.025945 (0.056039)  time: 0.503928  data: 0.465808  max mem: 211\n",
      "Epoch: [3]  [1720/2012]  eta: 0:02:24  lr: 0.000000  loss: 0.018659 (0.055798)  time: 0.537077  data: 0.498927  max mem: 211\n",
      "Epoch: [3]  [1740/2012]  eta: 0:02:14  lr: 0.000000  loss: 0.026275 (0.055964)  time: 0.485901  data: 0.447781  max mem: 211\n",
      "Epoch: [3]  [1760/2012]  eta: 0:02:04  lr: 0.000000  loss: 0.019765 (0.055994)  time: 0.485053  data: 0.446927  max mem: 211\n",
      "Epoch: [3]  [1780/2012]  eta: 0:01:55  lr: 0.000000  loss: 0.024555 (0.056216)  time: 0.528430  data: 0.490307  max mem: 211\n",
      "Epoch: [3]  [1800/2012]  eta: 0:01:45  lr: 0.000000  loss: 0.011816 (0.056000)  time: 0.505385  data: 0.467284  max mem: 211\n",
      "Epoch: [3]  [1820/2012]  eta: 0:01:35  lr: 0.000000  loss: 0.015423 (0.055872)  time: 0.495031  data: 0.456942  max mem: 211\n",
      "Epoch: [3]  [1840/2012]  eta: 0:01:25  lr: 0.000000  loss: 0.024642 (0.055960)  time: 0.475618  data: 0.437470  max mem: 211\n",
      "Epoch: [3]  [1860/2012]  eta: 0:01:15  lr: 0.000000  loss: 0.028956 (0.056100)  time: 0.470932  data: 0.432810  max mem: 211\n",
      "Epoch: [3]  [1880/2012]  eta: 0:01:05  lr: 0.000000  loss: 0.031389 (0.056104)  time: 0.471567  data: 0.433440  max mem: 211\n",
      "Epoch: [3]  [1900/2012]  eta: 0:00:55  lr: 0.000000  loss: 0.019559 (0.055802)  time: 0.537279  data: 0.499146  max mem: 211\n",
      "Epoch: [3]  [1920/2012]  eta: 0:00:45  lr: 0.000000  loss: 0.024595 (0.055556)  time: 0.512752  data: 0.474678  max mem: 211\n",
      "Epoch: [3]  [1940/2012]  eta: 0:00:35  lr: 0.000000  loss: 0.011221 (0.055276)  time: 0.529335  data: 0.491295  max mem: 211\n",
      "Epoch: [3]  [1960/2012]  eta: 0:00:25  lr: 0.000000  loss: 0.012080 (0.055450)  time: 0.521597  data: 0.483578  max mem: 211\n",
      "Epoch: [3]  [1980/2012]  eta: 0:00:15  lr: 0.000000  loss: 0.032499 (0.055303)  time: 0.533256  data: 0.495097  max mem: 211\n",
      "Epoch: [3]  [2000/2012]  eta: 0:00:05  lr: 0.000000  loss: 0.014571 (0.054978)  time: 0.511535  data: 0.473386  max mem: 211\n",
      "Epoch: [3]  [2011/2012]  eta: 0:00:00  lr: 0.000000  loss: 0.022406 (0.054966)  time: 0.476461  data: 0.439618  max mem: 211\n",
      "Epoch: [3] Total time: 0:16:40 (0.497086 s / it)\n",
      "Averaged stats: lr: 0.000000  loss: 0.022406 (0.054966)\n",
      "Test:  [ 0/79]  eta: 0:00:38  loss: 0.006180 (0.006180)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.488317  data: 0.450113  max mem: 211\n",
      "Test:  [20/79]  eta: 0:00:32  loss: 0.064749 (0.105674)  acc1: 93.750000 (96.726190)  acc5: 100.000000 (100.000000)  time: 0.547606  data: 0.509498  max mem: 211\n",
      "Test:  [40/79]  eta: 0:00:21  loss: 0.057951 (0.084190)  acc1: 100.000000 (97.256098)  acc5: 100.000000 (100.000000)  time: 0.535859  data: 0.497712  max mem: 211\n",
      "Test:  [60/79]  eta: 0:00:10  loss: 0.068573 (0.089204)  acc1: 100.000000 (97.028689)  acc5: 100.000000 (100.000000)  time: 0.504568  data: 0.466430  max mem: 211\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 0.017033 (0.085295)  acc1: 100.000000 (97.600000)  acc5: 100.000000 (100.000000)  time: 0.473925  data: 0.437110  max mem: 211\n",
      "Test: Total time: 0:00:40 (0.516050 s / it)\n",
      "* Acc@1 97.600 Acc@5 100.000 loss 0.085\n",
      "Accuracy at epoch 3 of the network on the 79 test images: 97.6%\n",
      "Accuracy at epoch 3 of the network on the 1250 test images: 97.6%\n",
      "Max accuracy so far: 97.60%\n",
      "Epoch: [4]  [   0/2012]  eta: 0:15:29  lr: 0.000063  loss: 0.003600 (0.003600)  time: 0.462043  data: 0.423894  max mem: 211\n",
      "Epoch: [4]  [  20/2012]  eta: 0:16:17  lr: 0.000063  loss: 0.009492 (0.037619)  time: 0.492328  data: 0.454170  max mem: 211\n",
      "Epoch: [4]  [  40/2012]  eta: 0:16:15  lr: 0.000063  loss: 0.028145 (0.051514)  time: 0.498349  data: 0.460196  max mem: 211\n",
      "Epoch: [4]  [  60/2012]  eta: 0:16:13  lr: 0.000063  loss: 0.015275 (0.060694)  time: 0.507618  data: 0.469494  max mem: 211\n",
      "Epoch: [4]  [  80/2012]  eta: 0:15:53  lr: 0.000063  loss: 0.033200 (0.061361)  time: 0.477290  data: 0.439190  max mem: 211\n",
      "Epoch: [4]  [ 100/2012]  eta: 0:15:48  lr: 0.000063  loss: 0.012312 (0.054097)  time: 0.506314  data: 0.468215  max mem: 211\n",
      "Epoch: [4]  [ 120/2012]  eta: 0:15:36  lr: 0.000063  loss: 0.026422 (0.055009)  time: 0.491174  data: 0.453054  max mem: 211\n",
      "Epoch: [4]  [ 140/2012]  eta: 0:15:26  lr: 0.000063  loss: 0.034937 (0.057157)  time: 0.492172  data: 0.454056  max mem: 211\n",
      "Epoch: [4]  [ 160/2012]  eta: 0:15:32  lr: 0.000063  loss: 0.021426 (0.055744)  time: 0.565884  data: 0.527760  max mem: 211\n",
      "Epoch: [4]  [ 180/2012]  eta: 0:15:14  lr: 0.000063  loss: 0.018799 (0.055746)  time: 0.464596  data: 0.426467  max mem: 211\n",
      "Epoch: [4]  [ 200/2012]  eta: 0:14:59  lr: 0.000063  loss: 0.010096 (0.053638)  time: 0.472757  data: 0.434623  max mem: 211\n",
      "Epoch: [4]  [ 220/2012]  eta: 0:14:47  lr: 0.000063  loss: 0.033122 (0.053651)  time: 0.478165  data: 0.440059  max mem: 211\n",
      "Epoch: [4]  [ 240/2012]  eta: 0:14:42  lr: 0.000063  loss: 0.023429 (0.052342)  time: 0.533465  data: 0.495332  max mem: 211\n",
      "Epoch: [4]  [ 260/2012]  eta: 0:14:49  lr: 0.000063  loss: 0.031591 (0.051004)  time: 0.619963  data: 0.581822  max mem: 211\n",
      "Epoch: [4]  [ 280/2012]  eta: 0:14:37  lr: 0.000063  loss: 0.023539 (0.052159)  time: 0.498725  data: 0.460672  max mem: 211\n",
      "Epoch: [4]  [ 300/2012]  eta: 0:14:32  lr: 0.000063  loss: 0.030350 (0.051494)  time: 0.549176  data: 0.511140  max mem: 211\n",
      "Epoch: [4]  [ 320/2012]  eta: 0:14:19  lr: 0.000063  loss: 0.012550 (0.051408)  time: 0.484111  data: 0.446030  max mem: 211\n",
      "Epoch: [4]  [ 340/2012]  eta: 0:14:07  lr: 0.000063  loss: 0.017149 (0.051692)  time: 0.487002  data: 0.448847  max mem: 211\n",
      "Epoch: [4]  [ 360/2012]  eta: 0:14:05  lr: 0.000063  loss: 0.015036 (0.052064)  time: 0.592168  data: 0.554022  max mem: 211\n",
      "Epoch: [4]  [ 380/2012]  eta: 0:13:53  lr: 0.000063  loss: 0.008271 (0.051987)  time: 0.491264  data: 0.453146  max mem: 211\n",
      "Epoch: [4]  [ 400/2012]  eta: 0:13:40  lr: 0.000063  loss: 0.036893 (0.052437)  time: 0.479674  data: 0.441578  max mem: 211\n",
      "Epoch: [4]  [ 420/2012]  eta: 0:13:27  lr: 0.000063  loss: 0.006644 (0.050967)  time: 0.476266  data: 0.438156  max mem: 211\n",
      "Epoch: [4]  [ 440/2012]  eta: 0:13:14  lr: 0.000063  loss: 0.029580 (0.050857)  time: 0.468887  data: 0.430755  max mem: 211\n",
      "Epoch: [4]  [ 460/2012]  eta: 0:13:03  lr: 0.000063  loss: 0.009167 (0.050751)  time: 0.490476  data: 0.452368  max mem: 211\n",
      "Epoch: [4]  [ 480/2012]  eta: 0:12:51  lr: 0.000063  loss: 0.029968 (0.051210)  time: 0.467114  data: 0.429004  max mem: 211\n",
      "Epoch: [4]  [ 500/2012]  eta: 0:12:43  lr: 0.000063  loss: 0.035518 (0.052810)  time: 0.536264  data: 0.498121  max mem: 211\n",
      "Epoch: [4]  [ 520/2012]  eta: 0:12:31  lr: 0.000063  loss: 0.023884 (0.053184)  time: 0.468942  data: 0.430793  max mem: 211\n",
      "Epoch: [4]  [ 540/2012]  eta: 0:12:19  lr: 0.000063  loss: 0.021676 (0.053238)  time: 0.477753  data: 0.439626  max mem: 211\n",
      "Epoch: [4]  [ 560/2012]  eta: 0:12:08  lr: 0.000063  loss: 0.028069 (0.053506)  time: 0.473160  data: 0.435021  max mem: 211\n",
      "Epoch: [4]  [ 580/2012]  eta: 0:11:56  lr: 0.000063  loss: 0.042072 (0.053932)  time: 0.472390  data: 0.434261  max mem: 211\n",
      "Epoch: [4]  [ 600/2012]  eta: 0:11:44  lr: 0.000063  loss: 0.009897 (0.053129)  time: 0.466297  data: 0.428163  max mem: 211\n",
      "Epoch: [4]  [ 620/2012]  eta: 0:11:34  lr: 0.000063  loss: 0.026515 (0.053439)  time: 0.488735  data: 0.450612  max mem: 211\n",
      "Epoch: [4]  [ 640/2012]  eta: 0:11:23  lr: 0.000063  loss: 0.010954 (0.053293)  time: 0.477122  data: 0.439014  max mem: 211\n",
      "Epoch: [4]  [ 660/2012]  eta: 0:11:12  lr: 0.000063  loss: 0.008750 (0.052313)  time: 0.474459  data: 0.436337  max mem: 211\n",
      "Epoch: [4]  [ 680/2012]  eta: 0:11:01  lr: 0.000063  loss: 0.043976 (0.053289)  time: 0.465029  data: 0.426782  max mem: 211\n",
      "Epoch: [4]  [ 700/2012]  eta: 0:10:51  lr: 0.000063  loss: 0.045588 (0.054465)  time: 0.487333  data: 0.449220  max mem: 211\n",
      "Epoch: [4]  [ 720/2012]  eta: 0:10:42  lr: 0.000063  loss: 0.016254 (0.054693)  time: 0.519697  data: 0.481579  max mem: 211\n",
      "Epoch: [4]  [ 740/2012]  eta: 0:10:30  lr: 0.000063  loss: 0.022660 (0.054329)  time: 0.462725  data: 0.424643  max mem: 211\n",
      "Epoch: [4]  [ 760/2012]  eta: 0:10:22  lr: 0.000063  loss: 0.010576 (0.053642)  time: 0.546028  data: 0.507990  max mem: 211\n",
      "Epoch: [4]  [ 780/2012]  eta: 0:10:12  lr: 0.000063  loss: 0.014388 (0.053456)  time: 0.493275  data: 0.455212  max mem: 211\n",
      "Epoch: [4]  [ 800/2012]  eta: 0:10:04  lr: 0.000063  loss: 0.016072 (0.053727)  time: 0.571169  data: 0.533019  max mem: 211\n",
      "Epoch: [4]  [ 820/2012]  eta: 0:09:54  lr: 0.000063  loss: 0.028679 (0.053663)  time: 0.487460  data: 0.449331  max mem: 211\n",
      "Epoch: [4]  [ 840/2012]  eta: 0:09:44  lr: 0.000063  loss: 0.022200 (0.053707)  time: 0.508056  data: 0.469938  max mem: 211\n",
      "Epoch: [4]  [ 860/2012]  eta: 0:09:34  lr: 0.000063  loss: 0.024343 (0.054137)  time: 0.503487  data: 0.465364  max mem: 211\n",
      "Epoch: [4]  [ 880/2012]  eta: 0:09:25  lr: 0.000063  loss: 0.022653 (0.054200)  time: 0.509791  data: 0.471665  max mem: 211\n",
      "Epoch: [4]  [ 900/2012]  eta: 0:09:16  lr: 0.000063  loss: 0.029308 (0.053877)  time: 0.555469  data: 0.517340  max mem: 211\n",
      "Epoch: [4]  [ 920/2012]  eta: 0:09:06  lr: 0.000063  loss: 0.039394 (0.054233)  time: 0.494508  data: 0.456403  max mem: 211\n",
      "Epoch: [4]  [ 940/2012]  eta: 0:08:56  lr: 0.000063  loss: 0.012745 (0.053659)  time: 0.481901  data: 0.443791  max mem: 211\n",
      "Epoch: [4]  [ 960/2012]  eta: 0:08:45  lr: 0.000063  loss: 0.032216 (0.053933)  time: 0.468848  data: 0.430686  max mem: 211\n",
      "Epoch: [4]  [ 980/2012]  eta: 0:08:35  lr: 0.000063  loss: 0.015984 (0.053673)  time: 0.521484  data: 0.483368  max mem: 211\n",
      "Epoch: [4]  [1000/2012]  eta: 0:08:25  lr: 0.000063  loss: 0.021394 (0.053867)  time: 0.487125  data: 0.449026  max mem: 211\n",
      "Epoch: [4]  [1020/2012]  eta: 0:08:15  lr: 0.000063  loss: 0.029352 (0.054693)  time: 0.477545  data: 0.439435  max mem: 211\n",
      "Epoch: [4]  [1040/2012]  eta: 0:08:04  lr: 0.000063  loss: 0.012560 (0.054342)  time: 0.483418  data: 0.445255  max mem: 211\n",
      "Epoch: [4]  [1060/2012]  eta: 0:07:54  lr: 0.000063  loss: 0.028478 (0.054151)  time: 0.487565  data: 0.449442  max mem: 211\n",
      "Epoch: [4]  [1080/2012]  eta: 0:07:44  lr: 0.000063  loss: 0.005226 (0.054077)  time: 0.498183  data: 0.460051  max mem: 211\n",
      "Epoch: [4]  [1100/2012]  eta: 0:07:34  lr: 0.000063  loss: 0.016802 (0.053962)  time: 0.472907  data: 0.434793  max mem: 211\n",
      "Epoch: [4]  [1120/2012]  eta: 0:07:24  lr: 0.000063  loss: 0.011477 (0.054498)  time: 0.484753  data: 0.446639  max mem: 211\n",
      "Epoch: [4]  [1140/2012]  eta: 0:07:15  lr: 0.000063  loss: 0.031665 (0.054673)  time: 0.568509  data: 0.530371  max mem: 211\n",
      "Epoch: [4]  [1160/2012]  eta: 0:07:05  lr: 0.000063  loss: 0.017599 (0.054571)  time: 0.501365  data: 0.463231  max mem: 211\n",
      "Epoch: [4]  [1180/2012]  eta: 0:06:55  lr: 0.000063  loss: 0.014306 (0.054006)  time: 0.529609  data: 0.491480  max mem: 211\n",
      "Epoch: [4]  [1200/2012]  eta: 0:06:46  lr: 0.000063  loss: 0.037315 (0.054340)  time: 0.518485  data: 0.480454  max mem: 211\n",
      "Epoch: [4]  [1220/2012]  eta: 0:06:35  lr: 0.000063  loss: 0.009919 (0.054272)  time: 0.490570  data: 0.452532  max mem: 211\n",
      "Epoch: [4]  [1240/2012]  eta: 0:06:26  lr: 0.000063  loss: 0.015122 (0.054316)  time: 0.516221  data: 0.478072  max mem: 211\n",
      "Epoch: [4]  [1260/2012]  eta: 0:06:16  lr: 0.000063  loss: 0.011163 (0.054342)  time: 0.523048  data: 0.484908  max mem: 211\n",
      "Epoch: [4]  [1280/2012]  eta: 0:06:06  lr: 0.000063  loss: 0.014179 (0.054027)  time: 0.470870  data: 0.432728  max mem: 211\n",
      "Epoch: [4]  [1300/2012]  eta: 0:05:55  lr: 0.000063  loss: 0.008473 (0.053682)  time: 0.489631  data: 0.451485  max mem: 211\n",
      "Epoch: [4]  [1320/2012]  eta: 0:05:46  lr: 0.000063  loss: 0.018787 (0.053558)  time: 0.513623  data: 0.475501  max mem: 211\n",
      "Epoch: [4]  [1340/2012]  eta: 0:05:35  lr: 0.000063  loss: 0.019410 (0.053487)  time: 0.470379  data: 0.432275  max mem: 211\n",
      "Epoch: [4]  [1360/2012]  eta: 0:05:25  lr: 0.000063  loss: 0.011468 (0.053338)  time: 0.467526  data: 0.429374  max mem: 211\n",
      "Epoch: [4]  [1380/2012]  eta: 0:05:15  lr: 0.000063  loss: 0.004870 (0.053404)  time: 0.499062  data: 0.460945  max mem: 211\n",
      "Epoch: [4]  [1400/2012]  eta: 0:05:05  lr: 0.000063  loss: 0.009009 (0.053168)  time: 0.467627  data: 0.429456  max mem: 211\n",
      "Epoch: [4]  [1420/2012]  eta: 0:04:54  lr: 0.000063  loss: 0.034312 (0.054224)  time: 0.464274  data: 0.426129  max mem: 211\n",
      "Epoch: [4]  [1440/2012]  eta: 0:04:44  lr: 0.000063  loss: 0.015006 (0.053945)  time: 0.473903  data: 0.435769  max mem: 211\n",
      "Epoch: [4]  [1460/2012]  eta: 0:04:34  lr: 0.000063  loss: 0.018036 (0.053597)  time: 0.470738  data: 0.432597  max mem: 211\n",
      "Epoch: [4]  [1480/2012]  eta: 0:04:24  lr: 0.000063  loss: 0.023375 (0.053422)  time: 0.510014  data: 0.471900  max mem: 211\n",
      "Epoch: [4]  [1500/2012]  eta: 0:04:15  lr: 0.000063  loss: 0.010209 (0.053136)  time: 0.602873  data: 0.564764  max mem: 211\n",
      "Epoch: [4]  [1520/2012]  eta: 0:04:05  lr: 0.000063  loss: 0.015585 (0.053099)  time: 0.536510  data: 0.498365  max mem: 211\n",
      "Epoch: [4]  [1540/2012]  eta: 0:03:55  lr: 0.000063  loss: 0.022277 (0.053042)  time: 0.510499  data: 0.472372  max mem: 211\n",
      "Epoch: [4]  [1560/2012]  eta: 0:03:46  lr: 0.000063  loss: 0.023127 (0.053242)  time: 0.546805  data: 0.508674  max mem: 211\n",
      "Epoch: [4]  [1580/2012]  eta: 0:03:36  lr: 0.000063  loss: 0.021125 (0.053766)  time: 0.527376  data: 0.489265  max mem: 211\n",
      "Epoch: [4]  [1600/2012]  eta: 0:03:26  lr: 0.000063  loss: 0.021988 (0.053564)  time: 0.514026  data: 0.475882  max mem: 211\n",
      "Epoch: [4]  [1620/2012]  eta: 0:03:16  lr: 0.000063  loss: 0.018096 (0.053425)  time: 0.477377  data: 0.439282  max mem: 211\n",
      "Epoch: [4]  [1640/2012]  eta: 0:03:06  lr: 0.000063  loss: 0.027763 (0.053506)  time: 0.461952  data: 0.423879  max mem: 211\n",
      "Epoch: [4]  [1660/2012]  eta: 0:02:55  lr: 0.000063  loss: 0.029215 (0.053415)  time: 0.467867  data: 0.429786  max mem: 211\n",
      "Epoch: [4]  [1680/2012]  eta: 0:02:45  lr: 0.000063  loss: 0.006132 (0.053223)  time: 0.465425  data: 0.427401  max mem: 211\n",
      "Epoch: [4]  [1700/2012]  eta: 0:02:35  lr: 0.000063  loss: 0.033552 (0.053345)  time: 0.483969  data: 0.445805  max mem: 211\n",
      "Epoch: [4]  [1720/2012]  eta: 0:02:25  lr: 0.000063  loss: 0.018446 (0.053174)  time: 0.498591  data: 0.460467  max mem: 211\n",
      "Epoch: [4]  [1740/2012]  eta: 0:02:15  lr: 0.000063  loss: 0.021073 (0.053392)  time: 0.527865  data: 0.489754  max mem: 211\n",
      "Epoch: [4]  [1760/2012]  eta: 0:02:05  lr: 0.000063  loss: 0.012688 (0.053386)  time: 0.488265  data: 0.450158  max mem: 211\n",
      "Epoch: [4]  [1780/2012]  eta: 0:01:55  lr: 0.000063  loss: 0.023648 (0.053700)  time: 0.537364  data: 0.499232  max mem: 211\n",
      "Epoch: [4]  [1800/2012]  eta: 0:01:46  lr: 0.000063  loss: 0.007668 (0.053445)  time: 0.527048  data: 0.488931  max mem: 211\n",
      "Epoch: [4]  [1820/2012]  eta: 0:01:35  lr: 0.000063  loss: 0.030751 (0.053525)  time: 0.489409  data: 0.451261  max mem: 211\n",
      "Epoch: [4]  [1840/2012]  eta: 0:01:26  lr: 0.000063  loss: 0.018717 (0.053743)  time: 0.510461  data: 0.472348  max mem: 211\n",
      "Epoch: [4]  [1860/2012]  eta: 0:01:15  lr: 0.000063  loss: 0.028200 (0.053882)  time: 0.471443  data: 0.433301  max mem: 211\n",
      "Epoch: [4]  [1880/2012]  eta: 0:01:05  lr: 0.000063  loss: 0.022269 (0.053893)  time: 0.491378  data: 0.453235  max mem: 211\n",
      "Epoch: [4]  [1900/2012]  eta: 0:00:55  lr: 0.000063  loss: 0.013572 (0.053646)  time: 0.491302  data: 0.453167  max mem: 211\n",
      "Epoch: [4]  [1920/2012]  eta: 0:00:45  lr: 0.000063  loss: 0.018515 (0.053472)  time: 0.523858  data: 0.485713  max mem: 211\n",
      "Epoch: [4]  [1940/2012]  eta: 0:00:35  lr: 0.000063  loss: 0.014792 (0.053265)  time: 0.469240  data: 0.431115  max mem: 211\n",
      "Epoch: [4]  [1960/2012]  eta: 0:00:25  lr: 0.000063  loss: 0.009645 (0.053542)  time: 0.469107  data: 0.430981  max mem: 211\n",
      "Epoch: [4]  [1980/2012]  eta: 0:00:15  lr: 0.000063  loss: 0.029118 (0.053558)  time: 0.465374  data: 0.427249  max mem: 211\n",
      "Epoch: [4]  [2000/2012]  eta: 0:00:05  lr: 0.000063  loss: 0.016491 (0.053302)  time: 0.477235  data: 0.439102  max mem: 211\n",
      "Epoch: [4]  [2011/2012]  eta: 0:00:00  lr: 0.000063  loss: 0.023966 (0.053332)  time: 0.469810  data: 0.432988  max mem: 211\n",
      "Epoch: [4] Total time: 0:16:43 (0.498526 s / it)\n",
      "Averaged stats: lr: 0.000063  loss: 0.023966 (0.053332)\n",
      "Test:  [ 0/79]  eta: 0:00:37  loss: 0.003950 (0.003950)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  time: 0.480791  data: 0.442617  max mem: 211\n",
      "Test:  [20/79]  eta: 0:00:31  loss: 0.060876 (0.102474)  acc1: 93.750000 (96.726190)  acc5: 100.000000 (100.000000)  time: 0.538307  data: 0.500164  max mem: 211\n",
      "Test:  [40/79]  eta: 0:00:20  loss: 0.038588 (0.078956)  acc1: 100.000000 (97.713415)  acc5: 100.000000 (100.000000)  time: 0.513151  data: 0.475019  max mem: 211\n",
      "Test:  [60/79]  eta: 0:00:09  loss: 0.061207 (0.084029)  acc1: 100.000000 (97.540984)  acc5: 100.000000 (100.000000)  time: 0.500913  data: 0.462762  max mem: 211\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 0.015308 (0.080008)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (100.000000)  time: 0.469882  data: 0.433146  max mem: 211\n",
      "Test: Total time: 0:00:39 (0.506188 s / it)\n",
      "* Acc@1 98.000 Acc@5 100.000 loss 0.080\n",
      "Accuracy at epoch 4 of the network on the 79 test images: 98.0%\n",
      "Accuracy at epoch 4 of the network on the 1250 test images: 98.0%\n",
      "Max accuracy so far: 98.00%\n",
      "Training of the supervised linear classifier on frozen features completed.\n",
      "Top-1 test accuracy: 98.0\n",
      "Finished Training, saving model to /cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes_full_v1/clean.pt and log to /cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes_full_v1/clean.pt\n"
     ]
    }
   ],
   "source": [
    "for attack, loaders in loader_dict.items():\n",
    "    \n",
    "    # Initialise classifier\n",
    "    adv_linear_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                         num_labels=len(CLASS_SUBSET))\n",
    "    adv_linear_classifier = adv_linear_classifier.cuda()\n",
    "\n",
    "    # Metric logger path\n",
    "    LOG_PATH = Path(LOG_BASE_PATH, 'adv_classifier', version, attack)\n",
    "    if not os.path.isdir(LOG_PATH):\n",
    "        os.makedirs(LOG_PATH)\n",
    "    \n",
    "    # train\n",
    "    pstr = \"#\"*50 + f\"Training classifier for {attack}\"+ \"#\"*50\n",
    "    print(len(pstr)*\"#\")\n",
    "    print(pstr)\n",
    "    print(len(pstr)*\"#\")\n",
    "    loggers = train(model, \n",
    "                    adv_linear_classifier, \n",
    "                    loaders[\"train\"], \n",
    "                    loaders[\"validation\"], \n",
    "                    log_dir=LOG_PATH, \n",
    "                    tensor_dir=None, \n",
    "                    optimizer=None, \n",
    "                    adversarial_attack=None,\n",
    "                    criterion=nn.CrossEntropyLoss(),\n",
    "                    epochs=5, \n",
    "                    val_freq=1, \n",
    "                    batch_size=16,  \n",
    "                    lr=0.001, \n",
    "                    to_restore = {\"epoch\": 0, \"best_acc\": 0.}, \n",
    "                    n=4, \n",
    "                    avgpool_patchtokens=False, \n",
    "                    show_image=False)\n",
    "    \n",
    "    # Save adversarial Classifier\n",
    "    save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version)\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    save_file_model = f\"{attack}.pt\"\n",
    "    save_file_log = f\"log_{attack}.pt\"\n",
    "    torch.save(adv_linear_classifier.state_dict(), str(save_path) + \"/\" + save_file_model)\n",
    "    torch.save(loggers, str(save_path) + \"/\" + save_file_log)\n",
    "    print(f'Finished Training, saving model to {str(save_path)}/{save_file_model} and log to {str(save_path)}/{save_file_model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on all adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-45ab92d45f29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mattacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattack\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"#\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\" evaluating adv_classifier trained on {attack} \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"#\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader_dict' is not defined"
     ]
    }
   ],
   "source": [
    "attacks = [x for x in loader_dict.keys()]\n",
    "\n",
    "for attack in attacks:\n",
    "    pstr = \"#\"*30 + f''' evaluating adv_classifier trained on {attack} ''' + \"#\"*30\n",
    "    print(len(pstr)*\"#\")\n",
    "    print(pstr)\n",
    "    print(len(pstr)*\"#\")\n",
    "    adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                             num_labels=len(CLASS_SUBSET))\n",
    "    adv_classifier.to(device)\n",
    "    \n",
    "    save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version)\n",
    "    save_file = f\"{attack}.pt\"\n",
    "    adv_classifier.load_state_dict(torch.load(str(save_path) + \"/\" + save_file))\n",
    "    \n",
    "    for applied_attack in attacks:\n",
    "        \n",
    "#         if applied_attack == attack:\n",
    "#             continue\n",
    "        \n",
    "        print(\"-\"*50 + f\" {applied_attack} dataset \" + \"-\"*50)\n",
    "        logger_dict, logger = validate_network(model, \n",
    "                                               adv_classifier, \n",
    "                                               loader_dict[applied_attack][\"validation\"], \n",
    "                                               criterion=nn.CrossEntropyLoss(),\n",
    "                                               tensor_dir=None, \n",
    "                                               adversarial_attack=None, \n",
    "                                               n=4, \n",
    "                                               avgpool_patchtokens=False, \n",
    "                                               path_predictions=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on newly generated attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################################################################################################\n",
      "################################################## evaluating adv_classifier trained on pgd ##################################################\n",
      "##############################################################################################################################################\n",
      "-------------------------------------------------- applying attack: PGD(model_name=ViTWrapper, device=cuda:0, eps=0.3, alpha=0.023529411764705882, steps=15, random_start=True, attack_mode=default, return_type=float) --------------------------------------------------\n",
      "Test:  [ 0/79]  eta: 0:02:02  loss: 11.795971 (11.795971)  acc1: 12.500000 (12.500000)  acc5: 25.000000 (25.000000)  adv_loss: 44.400982 (44.400982)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.550437  data: 0.273152  max mem: 1433\n",
      "Test:  [20/79]  eta: 0:01:26  loss: 11.416915 (11.553357)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (21.428571)  adv_loss: 47.603233 (47.071676)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.464201  data: 0.287168  max mem: 1433\n",
      "Test:  [40/79]  eta: 0:01:00  loss: 11.172411 (11.517733)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.426829)  adv_loss: 47.678123 (47.120677)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.627150  data: 0.439764  max mem: 1433\n",
      "Test:  [60/79]  eta: 0:00:37  loss: 11.313252 (11.424241)  acc1: 6.250000 (4.200820)  acc5: 12.500000 (20.696721)  adv_loss: 46.249008 (46.964159)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 2.786645  data: 1.591894  max mem: 1433\n",
      "Test:  [78/79]  eta: 0:00:02  loss: 11.222515 (11.402881)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (20.240000)  adv_loss: 47.642616 (47.050722)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 2.790981  data: 1.636100  max mem: 1433\n",
      "Test: Total time: 0:02:50 (2.161650 s / it)\n",
      "* Acc@1 4.000 Acc@5 20.240 loss 11.403\n",
      "* adv_Acc@1 0.000 adv_Acc@5 0.000 adv_loss 47.051\n",
      "-------------------------------------------------- applying attack: CW(model_name=ViTWrapper, device=cuda:0, c=10, kappa=0, steps=30, lr=0.003, attack_mode=default, return_type=float) --------------------------------------------------\n",
      "Test:  [ 0/79]  eta: 0:02:20  loss: 11.795971 (11.795971)  acc1: 12.500000 (12.500000)  acc5: 25.000000 (25.000000)  adv_loss: 11.874754 (11.874754)  adv_acc1: 6.250000 (6.250000)  adv_acc5: 25.000000 (25.000000)  time: 1.780596  data: 0.710976  max mem: 1558\n",
      "Test:  [20/79]  eta: 0:02:05  loss: 11.416915 (11.553357)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (21.428571)  adv_loss: 11.416916 (11.571762)  adv_acc1: 0.000000 (3.273810)  adv_acc5: 18.750000 (21.428571)  time: 2.136336  data: 1.373946  max mem: 1558\n",
      "Test:  [40/79]  eta: 0:02:13  loss: 11.172411 (11.517733)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.426829)  adv_loss: 11.233649 (11.533234)  adv_acc1: 0.000000 (3.201220)  adv_acc5: 18.750000 (20.426829)  time: 4.784059  data: 4.005501  max mem: 1558\n",
      "Test:  [60/79]  eta: 0:01:01  loss: 11.313252 (11.424241)  acc1: 6.250000 (4.200820)  acc5: 12.500000 (20.696721)  adv_loss: 11.313251 (11.438542)  adv_acc1: 6.250000 (3.586066)  adv_acc5: 12.500000 (20.696721)  time: 2.882557  data: 1.967276  max mem: 1558\n",
      "Test:  [78/79]  eta: 0:00:02  loss: 11.222515 (11.402881)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (20.240000)  adv_loss: 11.222515 (11.415385)  adv_acc1: 0.000000 (3.520000)  adv_acc5: 18.750000 (20.240000)  time: 0.853927  data: 0.280562  max mem: 1558\n",
      "Test: Total time: 0:03:32 (2.695750 s / it)\n",
      "* Acc@1 4.000 Acc@5 20.240 loss 11.403\n",
      "* adv_Acc@1 3.520 adv_Acc@5 20.240 adv_loss 11.415\n",
      "-------------------------------------------------- applying attack: FGSM(model_name=ViTWrapper, device=cuda:0, eps=0.03, attack_mode=default, return_type=float) --------------------------------------------------\n",
      "Test:  [ 0/79]  eta: 0:00:32  loss: 11.795971 (11.795971)  acc1: 12.500000 (12.500000)  acc5: 25.000000 (25.000000)  adv_loss: 22.777174 (22.777174)  adv_acc1: 12.500000 (12.500000)  adv_acc5: 12.500000 (12.500000)  time: 0.413137  data: 0.275537  max mem: 1558\n",
      "Test:  [20/79]  eta: 0:00:26  loss: 11.416915 (11.553357)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (21.428571)  adv_loss: 25.033527 (24.361036)  adv_acc1: 0.000000 (4.166667)  adv_acc5: 0.000000 (4.464286)  time: 0.442554  data: 0.304558  max mem: 1558\n",
      "Test:  [40/79]  eta: 0:00:17  loss: 11.172411 (11.517733)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.426829)  adv_loss: 24.894190 (24.309411)  adv_acc1: 0.000000 (3.963415)  adv_acc5: 0.000000 (4.115854)  time: 0.452485  data: 0.314736  max mem: 1558\n",
      "Test:  [60/79]  eta: 0:00:08  loss: 11.313252 (11.424241)  acc1: 6.250000 (4.200820)  acc5: 12.500000 (20.696721)  adv_loss: 23.473209 (24.138387)  adv_acc1: 6.250000 (4.200820)  adv_acc5: 6.250000 (4.303279)  time: 0.438560  data: 0.300788  max mem: 1558\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 11.222515 (11.402881)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (20.240000)  adv_loss: 24.252882 (24.197799)  adv_acc1: 0.000000 (4.000000)  adv_acc5: 0.000000 (4.080000)  time: 0.413944  data: 0.280303  max mem: 1558\n",
      "Test: Total time: 0:00:34 (0.437207 s / it)\n",
      "* Acc@1 4.000 Acc@5 20.240 loss 11.403\n",
      "* adv_Acc@1 4.000 adv_Acc@5 4.080 adv_loss 24.198\n",
      "#############################################################################################################################################\n",
      "################################################## evaluating adv_classifier trained on cw ##################################################\n",
      "#############################################################################################################################################\n",
      "-------------------------------------------------- applying attack: PGD(model_name=ViTWrapper, device=cuda:0, eps=0.3, alpha=0.023529411764705882, steps=15, random_start=True, attack_mode=default, return_type=float) --------------------------------------------------\n",
      "Test:  [ 0/79]  eta: 0:01:55  loss: 13.525923 (13.525923)  acc1: 12.500000 (12.500000)  acc5: 18.750000 (18.750000)  adv_loss: 50.228851 (50.228851)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.456312  data: 0.277095  max mem: 1558\n",
      "Test:  [20/79]  eta: 0:01:26  loss: 14.728826 (15.022107)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (19.940476)  adv_loss: 54.583199 (53.630983)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.471459  data: 0.284110  max mem: 1558\n",
      "Test:  [40/79]  eta: 0:00:58  loss: 14.830462 (14.869930)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.884146)  adv_loss: 54.133759 (53.636340)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.509885  data: 0.315860  max mem: 1558\n",
      "Test:  [60/79]  eta: 0:00:28  loss: 13.856272 (14.738635)  acc1: 6.250000 (4.200820)  acc5: 18.750000 (21.004098)  adv_loss: 53.360790 (53.465936)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.509206  data: 0.310392  max mem: 1558\n",
      "Test:  [78/79]  eta: 0:00:01  loss: 14.662786 (14.743645)  acc1: 0.000000 (4.000000)  acc5: 25.000000 (21.200000)  adv_loss: 54.191483 (53.539205)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.443840  data: 0.281239  max mem: 1558\n",
      "Test: Total time: 0:01:57 (1.483134 s / it)\n",
      "* Acc@1 4.000 Acc@5 21.200 loss 14.744\n",
      "* adv_Acc@1 0.000 adv_Acc@5 0.000 adv_loss 53.539\n",
      "-------------------------------------------------- applying attack: CW(model_name=ViTWrapper, device=cuda:0, c=10, kappa=0, steps=30, lr=0.003, attack_mode=default, return_type=float) --------------------------------------------------\n",
      "Test:  [ 0/79]  eta: 0:02:04  loss: 13.525923 (13.525923)  acc1: 12.500000 (12.500000)  acc5: 18.750000 (18.750000)  adv_loss: 13.641943 (13.641943)  adv_acc1: 6.250000 (6.250000)  adv_acc5: 18.750000 (18.750000)  time: 1.574844  data: 0.271150  max mem: 1558\n",
      "Test:  [20/79]  eta: 0:01:05  loss: 14.728826 (15.022107)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (19.940476)  adv_loss: 14.736960 (15.037877)  adv_acc1: 0.000000 (3.869048)  adv_acc5: 18.750000 (19.940476)  time: 1.084971  data: 0.275715  max mem: 1558\n",
      "Test:  [40/79]  eta: 0:00:43  loss: 14.830462 (14.869930)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.884146)  adv_loss: 14.830461 (14.887792)  adv_acc1: 0.000000 (3.353659)  adv_acc5: 18.750000 (20.884146)  time: 1.120549  data: 0.307961  max mem: 1558\n",
      "Test:  [60/79]  eta: 0:00:21  loss: 13.856272 (14.738635)  acc1: 6.250000 (4.200820)  acc5: 18.750000 (21.004098)  adv_loss: 13.863402 (14.754136)  adv_acc1: 6.250000 (3.688525)  adv_acc5: 18.750000 (21.004098)  time: 1.241519  data: 0.300216  max mem: 1558\n",
      "Test:  [78/79]  eta: 0:00:01  loss: 14.662786 (14.743645)  acc1: 0.000000 (4.000000)  acc5: 25.000000 (21.200000)  adv_loss: 14.662786 (14.757200)  adv_acc1: 0.000000 (3.600000)  adv_acc5: 25.000000 (21.200000)  time: 0.895050  data: 0.278800  max mem: 1558\n",
      "Test: Total time: 0:01:26 (1.094108 s / it)\n",
      "* Acc@1 4.000 Acc@5 21.200 loss 14.744\n",
      "* adv_Acc@1 3.600 adv_Acc@5 21.200 adv_loss 14.757\n",
      "-------------------------------------------------- applying attack: FGSM(model_name=ViTWrapper, device=cuda:0, eps=0.03, attack_mode=default, return_type=float) --------------------------------------------------\n",
      "Test:  [ 0/79]  eta: 0:00:32  loss: 13.525923 (13.525923)  acc1: 12.500000 (12.500000)  acc5: 18.750000 (18.750000)  adv_loss: 22.692768 (22.692768)  adv_acc1: 6.250000 (6.250000)  adv_acc5: 6.250000 (6.250000)  time: 0.409987  data: 0.269854  max mem: 1558\n",
      "Test:  [20/79]  eta: 0:00:24  loss: 14.728826 (15.022107)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (19.940476)  adv_loss: 25.877436 (25.182171)  adv_acc1: 0.000000 (3.273810)  adv_acc5: 0.000000 (4.761905)  time: 0.420589  data: 0.281123  max mem: 1558\n",
      "Test:  [40/79]  eta: 0:00:16  loss: 14.830462 (14.869930)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.884146)  adv_loss: 25.132448 (25.141571)  adv_acc1: 0.000000 (3.201220)  adv_acc5: 0.000000 (4.725610)  time: 0.447904  data: 0.309015  max mem: 1558\n",
      "Test:  [60/79]  eta: 0:00:08  loss: 13.856272 (14.738635)  acc1: 6.250000 (4.200820)  acc5: 18.750000 (21.004098)  adv_loss: 23.668444 (24.880389)  adv_acc1: 6.250000 (3.586066)  adv_acc5: 6.250000 (4.918033)  time: 0.438044  data: 0.299079  max mem: 1558\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 14.662786 (14.743645)  acc1: 0.000000 (4.000000)  acc5: 25.000000 (21.200000)  adv_loss: 25.176540 (24.971769)  adv_acc1: 0.000000 (3.520000)  adv_acc5: 0.000000 (4.560000)  time: 0.415547  data: 0.281239  max mem: 1558\n",
      "Test: Total time: 0:00:34 (0.430840 s / it)\n",
      "* Acc@1 4.000 Acc@5 21.200 loss 14.744\n",
      "* adv_Acc@1 3.520 adv_Acc@5 4.560 adv_loss 24.972\n",
      "###############################################################################################################################################\n",
      "################################################## evaluating adv_classifier trained on fgsm ##################################################\n",
      "###############################################################################################################################################\n",
      "-------------------------------------------------- applying attack: PGD(model_name=ViTWrapper, device=cuda:0, eps=0.3, alpha=0.023529411764705882, steps=15, random_start=True, attack_mode=default, return_type=float) --------------------------------------------------\n",
      "Test:  [ 0/79]  eta: 0:01:56  loss: 11.006817 (11.006817)  acc1: 12.500000 (12.500000)  acc5: 37.500000 (37.500000)  adv_loss: 48.733330 (48.733330)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.478861  data: 0.291605  max mem: 1558\n",
      "Test:  [20/79]  eta: 0:01:27  loss: 12.243925 (12.178569)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (22.916667)  adv_loss: 53.032795 (52.088713)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.476245  data: 0.283543  max mem: 1558\n",
      "Test:  [40/79]  eta: 0:00:58  loss: 12.146588 (12.163893)  acc1: 0.000000 (3.963415)  acc5: 25.000000 (23.628049)  adv_loss: 52.087830 (52.130547)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.506340  data: 0.309212  max mem: 1558\n",
      "Test:  [60/79]  eta: 0:00:28  loss: 11.950278 (12.096414)  acc1: 6.250000 (4.200820)  acc5: 25.000000 (24.077869)  adv_loss: 51.130219 (51.972829)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.514128  data: 0.314050  max mem: 1558\n",
      "Test:  [78/79]  eta: 0:00:01  loss: 12.237529 (12.061623)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (24.240000)  adv_loss: 53.000629 (52.050837)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.441042  data: 0.278652  max mem: 1558\n",
      "Test: Total time: 0:01:57 (1.484680 s / it)\n",
      "* Acc@1 4.000 Acc@5 24.240 loss 12.062\n",
      "* adv_Acc@1 0.000 adv_Acc@5 0.000 adv_loss 52.051\n",
      "-------------------------------------------------- applying attack: CW(model_name=ViTWrapper, device=cuda:0, c=10, kappa=0, steps=30, lr=0.003, attack_mode=default, return_type=float) --------------------------------------------------\n",
      "Test:  [ 0/79]  eta: 0:02:04  loss: 11.006817 (11.006817)  acc1: 12.500000 (12.500000)  acc5: 37.500000 (37.500000)  adv_loss: 11.043774 (11.043774)  adv_acc1: 12.500000 (12.500000)  adv_acc5: 37.500000 (37.500000)  time: 1.572145  data: 0.268780  max mem: 1558\n",
      "Test:  [20/79]  eta: 0:01:06  loss: 12.243925 (12.178569)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (22.916667)  adv_loss: 12.271987 (12.188224)  adv_acc1: 0.000000 (4.166667)  adv_acc5: 18.750000 (22.916667)  time: 1.100816  data: 0.270748  max mem: 1558\n",
      "Test:  [40/79]  eta: 0:00:42  loss: 12.146588 (12.163893)  acc1: 0.000000 (3.963415)  acc5: 25.000000 (23.628049)  adv_loss: 12.149730 (12.174298)  adv_acc1: 0.000000 (3.810976)  adv_acc5: 25.000000 (23.628049)  time: 1.068748  data: 0.313178  max mem: 1558\n",
      "Test:  [60/79]  eta: 0:00:22  loss: 11.950278 (12.096414)  acc1: 6.250000 (4.200820)  acc5: 25.000000 (24.077869)  adv_loss: 11.965483 (12.105202)  adv_acc1: 6.250000 (4.098361)  adv_acc5: 25.000000 (24.077869)  time: 1.283807  data: 0.297690  max mem: 1558\n",
      "Test:  [78/79]  eta: 0:00:01  loss: 12.237529 (12.061623)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (24.240000)  adv_loss: 12.237530 (12.070373)  adv_acc1: 0.000000 (3.840000)  adv_acc5: 18.750000 (24.240000)  time: 0.873909  data: 0.281142  max mem: 1558\n",
      "Test: Total time: 0:01:26 (1.093244 s / it)\n",
      "* Acc@1 4.000 Acc@5 24.240 loss 12.062\n",
      "* adv_Acc@1 3.840 adv_Acc@5 24.240 adv_loss 12.070\n",
      "-------------------------------------------------- applying attack: FGSM(model_name=ViTWrapper, device=cuda:0, eps=0.03, attack_mode=default, return_type=float) --------------------------------------------------\n",
      "Test:  [ 0/79]  eta: 0:00:32  loss: 11.006817 (11.006817)  acc1: 12.500000 (12.500000)  acc5: 37.500000 (37.500000)  adv_loss: 24.085732 (24.085732)  adv_acc1: 12.500000 (12.500000)  adv_acc5: 12.500000 (12.500000)  time: 0.412137  data: 0.273039  max mem: 1558\n",
      "Test:  [20/79]  eta: 0:00:24  loss: 12.243925 (12.178569)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (22.916667)  adv_loss: 28.157070 (27.304694)  adv_acc1: 0.000000 (4.166667)  adv_acc5: 0.000000 (5.059524)  time: 0.421314  data: 0.282266  max mem: 1558\n",
      "Test:  [40/79]  eta: 0:00:16  loss: 12.146588 (12.163893)  acc1: 0.000000 (3.963415)  acc5: 25.000000 (23.628049)  adv_loss: 27.329895 (27.287895)  adv_acc1: 0.000000 (3.963415)  adv_acc5: 0.000000 (4.725610)  time: 0.448644  data: 0.309718  max mem: 1558\n",
      "Test:  [60/79]  eta: 0:00:08  loss: 11.950278 (12.096414)  acc1: 6.250000 (4.200820)  acc5: 25.000000 (24.077869)  adv_loss: 26.652597 (27.150171)  adv_acc1: 6.250000 (4.200820)  adv_acc5: 6.250000 (4.918033)  time: 0.436559  data: 0.297643  max mem: 1558\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 12.237529 (12.061623)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (24.240000)  adv_loss: 27.796087 (27.169391)  adv_acc1: 0.000000 (4.000000)  adv_acc5: 6.250000 (4.960000)  time: 0.413032  data: 0.278652  max mem: 1558\n",
      "Test: Total time: 0:00:33 (0.430280 s / it)\n",
      "* Acc@1 4.000 Acc@5 24.240 loss 12.062\n",
      "* adv_Acc@1 4.000 adv_Acc@5 4.960 adv_loss 27.169\n",
      "################################################################################################################################################\n",
      "################################################## evaluating adv_classifier trained on clean ##################################################\n",
      "################################################################################################################################################\n",
      "-------------------------------------------------- applying attack: PGD(model_name=ViTWrapper, device=cuda:0, eps=0.3, alpha=0.023529411764705882, steps=15, random_start=True, attack_mode=default, return_type=float) --------------------------------------------------\n",
      "Test:  [ 0/79]  eta: 0:01:55  loss: 0.003950 (0.003950)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  adv_loss: 35.267078 (35.267078)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.459516  data: 0.272776  max mem: 1558\n",
      "Test:  [20/79]  eta: 0:01:26  loss: 0.060876 (0.102474)  acc1: 93.750000 (96.726190)  acc5: 100.000000 (100.000000)  adv_loss: 35.133877 (35.382921)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.472912  data: 0.281304  max mem: 1558\n",
      "Test:  [40/79]  eta: 0:00:58  loss: 0.038588 (0.078956)  acc1: 100.000000 (97.713415)  acc5: 100.000000 (100.000000)  adv_loss: 35.532440 (35.351745)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.512504  data: 0.316078  max mem: 1558\n",
      "Test:  [60/79]  eta: 0:00:28  loss: 0.061207 (0.084029)  acc1: 100.000000 (97.540984)  acc5: 100.000000 (100.000000)  adv_loss: 35.539730 (35.398874)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.506250  data: 0.306994  max mem: 1558\n",
      "Test:  [78/79]  eta: 0:00:01  loss: 0.015308 (0.080008)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (100.000000)  adv_loss: 35.127617 (35.360297)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 0.000000 (0.000000)  time: 1.434455  data: 0.276219  max mem: 1558\n",
      "Test: Total time: 0:01:57 (1.481596 s / it)\n",
      "* Acc@1 98.000 Acc@5 100.000 loss 0.080\n",
      "* adv_Acc@1 0.000 adv_Acc@5 0.000 adv_loss 35.360\n",
      "-------------------------------------------------- applying attack: CW(model_name=ViTWrapper, device=cuda:0, c=10, kappa=0, steps=30, lr=0.003, attack_mode=default, return_type=float) --------------------------------------------------\n",
      "Test:  [ 0/79]  eta: 0:01:26  loss: 0.003950 (0.003950)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  adv_loss: 0.570802 (0.570802)  adv_acc1: 81.250000 (81.250000)  adv_acc5: 100.000000 (100.000000)  time: 1.100430  data: 0.259296  max mem: 1558\n",
      "Test:  [20/79]  eta: 0:01:29  loss: 0.060876 (0.102474)  acc1: 93.750000 (96.726190)  acc5: 100.000000 (100.000000)  adv_loss: 0.516826 (0.585112)  adv_acc1: 81.250000 (76.785714)  adv_acc5: 100.000000 (99.404762)  time: 1.545380  data: 0.283950  max mem: 1558\n",
      "Test:  [40/79]  eta: 0:00:59  loss: 0.038588 (0.078956)  acc1: 100.000000 (97.713415)  acc5: 100.000000 (100.000000)  adv_loss: 0.444854 (0.509524)  adv_acc1: 81.250000 (80.030488)  adv_acc5: 100.000000 (99.695122)  time: 1.525148  data: 0.324134  max mem: 1558\n",
      "Test:  [60/79]  eta: 0:00:28  loss: 0.061207 (0.084029)  acc1: 100.000000 (97.540984)  acc5: 100.000000 (100.000000)  adv_loss: 0.521573 (0.506258)  adv_acc1: 81.250000 (80.327869)  adv_acc5: 100.000000 (99.795082)  time: 1.403541  data: 0.316276  max mem: 1558\n",
      "Test:  [78/79]  eta: 0:00:01  loss: 0.015308 (0.080008)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (100.000000)  adv_loss: 0.482014 (0.512122)  adv_acc1: 75.000000 (80.640000)  adv_acc5: 100.000000 (99.840000)  time: 1.484256  data: 0.279443  max mem: 1558\n",
      "Test: Total time: 0:01:57 (1.491225 s / it)\n",
      "* Acc@1 98.000 Acc@5 100.000 loss 0.080\n",
      "* adv_Acc@1 80.640 adv_Acc@5 99.840 adv_loss 0.512\n",
      "-------------------------------------------------- applying attack: FGSM(model_name=ViTWrapper, device=cuda:0, eps=0.03, attack_mode=default, return_type=float) --------------------------------------------------\n",
      "Test:  [ 0/79]  eta: 0:00:32  loss: 0.003950 (0.003950)  acc1: 100.000000 (100.000000)  acc5: 100.000000 (100.000000)  adv_loss: 4.984804 (4.984804)  adv_acc1: 0.000000 (0.000000)  adv_acc5: 62.500000 (62.500000)  time: 0.412938  data: 0.272225  max mem: 1558\n",
      "Test:  [20/79]  eta: 0:00:24  loss: 0.060876 (0.102474)  acc1: 93.750000 (96.726190)  acc5: 100.000000 (100.000000)  adv_loss: 5.499987 (5.718628)  adv_acc1: 12.500000 (16.666667)  adv_acc5: 56.250000 (56.250000)  time: 0.422410  data: 0.282636  max mem: 1558\n",
      "Test:  [40/79]  eta: 0:00:16  loss: 0.038588 (0.078956)  acc1: 100.000000 (97.713415)  acc5: 100.000000 (100.000000)  adv_loss: 5.082420 (5.513557)  adv_acc1: 18.750000 (17.682927)  adv_acc5: 56.250000 (56.859756)  time: 0.448420  data: 0.309259  max mem: 1558\n",
      "Test:  [60/79]  eta: 0:00:08  loss: 0.061207 (0.084029)  acc1: 100.000000 (97.540984)  acc5: 100.000000 (100.000000)  adv_loss: 5.486139 (5.504612)  adv_acc1: 12.500000 (17.008197)  adv_acc5: 50.000000 (55.327869)  time: 0.437817  data: 0.298902  max mem: 1558\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 0.015308 (0.080008)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (100.000000)  adv_loss: 5.505926 (5.520856)  adv_acc1: 12.500000 (16.480000)  adv_acc5: 50.000000 (55.200000)  time: 0.412945  data: 0.278355  max mem: 1558\n",
      "Test: Total time: 0:00:34 (0.430763 s / it)\n",
      "* Acc@1 98.000 Acc@5 100.000 loss 0.080\n",
      "* adv_Acc@1 16.480 adv_Acc@5 55.200 adv_loss 5.521\n"
     ]
    }
   ],
   "source": [
    "attacks = [x for x in loader_dict.keys()]\n",
    "for attack in attacks:\n",
    "    \n",
    "    pstr = \"#\"*30 + f''' evaluating adv_classifier trained on {attack} ''' + \"#\"*30\n",
    "    print(len(pstr)*\"#\")\n",
    "    print(pstr)\n",
    "    print(len(pstr)*\"#\")\n",
    "    \n",
    "    adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                             num_labels=len(CLASS_SUBSET))\n",
    "    adv_classifier.to(device)\n",
    "    \n",
    "    save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version)\n",
    "    save_file = f\"{attack}.pt\"\n",
    "    adv_classifier.load_state_dict(torch.load(str(save_path) + \"/\" + save_file))\n",
    "    \n",
    "    vits = ViTWrapper(model, adv_classifier, transform=None)\n",
    "\n",
    "    for applied_attack in attacks:\n",
    "        \n",
    "        if applied_attack == \"pgd\":\n",
    "            ev_attack = PGD(vits, eps=0.3, alpha=6/255, steps=15)\n",
    "        elif applied_attack == \"cw\":\n",
    "            ev_attack = CW(vits, c=10, lr=0.003, steps=30)\n",
    "        elif applied_attack == \"fgsm\":\n",
    "            ev_attack = FGSM(vits, eps=0.03)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        print(\"-\"*50 + f''' applying attack: {ev_attack} ''' + \"-\"*50)\n",
    "        logger_dict, logger = validate_network(model, \n",
    "                                               adv_classifier, \n",
    "                                               loader_dict[\"clean\"][\"validation\"],\n",
    "                                               criterion=nn.CrossEntropyLoss(),\n",
    "                                               tensor_dir=None, \n",
    "                                               adversarial_attack=ev_attack,\n",
    "                                               n=4, \n",
    "                                               avgpool_patchtokens=False, \n",
    "                                               path_predictions=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on full pipeline with post-hoc as multiplexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load clean_classifier\n",
    "name=\"clean\"\n",
    "clean_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                    num_labels=len(CLASS_SUBSET))\n",
    "clean_classifier.to(device)\n",
    "clean_classifier.load_state_dict(torch.load(f\"/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/{version}/{name}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## Validating Posthoc: cw and adv_classifier: cw on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 1.307345 (1.307345)  acc1: 75.000000 (75.000000)  acc5: 87.500000 (87.500000)  time: 0.214432  data: 0.173989  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 1.143139 (1.133734)  acc1: 75.000000 (78.000000)  acc5: 87.500000 (86.000000)  time: 0.219044  data: 0.185667  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.219371 s / it)\n",
      "* Acc@1 78.000 Acc@5 86.000 loss 1.134\n",
      "################################################## Validating Posthoc: cw and adv_classifier: cw on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:02  loss: 5.649935 (5.649935)  acc1: 50.000000 (50.000000)  acc5: 62.500000 (62.500000)  time: 0.534863  data: 0.494440  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 6.480145 (6.743094)  acc1: 31.250000 (38.000000)  acc5: 62.500000 (66.000000)  time: 0.324902  data: 0.291575  max mem: 210\n",
      "Test: Total time: 0:00:01 (0.325233 s / it)\n",
      "* Acc@1 38.000 Acc@5 66.000 loss 6.743\n",
      "################################################## Validating Posthoc: cw and adv_classifier: cw on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:02  loss: 2.073831 (2.073831)  acc1: 75.000000 (75.000000)  acc5: 75.000000 (75.000000)  time: 0.567346  data: 0.527024  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.953828 (1.435039)  acc1: 75.000000 (78.000000)  acc5: 75.000000 (80.000000)  time: 0.345518  data: 0.312010  max mem: 210\n",
      "Test: Total time: 0:00:01 (0.345850 s / it)\n",
      "* Acc@1 78.000 Acc@5 80.000 loss 1.435\n",
      "################################################## Validating Posthoc: cw and adv_classifier: cw on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:42  loss: 13.525923 (13.525923)  acc1: 12.500000 (12.500000)  acc5: 18.750000 (18.750000)  time: 0.539719  data: 0.499344  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:41  loss: 13.525923 (14.304344)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (26.041667)  time: 0.558826  data: 0.518431  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:38  loss: 14.199209 (14.420988)  acc1: 6.250000 (6.250000)  acc5: 25.000000 (23.863636)  time: 0.559768  data: 0.519416  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:35  loss: 14.369546 (14.993355)  acc1: 0.000000 (4.296875)  acc5: 18.750000 (19.531250)  time: 0.560960  data: 0.520625  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:33  loss: 14.728824 (14.972114)  acc1: 0.000000 (4.464286)  acc5: 18.750000 (20.238095)  time: 0.572157  data: 0.531811  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:32  loss: 14.875451 (14.904237)  acc1: 0.000000 (4.567308)  acc5: 18.750000 (20.432692)  time: 0.608295  data: 0.568020  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:29  loss: 15.143551 (14.824061)  acc1: 0.000000 (4.032258)  acc5: 18.750000 (20.362903)  time: 0.612796  data: 0.572567  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:25  loss: 15.143551 (14.901293)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (21.180556)  time: 0.597851  data: 0.557663  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:22  loss: 14.830462 (14.798985)  acc1: 0.000000 (4.268293)  acc5: 18.750000 (21.341463)  time: 0.597941  data: 0.557831  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:19  loss: 14.137599 (14.686963)  acc1: 0.000000 (4.619565)  acc5: 25.000000 (22.010870)  time: 0.551667  data: 0.511560  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:16  loss: 14.249454 (14.673475)  acc1: 6.250000 (4.779412)  acc5: 25.000000 (22.058824)  time: 0.571363  data: 0.531251  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:14  loss: 13.856274 (14.595517)  acc1: 6.250000 (4.799107)  acc5: 18.750000 (22.098214)  time: 0.661753  data: 0.621652  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:11  loss: 13.760896 (14.611113)  acc1: 6.250000 (4.918033)  acc5: 18.750000 (21.721311)  time: 0.658904  data: 0.618726  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:08  loss: 14.420955 (14.672373)  acc1: 0.000000 (4.545455)  acc5: 18.750000 (21.590909)  time: 0.668818  data: 0.628551  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:05  loss: 14.603889 (14.681700)  acc1: 0.000000 (4.401408)  acc5: 18.750000 (22.007042)  time: 0.630305  data: 0.589927  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:02  loss: 14.662786 (14.632332)  acc1: 0.000000 (4.769737)  acc5: 18.750000 (21.957237)  time: 0.564207  data: 0.523740  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 14.662786 (14.629543)  acc1: 0.000000 (4.640000)  acc5: 25.000000 (21.840000)  time: 0.517979  data: 0.478868  max mem: 210\n",
      "Test: Total time: 0:00:46 (0.588676 s / it)\n",
      "* Acc@1 4.640 Acc@5 21.840 loss 14.630\n",
      "################################################## Validating Posthoc: cw and adv_classifier: fgsm on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 1.307345 (1.307345)  acc1: 75.000000 (75.000000)  acc5: 87.500000 (87.500000)  time: 0.192384  data: 0.151429  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 1.143139 (1.133734)  acc1: 75.000000 (78.000000)  acc5: 87.500000 (86.000000)  time: 0.151673  data: 0.117963  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.152015 s / it)\n",
      "* Acc@1 78.000 Acc@5 86.000 loss 1.134\n",
      "################################################## Validating Posthoc: cw and adv_classifier: fgsm on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 5.649935 (5.649935)  acc1: 50.000000 (50.000000)  acc5: 62.500000 (62.500000)  time: 0.189451  data: 0.148428  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 6.480145 (6.743094)  acc1: 31.250000 (38.000000)  acc5: 62.500000 (66.000000)  time: 0.151013  data: 0.117366  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.151339 s / it)\n",
      "* Acc@1 38.000 Acc@5 66.000 loss 6.743\n",
      "################################################## Validating Posthoc: cw and adv_classifier: fgsm on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 2.073831 (2.073831)  acc1: 75.000000 (75.000000)  acc5: 75.000000 (75.000000)  time: 0.190943  data: 0.150450  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.953828 (1.435039)  acc1: 75.000000 (78.000000)  acc5: 75.000000 (80.000000)  time: 0.150274  data: 0.116782  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.150606 s / it)\n",
      "* Acc@1 78.000 Acc@5 80.000 loss 1.435\n",
      "################################################## Validating Posthoc: cw and adv_classifier: fgsm on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:24  loss: 11.006817 (11.006817)  acc1: 12.500000 (12.500000)  acc5: 37.500000 (37.500000)  time: 0.311788  data: 0.271095  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:23  loss: 11.006817 (11.411270)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (25.000000)  time: 0.314115  data: 0.273722  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:21  loss: 11.137864 (11.755114)  acc1: 6.250000 (6.250000)  acc5: 25.000000 (25.000000)  time: 0.314514  data: 0.274151  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:20  loss: 12.243926 (12.283812)  acc1: 0.000000 (4.296875)  acc5: 18.750000 (22.656250)  time: 0.314371  data: 0.274005  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:18  loss: 12.243926 (12.141456)  acc1: 0.000000 (4.464286)  acc5: 18.750000 (23.214286)  time: 0.320954  data: 0.280578  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:17  loss: 12.243926 (12.138654)  acc1: 0.000000 (4.567308)  acc5: 18.750000 (23.557692)  time: 0.332854  data: 0.292445  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:16  loss: 12.282470 (12.147343)  acc1: 0.000000 (4.032258)  acc5: 25.000000 (24.798387)  time: 0.337315  data: 0.296883  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:14  loss: 12.161049 (12.224302)  acc1: 0.000000 (4.166667)  acc5: 25.000000 (24.131944)  time: 0.337769  data: 0.297348  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:12  loss: 12.146587 (12.111520)  acc1: 0.000000 (4.268293)  acc5: 25.000000 (24.085366)  time: 0.345369  data: 0.304869  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:11  loss: 11.736071 (11.998845)  acc1: 0.000000 (4.619565)  acc5: 25.000000 (24.456522)  time: 0.341295  data: 0.300820  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:09  loss: 11.736071 (11.975646)  acc1: 6.250000 (4.779412)  acc5: 25.000000 (25.490196)  time: 0.336239  data: 0.295772  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:08  loss: 11.204629 (11.994452)  acc1: 6.250000 (4.799107)  acc5: 25.000000 (24.776786)  time: 0.351464  data: 0.310976  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:06  loss: 11.736071 (11.987443)  acc1: 6.250000 (4.918033)  acc5: 25.000000 (24.692623)  time: 0.336245  data: 0.295879  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:04  loss: 12.443351 (12.022660)  acc1: 0.000000 (4.545455)  acc5: 25.000000 (24.242424)  time: 0.335786  data: 0.295425  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:03  loss: 12.285851 (12.016546)  acc1: 0.000000 (4.401408)  acc5: 18.750000 (24.295775)  time: 0.342678  data: 0.302320  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 12.285851 (11.992317)  acc1: 0.000000 (4.769737)  acc5: 18.750000 (24.671053)  time: 0.346496  data: 0.306152  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 12.139828 (11.964970)  acc1: 0.000000 (4.640000)  acc5: 18.750000 (24.720000)  time: 0.344171  data: 0.305228  max mem: 210\n",
      "Test: Total time: 0:00:26 (0.337125 s / it)\n",
      "* Acc@1 4.640 Acc@5 24.720 loss 11.965\n",
      "################################################## Validating Posthoc: cw and adv_classifier: pgd on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 1.307345 (1.307345)  acc1: 75.000000 (75.000000)  acc5: 87.500000 (87.500000)  time: 0.186048  data: 0.145842  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 1.143139 (1.133734)  acc1: 75.000000 (78.000000)  acc5: 87.500000 (86.000000)  time: 0.146997  data: 0.113706  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.147374 s / it)\n",
      "* Acc@1 78.000 Acc@5 86.000 loss 1.134\n",
      "################################################## Validating Posthoc: cw and adv_classifier: pgd on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 5.649935 (5.649935)  acc1: 50.000000 (50.000000)  acc5: 62.500000 (62.500000)  time: 0.185681  data: 0.145531  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 6.480145 (6.743094)  acc1: 31.250000 (38.000000)  acc5: 62.500000 (66.000000)  time: 0.146900  data: 0.113581  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.147223 s / it)\n",
      "* Acc@1 38.000 Acc@5 66.000 loss 6.743\n",
      "################################################## Validating Posthoc: cw and adv_classifier: pgd on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:04  loss: 2.073831 (2.073831)  acc1: 75.000000 (75.000000)  acc5: 75.000000 (75.000000)  time: 1.073149  data: 1.032895  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.953828 (1.435039)  acc1: 75.000000 (78.000000)  acc5: 75.000000 (80.000000)  time: 0.378200  data: 0.344919  max mem: 210\n",
      "Test: Total time: 0:00:01 (0.378526 s / it)\n",
      "* Acc@1 78.000 Acc@5 80.000 loss 1.435\n",
      "################################################## Validating Posthoc: cw and adv_classifier: pgd on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:24  loss: 11.795970 (11.795970)  acc1: 12.500000 (12.500000)  acc5: 25.000000 (25.000000)  time: 0.308286  data: 0.268024  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:26  loss: 11.018625 (11.406765)  acc1: 6.250000 (8.333333)  acc5: 18.750000 (20.833333)  time: 0.362544  data: 0.322253  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:27  loss: 11.580111 (11.400425)  acc1: 6.250000 (6.250000)  acc5: 18.750000 (21.022727)  time: 0.399003  data: 0.358692  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:26  loss: 11.580111 (11.514577)  acc1: 0.000000 (4.296875)  acc5: 18.750000 (21.484375)  time: 0.416612  data: 0.376307  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:23  loss: 11.416915 (11.518286)  acc1: 0.000000 (4.464286)  acc5: 18.750000 (21.726190)  time: 0.408505  data: 0.368201  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:22  loss: 11.416915 (11.503093)  acc1: 0.000000 (4.567308)  acc5: 18.750000 (20.913462)  time: 0.443936  data: 0.403623  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:20  loss: 11.222027 (11.483163)  acc1: 0.000000 (4.032258)  acc5: 18.750000 (21.169355)  time: 0.417633  data: 0.377315  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:17  loss: 11.172412 (11.535981)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (21.180556)  time: 0.383787  data: 0.343462  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:15  loss: 11.107396 (11.460660)  acc1: 0.000000 (4.268293)  acc5: 18.750000 (20.884146)  time: 0.386435  data: 0.346073  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:13  loss: 11.096105 (11.410004)  acc1: 0.000000 (4.619565)  acc5: 18.750000 (20.108696)  time: 0.345116  data: 0.304751  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:11  loss: 10.488884 (11.350741)  acc1: 6.250000 (4.779412)  acc5: 18.750000 (21.078431)  time: 0.339343  data: 0.298986  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:09  loss: 11.313253 (11.334938)  acc1: 6.250000 (4.799107)  acc5: 12.500000 (20.758929)  time: 0.354360  data: 0.313987  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:07  loss: 11.025122 (11.322228)  acc1: 6.250000 (4.918033)  acc5: 12.500000 (21.209016)  time: 0.344075  data: 0.303726  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:05  loss: 11.194990 (11.320700)  acc1: 0.000000 (4.545455)  acc5: 18.750000 (20.928030)  time: 0.346763  data: 0.306413  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:03  loss: 11.313253 (11.340247)  acc1: 0.000000 (4.401408)  acc5: 12.500000 (20.422535)  time: 0.365034  data: 0.324678  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 11.194990 (11.311812)  acc1: 0.000000 (4.769737)  acc5: 18.750000 (20.805921)  time: 0.357686  data: 0.317337  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 11.222515 (11.313910)  acc1: 0.000000 (4.640000)  acc5: 18.750000 (20.720000)  time: 0.350100  data: 0.311153  max mem: 210\n",
      "Test: Total time: 0:00:29 (0.371983 s / it)\n",
      "* Acc@1 4.640 Acc@5 20.720 loss 11.314\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: cw on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 5.488269 (5.488269)  acc1: 18.750000 (18.750000)  acc5: 56.250000 (56.250000)  time: 0.186534  data: 0.146108  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 5.259805 (4.969323)  acc1: 18.750000 (16.000000)  acc5: 56.250000 (58.000000)  time: 0.147838  data: 0.114561  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.148223 s / it)\n",
      "* Acc@1 16.000 Acc@5 58.000 loss 4.969\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: cw on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.298653 (0.298653)  acc1: 93.750000 (93.750000)  acc5: 93.750000 (93.750000)  time: 0.184226  data: 0.143873  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.074663)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (98.000000)  time: 0.147216  data: 0.113967  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.147603 s / it)\n",
      "* Acc@1 98.000 Acc@5 98.000 loss 0.075\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: cw on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 7.155119 (7.155119)  acc1: 6.250000 (6.250000)  acc5: 37.500000 (37.500000)  time: 0.191207  data: 0.150940  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 6.094499 (6.365180)  acc1: 0.000000 (4.000000)  acc5: 37.500000 (46.000000)  time: 0.150229  data: 0.116721  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.150618 s / it)\n",
      "* Acc@1 4.000 Acc@5 46.000 loss 6.365\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: cw on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:27  loss: 13.525923 (13.525923)  acc1: 12.500000 (12.500000)  acc5: 18.750000 (18.750000)  time: 0.345704  data: 0.305245  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:28  loss: 13.525923 (14.304344)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (26.041667)  time: 0.384810  data: 0.344460  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:29  loss: 14.369546 (14.516430)  acc1: 6.250000 (5.681818)  acc5: 25.000000 (23.295455)  time: 0.432829  data: 0.392446  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:26  loss: 14.498019 (15.058971)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (19.140625)  time: 0.413024  data: 0.372660  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:25  loss: 14.728824 (15.022107)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (19.940476)  time: 0.428533  data: 0.388172  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:22  loss: 14.875451 (14.944617)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (20.192308)  time: 0.434050  data: 0.393704  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:20  loss: 15.143551 (14.882747)  acc1: 0.000000 (3.830645)  acc5: 18.750000 (19.959677)  time: 0.394706  data: 0.354357  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:17  loss: 15.143551 (14.951828)  acc1: 0.000000 (3.993056)  acc5: 18.750000 (20.833333)  time: 0.380903  data: 0.340567  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:15  loss: 14.830462 (14.869930)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.884146)  time: 0.355268  data: 0.314951  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:13  loss: 14.137599 (14.812295)  acc1: 0.000000 (3.940217)  acc5: 25.000000 (21.331522)  time: 0.339276  data: 0.298921  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:10  loss: 14.249454 (14.806942)  acc1: 6.250000 (4.044118)  acc5: 25.000000 (21.323529)  time: 0.333881  data: 0.293553  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:09  loss: 13.856274 (14.717068)  acc1: 6.250000 (4.129464)  acc5: 18.750000 (21.428571)  time: 0.349810  data: 0.309450  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:07  loss: 13.856274 (14.738635)  acc1: 6.250000 (4.200820)  acc5: 18.750000 (21.004098)  time: 0.337313  data: 0.296958  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:05  loss: 14.457070 (14.790233)  acc1: 0.000000 (3.882576)  acc5: 18.750000 (20.928030)  time: 0.336253  data: 0.295912  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:03  loss: 14.620043 (14.808658)  acc1: 0.000000 (3.697183)  acc5: 18.750000 (21.302817)  time: 0.339340  data: 0.298993  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 14.662786 (14.750938)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (21.299342)  time: 0.326516  data: 0.286191  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 14.662786 (14.743645)  acc1: 0.000000 (4.000000)  acc5: 25.000000 (21.200000)  time: 0.313945  data: 0.275016  max mem: 210\n",
      "Test: Total time: 0:00:28 (0.360045 s / it)\n",
      "* Acc@1 4.000 Acc@5 21.200 loss 14.744\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: fgsm on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 5.488269 (5.488269)  acc1: 18.750000 (18.750000)  acc5: 56.250000 (56.250000)  time: 0.186707  data: 0.146438  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 5.259805 (4.969323)  acc1: 18.750000 (16.000000)  acc5: 56.250000 (58.000000)  time: 0.148268  data: 0.115083  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.148579 s / it)\n",
      "* Acc@1 16.000 Acc@5 58.000 loss 4.969\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: fgsm on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.298653 (0.298653)  acc1: 93.750000 (93.750000)  acc5: 93.750000 (93.750000)  time: 0.187712  data: 0.147385  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.074663)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (98.000000)  time: 0.147314  data: 0.114074  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.147628 s / it)\n",
      "* Acc@1 98.000 Acc@5 98.000 loss 0.075\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: fgsm on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 7.155119 (7.155119)  acc1: 6.250000 (6.250000)  acc5: 37.500000 (37.500000)  time: 0.187254  data: 0.146930  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 6.094499 (6.365180)  acc1: 0.000000 (4.000000)  acc5: 37.500000 (46.000000)  time: 0.148164  data: 0.114859  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.148488 s / it)\n",
      "* Acc@1 4.000 Acc@5 46.000 loss 6.365\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: fgsm on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:24  loss: 11.006817 (11.006817)  acc1: 12.500000 (12.500000)  acc5: 37.500000 (37.500000)  time: 0.307253  data: 0.267024  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:23  loss: 11.006817 (11.411270)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (25.000000)  time: 0.313272  data: 0.273003  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:21  loss: 11.917228 (11.825965)  acc1: 6.250000 (5.681818)  acc5: 25.000000 (24.431818)  time: 0.313814  data: 0.273477  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:19  loss: 12.243926 (12.332522)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (22.265625)  time: 0.312060  data: 0.271722  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:18  loss: 12.243926 (12.178569)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (22.916667)  time: 0.318922  data: 0.278553  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:17  loss: 12.243926 (12.168630)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (23.317308)  time: 0.329988  data: 0.289584  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:16  loss: 12.282470 (12.195342)  acc1: 0.000000 (3.830645)  acc5: 25.000000 (24.395161)  time: 0.335766  data: 0.295361  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:14  loss: 12.161049 (12.265634)  acc1: 0.000000 (3.993056)  acc5: 25.000000 (23.784722)  time: 0.338926  data: 0.298530  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:12  loss: 12.146587 (12.163893)  acc1: 0.000000 (3.963415)  acc5: 25.000000 (23.628049)  time: 0.345224  data: 0.304846  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:11  loss: 12.004870 (12.097364)  acc1: 0.000000 (3.940217)  acc5: 25.000000 (23.913043)  time: 0.341938  data: 0.301578  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:09  loss: 11.792064 (12.083798)  acc1: 6.250000 (4.044118)  acc5: 25.000000 (24.877451)  time: 0.336466  data: 0.296084  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:08  loss: 11.646162 (12.092948)  acc1: 6.250000 (4.129464)  acc5: 25.000000 (24.218750)  time: 0.350241  data: 0.309852  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:06  loss: 11.950278 (12.096414)  acc1: 6.250000 (4.200820)  acc5: 25.000000 (24.077869)  time: 0.337094  data: 0.296706  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:04  loss: 12.443351 (12.123376)  acc1: 0.000000 (3.882576)  acc5: 25.000000 (23.674242)  time: 0.336451  data: 0.296056  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:03  loss: 12.285851 (12.124089)  acc1: 0.000000 (3.697183)  acc5: 18.750000 (23.767606)  time: 0.339456  data: 0.299093  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 12.285851 (12.092785)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (24.177632)  time: 0.327055  data: 0.286693  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 12.237530 (12.061623)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (24.240000)  time: 0.314591  data: 0.275674  max mem: 210\n",
      "Test: Total time: 0:00:26 (0.329303 s / it)\n",
      "* Acc@1 4.000 Acc@5 24.240 loss 12.062\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: pgd on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 5.488269 (5.488269)  acc1: 18.750000 (18.750000)  acc5: 56.250000 (56.250000)  time: 0.191830  data: 0.151345  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 5.259805 (4.969323)  acc1: 18.750000 (16.000000)  acc5: 56.250000 (58.000000)  time: 0.149240  data: 0.115867  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.149649 s / it)\n",
      "* Acc@1 16.000 Acc@5 58.000 loss 4.969\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: pgd on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.298653 (0.298653)  acc1: 93.750000 (93.750000)  acc5: 93.750000 (93.750000)  time: 0.193476  data: 0.153124  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.074664)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (98.000000)  time: 0.148913  data: 0.115489  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.149242 s / it)\n",
      "* Acc@1 98.000 Acc@5 98.000 loss 0.075\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: pgd on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 7.155119 (7.155119)  acc1: 6.250000 (6.250000)  acc5: 37.500000 (37.500000)  time: 0.188547  data: 0.148005  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 6.094499 (6.365180)  acc1: 0.000000 (4.000000)  acc5: 37.500000 (46.000000)  time: 0.149817  data: 0.116219  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.150212 s / it)\n",
      "* Acc@1 4.000 Acc@5 46.000 loss 6.365\n",
      "################################################## Validating Posthoc: fgsm_06 and adv_classifier: pgd on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:24  loss: 11.795970 (11.795970)  acc1: 12.500000 (12.500000)  acc5: 25.000000 (25.000000)  time: 0.311119  data: 0.270719  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:23  loss: 11.018625 (11.406765)  acc1: 6.250000 (8.333333)  acc5: 18.750000 (20.833333)  time: 0.312625  data: 0.272291  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:21  loss: 11.580111 (11.467379)  acc1: 6.250000 (5.681818)  acc5: 18.750000 (20.454545)  time: 0.313619  data: 0.273288  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:19  loss: 11.580111 (11.560608)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (21.093750)  time: 0.312035  data: 0.271713  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:19  loss: 11.416915 (11.553357)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (21.428571)  time: 0.329029  data: 0.288717  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:18  loss: 11.416915 (11.531420)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (20.673077)  time: 0.341365  data: 0.301044  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:16  loss: 11.416915 (11.530670)  acc1: 0.000000 (3.830645)  acc5: 18.750000 (20.766129)  time: 0.345998  data: 0.305614  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:14  loss: 11.416915 (11.576890)  acc1: 0.000000 (3.993056)  acc5: 18.750000 (20.833333)  time: 0.348666  data: 0.308274  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:13  loss: 11.172412 (11.517733)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.426829)  time: 0.344823  data: 0.304416  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:11  loss: 11.421421 (11.511051)  acc1: 0.000000 (3.940217)  acc5: 18.750000 (19.565217)  time: 0.360152  data: 0.319731  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:09  loss: 10.739026 (11.458579)  acc1: 6.250000 (4.044118)  acc5: 18.750000 (20.465686)  time: 0.354297  data: 0.313947  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:08  loss: 11.313253 (11.433147)  acc1: 6.250000 (4.129464)  acc5: 12.500000 (20.200893)  time: 0.381267  data: 0.340907  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:06  loss: 11.313253 (11.424241)  acc1: 6.250000 (4.200820)  acc5: 12.500000 (20.696721)  time: 0.376882  data: 0.336537  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:04  loss: 11.313253 (11.414984)  acc1: 0.000000 (3.882576)  acc5: 18.750000 (20.454545)  time: 0.366163  data: 0.325845  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:03  loss: 11.385376 (11.439243)  acc1: 0.000000 (3.697183)  acc5: 12.500000 (19.894366)  time: 0.369783  data: 0.329464  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 11.385376 (11.404295)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (20.312500)  time: 0.364930  data: 0.324615  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 11.222515 (11.402881)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (20.240000)  time: 0.353407  data: 0.314485  max mem: 210\n",
      "Test: Total time: 0:00:27 (0.349243 s / it)\n",
      "* Acc@1 4.000 Acc@5 20.240 loss 11.403\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: cw on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 3.319745 (3.319745)  acc1: 50.000000 (50.000000)  acc5: 68.750000 (68.750000)  time: 0.201985  data: 0.161675  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 3.319745 (3.568536)  acc1: 43.750000 (42.000000)  acc5: 68.750000 (68.000000)  time: 0.151556  data: 0.118446  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.151930 s / it)\n",
      "* Acc@1 42.000 Acc@5 68.000 loss 3.569\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: cw on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.298653 (0.298653)  acc1: 93.750000 (93.750000)  acc5: 93.750000 (93.750000)  time: 0.187501  data: 0.147221  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.074663)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (98.000000)  time: 0.147653  data: 0.114355  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.147970 s / it)\n",
      "* Acc@1 98.000 Acc@5 98.000 loss 0.075\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: cw on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 4.827378 (4.827378)  acc1: 43.750000 (43.750000)  acc5: 56.250000 (56.250000)  time: 0.188797  data: 0.148307  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 3.839459 (3.874488)  acc1: 43.750000 (42.000000)  acc5: 62.500000 (64.000000)  time: 0.149808  data: 0.116442  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.150123 s / it)\n",
      "* Acc@1 42.000 Acc@5 64.000 loss 3.874\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: cw on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:24  loss: 13.525923 (13.525923)  acc1: 12.500000 (12.500000)  acc5: 18.750000 (18.750000)  time: 0.314523  data: 0.273629  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:23  loss: 13.525923 (14.304344)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (26.041667)  time: 0.314499  data: 0.273974  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:21  loss: 14.369546 (14.516430)  acc1: 6.250000 (5.681818)  acc5: 25.000000 (23.295455)  time: 0.314415  data: 0.273983  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:24  loss: 14.498019 (15.058971)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (19.140625)  time: 0.386312  data: 0.345889  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:23  loss: 14.728824 (15.022107)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (19.940476)  time: 0.402375  data: 0.361988  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:21  loss: 14.875451 (14.944617)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (20.192308)  time: 0.413374  data: 0.373012  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:19  loss: 15.143551 (14.882747)  acc1: 0.000000 (3.830645)  acc5: 18.750000 (19.959677)  time: 0.429803  data: 0.389415  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:16  loss: 15.143551 (14.951828)  acc1: 0.000000 (3.993056)  acc5: 18.750000 (20.833333)  time: 0.373389  data: 0.333001  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:14  loss: 14.830462 (14.869930)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.884146)  time: 0.355115  data: 0.314727  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:12  loss: 14.137599 (14.812295)  acc1: 0.000000 (3.940217)  acc5: 25.000000 (21.331522)  time: 0.362163  data: 0.321780  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:10  loss: 14.249454 (14.806942)  acc1: 6.250000 (4.044118)  acc5: 25.000000 (21.323529)  time: 0.346350  data: 0.305984  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:08  loss: 13.856274 (14.717068)  acc1: 6.250000 (4.129464)  acc5: 18.750000 (21.428571)  time: 0.360886  data: 0.320536  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:07  loss: 13.856274 (14.738635)  acc1: 6.250000 (4.200820)  acc5: 18.750000 (21.004098)  time: 0.351752  data: 0.311408  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:05  loss: 14.457070 (14.790233)  acc1: 0.000000 (3.882576)  acc5: 18.750000 (20.928030)  time: 0.348054  data: 0.307681  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:03  loss: 14.620043 (14.808658)  acc1: 0.000000 (3.697183)  acc5: 18.750000 (21.302817)  time: 0.352221  data: 0.311841  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 14.662786 (14.750938)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (21.299342)  time: 0.341919  data: 0.301518  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 14.662786 (14.743645)  acc1: 0.000000 (4.000000)  acc5: 25.000000 (21.200000)  time: 0.327973  data: 0.288941  max mem: 210\n",
      "Test: Total time: 0:00:28 (0.359637 s / it)\n",
      "* Acc@1 4.000 Acc@5 21.200 loss 14.744\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: fgsm on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 3.319745 (3.319745)  acc1: 50.000000 (50.000000)  acc5: 68.750000 (68.750000)  time: 0.186796  data: 0.146633  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 3.319745 (3.568536)  acc1: 43.750000 (42.000000)  acc5: 68.750000 (68.000000)  time: 0.148535  data: 0.115025  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.148918 s / it)\n",
      "* Acc@1 42.000 Acc@5 68.000 loss 3.569\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: fgsm on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.298653 (0.298653)  acc1: 93.750000 (93.750000)  acc5: 93.750000 (93.750000)  time: 0.184684  data: 0.144389  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.074663)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (98.000000)  time: 0.162506  data: 0.129121  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.162937 s / it)\n",
      "* Acc@1 98.000 Acc@5 98.000 loss 0.075\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: fgsm on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 4.827378 (4.827378)  acc1: 43.750000 (43.750000)  acc5: 56.250000 (56.250000)  time: 0.186411  data: 0.145977  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 3.839459 (3.874488)  acc1: 43.750000 (42.000000)  acc5: 62.500000 (64.000000)  time: 0.147925  data: 0.114567  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.148288 s / it)\n",
      "* Acc@1 42.000 Acc@5 64.000 loss 3.874\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: fgsm on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:24  loss: 11.006817 (11.006817)  acc1: 12.500000 (12.500000)  acc5: 37.500000 (37.500000)  time: 0.307909  data: 0.267633  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:31  loss: 11.006817 (11.411270)  acc1: 6.250000 (8.333333)  acc5: 25.000000 (25.000000)  time: 0.422542  data: 0.382165  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:25  loss: 11.917228 (11.825965)  acc1: 6.250000 (5.681818)  acc5: 25.000000 (24.431818)  time: 0.374328  data: 0.333947  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:22  loss: 12.243926 (12.332522)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (22.265625)  time: 0.354860  data: 0.314496  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:20  loss: 12.243926 (12.178569)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (22.916667)  time: 0.355541  data: 0.315189  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:19  loss: 12.243926 (12.168630)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (23.317308)  time: 0.334220  data: 0.293875  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:17  loss: 12.282470 (12.195342)  acc1: 0.000000 (3.830645)  acc5: 25.000000 (24.395161)  time: 0.350611  data: 0.310215  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:15  loss: 12.161049 (12.265634)  acc1: 0.000000 (3.993056)  acc5: 25.000000 (23.784722)  time: 0.351618  data: 0.311231  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:13  loss: 12.146587 (12.163893)  acc1: 0.000000 (3.963415)  acc5: 25.000000 (23.628049)  time: 0.352689  data: 0.312341  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:11  loss: 12.004870 (12.097364)  acc1: 0.000000 (3.940217)  acc5: 25.000000 (23.913043)  time: 0.345753  data: 0.305471  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:10  loss: 11.792064 (12.083798)  acc1: 6.250000 (4.044118)  acc5: 25.000000 (24.877451)  time: 0.325471  data: 0.285293  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:08  loss: 11.646162 (12.092948)  acc1: 6.250000 (4.129464)  acc5: 25.000000 (24.218750)  time: 0.337746  data: 0.297600  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:06  loss: 11.950278 (12.096414)  acc1: 6.250000 (4.200820)  acc5: 25.000000 (24.077869)  time: 0.323628  data: 0.283491  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:04  loss: 12.443351 (12.123376)  acc1: 0.000000 (3.882576)  acc5: 25.000000 (23.674242)  time: 0.324495  data: 0.284365  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:03  loss: 12.285851 (12.124089)  acc1: 0.000000 (3.697183)  acc5: 18.750000 (23.767606)  time: 0.329601  data: 0.289485  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 12.285851 (12.092785)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (24.177632)  time: 0.317230  data: 0.277126  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 12.237530 (12.061623)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (24.240000)  time: 0.305862  data: 0.267216  max mem: 210\n",
      "Test: Total time: 0:00:26 (0.335202 s / it)\n",
      "* Acc@1 4.000 Acc@5 24.240 loss 12.062\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: pgd on pgd ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 3.319745 (3.319745)  acc1: 50.000000 (50.000000)  acc5: 68.750000 (68.750000)  time: 0.178374  data: 0.138324  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 3.319745 (3.568536)  acc1: 43.750000 (42.000000)  acc5: 68.750000 (68.000000)  time: 0.140574  data: 0.107852  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.140910 s / it)\n",
      "* Acc@1 42.000 Acc@5 68.000 loss 3.569\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: pgd on cw ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 0.298653 (0.298653)  acc1: 93.750000 (93.750000)  acc5: 93.750000 (93.750000)  time: 0.178241  data: 0.137773  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 0.000000 (0.074664)  acc1: 100.000000 (98.000000)  acc5: 100.000000 (98.000000)  time: 0.140889  data: 0.107965  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.141182 s / it)\n",
      "* Acc@1 98.000 Acc@5 98.000 loss 0.075\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: pgd on fgsm ##################################################\n",
      "Test:  [0/4]  eta: 0:00:00  loss: 4.827378 (4.827378)  acc1: 43.750000 (43.750000)  acc5: 56.250000 (56.250000)  time: 0.180035  data: 0.139980  max mem: 210\n",
      "Test:  [3/4]  eta: 0:00:00  loss: 3.839459 (3.874488)  acc1: 43.750000 (42.000000)  acc5: 62.500000 (64.000000)  time: 0.142237  data: 0.109280  max mem: 210\n",
      "Test: Total time: 0:00:00 (0.142524 s / it)\n",
      "* Acc@1 42.000 Acc@5 64.000 loss 3.874\n",
      "################################################## Validating Posthoc: pgd_06 and adv_classifier: pgd on clean ##################################################\n",
      "Test:  [ 0/79]  eta: 0:00:24  loss: 11.795970 (11.795970)  acc1: 12.500000 (12.500000)  acc5: 25.000000 (25.000000)  time: 0.306461  data: 0.266268  max mem: 210\n",
      "Test:  [ 5/79]  eta: 0:00:22  loss: 11.018625 (11.406765)  acc1: 6.250000 (8.333333)  acc5: 18.750000 (20.833333)  time: 0.306702  data: 0.266604  max mem: 210\n",
      "Test:  [10/79]  eta: 0:00:21  loss: 11.580111 (11.467379)  acc1: 6.250000 (5.681818)  acc5: 18.750000 (20.454545)  time: 0.306212  data: 0.266108  max mem: 210\n",
      "Test:  [15/79]  eta: 0:00:19  loss: 11.580111 (11.560608)  acc1: 0.000000 (3.906250)  acc5: 18.750000 (21.093750)  time: 0.304743  data: 0.264611  max mem: 210\n",
      "Test:  [20/79]  eta: 0:00:18  loss: 11.416915 (11.553357)  acc1: 0.000000 (4.166667)  acc5: 18.750000 (21.428571)  time: 0.313730  data: 0.273543  max mem: 210\n",
      "Test:  [25/79]  eta: 0:00:17  loss: 11.416915 (11.531420)  acc1: 0.000000 (4.326923)  acc5: 18.750000 (20.673077)  time: 0.329052  data: 0.288783  max mem: 210\n",
      "Test:  [30/79]  eta: 0:00:15  loss: 11.416915 (11.530670)  acc1: 0.000000 (3.830645)  acc5: 18.750000 (20.766129)  time: 0.337501  data: 0.297140  max mem: 210\n",
      "Test:  [35/79]  eta: 0:00:14  loss: 11.416915 (11.576890)  acc1: 0.000000 (3.993056)  acc5: 18.750000 (20.833333)  time: 0.342224  data: 0.301800  max mem: 210\n",
      "Test:  [40/79]  eta: 0:00:12  loss: 11.172412 (11.517733)  acc1: 0.000000 (3.963415)  acc5: 18.750000 (20.426829)  time: 0.349851  data: 0.309383  max mem: 210\n",
      "Test:  [45/79]  eta: 0:00:11  loss: 11.421421 (11.511051)  acc1: 0.000000 (3.940217)  acc5: 18.750000 (19.565217)  time: 0.346750  data: 0.306266  max mem: 210\n",
      "Test:  [50/79]  eta: 0:00:09  loss: 10.739026 (11.458579)  acc1: 6.250000 (4.044118)  acc5: 18.750000 (20.465686)  time: 0.344098  data: 0.303611  max mem: 210\n",
      "Test:  [55/79]  eta: 0:00:08  loss: 11.313253 (11.433147)  acc1: 6.250000 (4.129464)  acc5: 12.500000 (20.200893)  time: 0.374513  data: 0.334016  max mem: 210\n",
      "Test:  [60/79]  eta: 0:00:06  loss: 11.313253 (11.424241)  acc1: 6.250000 (4.200820)  acc5: 12.500000 (20.696721)  time: 0.371070  data: 0.330613  max mem: 210\n",
      "Test:  [65/79]  eta: 0:00:04  loss: 11.313253 (11.414984)  acc1: 0.000000 (3.882576)  acc5: 18.750000 (20.454545)  time: 0.368538  data: 0.328096  max mem: 210\n",
      "Test:  [70/79]  eta: 0:00:03  loss: 11.385376 (11.439243)  acc1: 0.000000 (3.697183)  acc5: 12.500000 (19.894366)  time: 0.367883  data: 0.327479  max mem: 210\n",
      "Test:  [75/79]  eta: 0:00:01  loss: 11.385376 (11.404295)  acc1: 0.000000 (4.111842)  acc5: 18.750000 (20.312500)  time: 0.338974  data: 0.298598  max mem: 210\n",
      "Test:  [78/79]  eta: 0:00:00  loss: 11.222515 (11.402881)  acc1: 0.000000 (4.000000)  acc5: 18.750000 (20.240000)  time: 0.318087  data: 0.279083  max mem: 210\n",
      "Test: Total time: 0:00:26 (0.337775 s / it)\n",
      "* Acc@1 4.000 Acc@5 20.240 loss 11.403\n"
     ]
    }
   ],
   "source": [
    "# Load posthoc\n",
    "posthocs=[\"cw\", \"fgsm_06\", \"pgd_06\"]\n",
    "\n",
    "adv_models = [\"cw\", \"fgsm\", \"pgd\"]\n",
    "\n",
    "# Perform validation on clean dataset\n",
    "for post_model in posthocs:\n",
    "    posthoc = LinearBC(1536)\n",
    "    posthoc.to(device)\n",
    "    posthoc.load_state_dict(torch.load(f'''/cluster/scratch/mmathys/dl_data/posthoc-models/{post_model}.pt'''))\n",
    "    \n",
    "    for adv_model in adv_models:\n",
    "        adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                          num_labels=len(CLASS_SUBSET))\n",
    "        adv_classifier.to(device)\n",
    "        adv_classifier.load_state_dict(torch.load(f'''/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/{version}/{adv_model}.pt'''))\n",
    "        \n",
    "        for attack, loaders in loader_dict.items():\n",
    "            print(\"#\"*30 + f''' Validating Posthoc: {post_model} and adv_classifier: {adv_model} on {attack} ''' + \"#\"*30)\n",
    "            log_dict, logger = validate_multihead_network(model, \n",
    "                                                          posthoc,\n",
    "                                                          adv_classifier,\n",
    "                                                          clean_classifier,\n",
    "                                                          loader_dict[attack][\"validation\"], \n",
    "                                                          tensor_dir=None, \n",
    "                                                          adversarial_attack=None, \n",
    "                                                          n=4, \n",
    "                                                          avgpool=False)\n",
    "            \n",
    "            # Save adversarial Classifier\n",
    "            save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version, \"benchmark\")\n",
    "            if not os.path.isdir(save_path):\n",
    "                os.makedirs(save_path)\n",
    "            save_file_log = f\"log_post{post_model}_adv{adv_model}_attack{attack}.pt\"\n",
    "            torch.save(logger, str(save_path) + \"/\" + save_file_log)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black Box on Multihead model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
