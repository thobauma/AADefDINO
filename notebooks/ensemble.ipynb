{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "# from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino, ViTWrapper\n",
    "from src.model.data import *\n",
    "from src.model.train import *\n",
    "from src.model.multihead_model import *\n",
    "from src.helpers.helpers import create_paths\n",
    "\n",
    "from torchattacks import *\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "MAX_PATH = Path('/','cluster', 'scratch', 'mmathys', 'dl_data')\n",
    "\n",
    "BASE_ADV_PATH = Path(MAX_PATH, 'adversarial_data_tensors')\n",
    "BASE_POSTHOC_PATH = Path(MAX_PATH, 'posthoc_tensors')\n",
    "POSTHOC_MODELS_PATH = Path(MAX_PATH, 'posthoc_models')\n",
    "\n",
    "LINEAR_CLASSIFIER_MODELS_PATH = Path(MAX_PATH, 'linear_classifier_models')\n",
    "\n",
    "ORI_PATH = Path(DATA_PATH, 'ori')\n",
    "CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "CLASS_SUBSET = np.load(CLASS_SUBSET_PATH)\n",
    "\n",
    "ADV_DATASETS = ['cw', 'fgsm_06', 'pgd_03']\n",
    "\n",
    "DATASETS = [*ADV_DATASETS, 'ori']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_SUBSET = None\n",
    "NUM_WORKERS= 0\n",
    "PIN_MEMORY=True\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS= 3\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATHS = create_paths(data_name='ori',\n",
    "                 datasets_paths=None,  \n",
    "                 initial_base_path=DATA_PATH, \n",
    "                 posthoc_base_path=BASE_POSTHOC_PATH, \n",
    "                 train_str='train', \n",
    "                 val_str='validation')\n",
    "\n",
    "for adv_ds in ADV_DATASETS:\n",
    "    DATA_PATHS = create_paths(data_name=adv_ds,\n",
    "                 datasets_paths=DATA_PATHS,  \n",
    "                 initial_base_path=BASE_ADV_PATH, \n",
    "                 posthoc_base_path=BASE_POSTHOC_PATH, \n",
    "                 train_str='train', \n",
    "                 val_str='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/cluster/scratch/mmathys/dl_data/adversarial_data_tensors/cw/train/images')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATHS[\"cw\"][\"init\"][\"train\"][\"images\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import DINO\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights.\n"
     ]
    }
   ],
   "source": [
    "model, linear_classifier = get_dino()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ori\n",
      "/cluster/scratch/thobauma/dl_data/ori/train/images\n",
      "/cluster/scratch/thobauma/dl_data/ori/train/labels.csv\n",
      "cw\n",
      "/cluster/scratch/mmathys/dl_data/adversarial_data_tensors/cw/train/images\n",
      "/cluster/scratch/mmathys/dl_data/adversarial_data_tensors/cw/train/labels.csv\n",
      "fgsm_06\n",
      "/cluster/scratch/mmathys/dl_data/adversarial_data_tensors/fgsm_06/train/images\n",
      "/cluster/scratch/mmathys/dl_data/adversarial_data_tensors/fgsm_06/train/labels.csv\n",
      "pgd_03\n",
      "/cluster/scratch/mmathys/dl_data/adversarial_data_tensors/pgd_03/train/images\n",
      "/cluster/scratch/mmathys/dl_data/adversarial_data_tensors/pgd_03/train/labels.csv\n"
     ]
    }
   ],
   "source": [
    "for k, v in DATA_PATHS.items():\n",
    "    print(k)\n",
    "    print(v[\"init\"][\"train\"][\"images\"])\n",
    "    print(v[\"init\"][\"train\"][\"label\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cw\n",
      "fgsm_06\n",
      "pgd_03\n"
     ]
    }
   ],
   "source": [
    "# Remember to set the correct transformation\n",
    "# encoder\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit([i for i in CLASS_SUBSET])\n",
    "\n",
    "loader_dict = defaultdict(dict)\n",
    "\n",
    "for k, v in DATA_PATHS.items():\n",
    "    if not k == \"ori\":\n",
    "        print(k)\n",
    "        adv_train_dataset = EnsembleDataset(v[\"init\"][\"train\"][\"images\"], \n",
    "                                            v[\"init\"][\"train\"][\"label\"])\n",
    "        \n",
    "        adv_val_dataset = EnsembleDataset(v[\"init\"][\"validation\"][\"images\"], \n",
    "                                          v[\"init\"][\"validation\"][\"label\"])\n",
    "\n",
    "        loader_dict[k][\"train\"] = DataLoader(adv_train_dataset, \n",
    "                                             batch_size=BATCH_SIZE, \n",
    "                                             num_workers=NUM_WORKERS, \n",
    "                                             pin_memory=PIN_MEMORY, \n",
    "                                             shuffle=True)\n",
    "        \n",
    "        loader_dict[k][\"validation\"] = DataLoader(adv_val_dataset, \n",
    "                                             batch_size=BATCH_SIZE, \n",
    "                                             num_workers=NUM_WORKERS, \n",
    "                                             pin_memory=PIN_MEMORY, \n",
    "                                             shuffle=False)\n",
    "    else:\n",
    "        clean_train_dataset = ImageDataset(v[\"init\"][\"train\"][\"images\"], \n",
    "                                           v[\"init\"][\"train\"][\"label\"], \n",
    "                                           ORIGINAL_TRANSFORM,\n",
    "                                           CLASS_SUBSET, \n",
    "                                           index_subset=None, \n",
    "                                           label_encoder=label_encoder)\n",
    "\n",
    "        clean_val_dataset = ImageDataset(v[\"init\"][\"validation\"][\"images\"], \n",
    "                                         v[\"init\"][\"validation\"][\"label\"],\n",
    "                                         ORIGINAL_TRANSFORM,\n",
    "                                         CLASS_SUBSET, \n",
    "                                         index_subset=None, \n",
    "                                         label_encoder=label_encoder)\n",
    "        \n",
    "        loader_dict[\"ori\"][\"train\"] = DataLoader(clean_train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=True)\n",
    "        \n",
    "        loader_dict[\"ori\"][\"validation\"] = DataLoader(clean_val_dataset,\n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              pin_memory=PIN_MEMORY, \n",
    "                              shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2b946ac272b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_dict[\"cw\"][\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'last_hope'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(loader_dict[\"pgd_03\"][\"train\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(dim, num_labels)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # linear layer\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Binary Classifier\n",
    "class LinearBC(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(LinearBC,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train various classifiers on all adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################################################################################################\n",
      "################################################## Training classifier for ori ##################################################\n",
      "#################################################################################################################################\n",
      "Epoch: [0]  [  0/126]  eta: 0:08:12  lr: 0.001000  loss: 4.562117 (4.562117)  time: 3.907374  data: 3.410567  max mem: 1723\n",
      "Epoch: [0]  [ 20/126]  eta: 0:06:16  lr: 0.001000  loss: 0.287717 (1.009202)  time: 3.537406  data: 3.031131  max mem: 1725\n",
      "Epoch: [0]  [ 40/126]  eta: 0:05:06  lr: 0.001000  loss: 0.127130 (0.577551)  time: 3.580346  data: 3.073099  max mem: 1725\n",
      "Epoch: [0]  [ 60/126]  eta: 0:03:55  lr: 0.001000  loss: 0.120224 (0.431370)  time: 3.568691  data: 3.064073  max mem: 1725\n",
      "Epoch: [0]  [ 80/126]  eta: 0:02:43  lr: 0.001000  loss: 0.089167 (0.348410)  time: 3.550090  data: 3.046097  max mem: 1725\n",
      "Epoch: [0]  [100/126]  eta: 0:01:32  lr: 0.001000  loss: 0.094006 (0.299358)  time: 3.529479  data: 3.023664  max mem: 1725\n",
      "Epoch: [0]  [120/126]  eta: 0:00:21  lr: 0.001000  loss: 0.083479 (0.265608)  time: 3.570855  data: 3.063783  max mem: 1725\n",
      "Epoch: [0]  [125/126]  eta: 0:00:03  lr: 0.001000  loss: 0.083299 (0.258620)  time: 3.532491  data: 3.031972  max mem: 1725\n",
      "Epoch: [0] Total time: 0:07:27 (3.553911 s / it)\n",
      "Averaged stats: lr: 0.001000  loss: 0.083299 (0.258620)\n",
      "Test:  [0/5]  eta: 0:00:18  loss: 0.101936 (0.101936)  acc1: 95.703125 (95.703125)  acc5: 100.000000 (100.000000)  time: 3.664612  data: 3.168752  max mem: 1725\n",
      "Test:  [1/5]  eta: 0:00:15  loss: 0.101936 (0.116585)  acc1: 95.703125 (95.898438)  acc5: 99.609375 (99.804688)  time: 3.901068  data: 3.396126  max mem: 1725\n",
      "Test:  [2/5]  eta: 0:00:11  loss: 0.115332 (0.116167)  acc1: 96.093750 (96.223958)  acc5: 100.000000 (99.869792)  time: 3.958859  data: 3.453350  max mem: 1725\n",
      "Test:  [3/5]  eta: 0:00:07  loss: 0.109823 (0.114581)  acc1: 95.703125 (96.093750)  acc5: 100.000000 (99.902344)  time: 3.950292  data: 3.442060  max mem: 1725\n",
      "Test:  [4/5]  eta: 0:00:03  loss: 0.109823 (0.100968)  acc1: 96.093750 (96.480000)  acc5: 100.000000 (99.920000)  time: 3.840405  data: 3.342698  max mem: 1725\n",
      "Test: Total time: 0:00:19 (3.841068 s / it)\n",
      "* Acc@1 96.480 Acc@5 99.920 loss 0.101\n",
      "Accuracy at epoch 0 of the network on the 5 test images: 96.5%\n",
      "Max accuracy so far: 96.48%\n",
      "Training of the supervised linear classifier on frozen features completed.\n",
      "Top-1 test accuracy: 96.5\n",
      "Finished Training classifier on ori\n",
      "################################################################################################################################\n",
      "################################################## Training classifier for cw ##################################################\n",
      "################################################################################################################################\n",
      "Epoch: [0]  [  0/126]  eta: 0:32:55  lr: 0.001000  loss: 4.467798 (4.467798)  time: 15.682453  data: 15.169256  max mem: 1725\n",
      "Epoch: [0]  [ 20/126]  eta: 0:27:52  lr: 0.001000  loss: 0.829293 (1.595452)  time: 15.787837  data: 15.282030  max mem: 1725\n",
      "Epoch: [0]  [ 40/126]  eta: 0:22:32  lr: 0.001000  loss: 0.437458 (1.029341)  time: 15.669223  data: 15.164578  max mem: 1725\n"
     ]
    }
   ],
   "source": [
    "run_attacks = [\n",
    "            \"ori\", \n",
    "            \"cw\",\n",
    "#             \"fgsm_06\", \n",
    "#             \"pgd_03\"\n",
    "]\n",
    "\n",
    "for attack, loaders in loader_dict.items():\n",
    "    if attack not in run_attacks:\n",
    "        print(f'''Skipping for {attack}''')\n",
    "        continue\n",
    "        \n",
    "    # Initialise classifier\n",
    "    adv_linear_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                         num_labels=len(CLASS_SUBSET))\n",
    "    adv_linear_classifier = adv_linear_classifier.cuda()\n",
    "    \n",
    "    # Metric logger path\n",
    "#     LOG_PATH = Path(LOG_BASE_PATH, 'adv_classifier', version, attack)\n",
    "#     if not os.path.isdir(LOG_PATH):\n",
    "#         os.makedirs(LOG_PATH)\n",
    "    \n",
    "    # train\n",
    "    pstr = \"#\"*50 + f''' Training classifier for {attack} ''' + \"#\"*50\n",
    "    print(len(pstr)*\"#\")\n",
    "    print(pstr)\n",
    "    print(len(pstr)*\"#\")\n",
    "    \n",
    "    loggers = train(model, \n",
    "                    adv_linear_classifier, \n",
    "                    loaders[\"train\"], \n",
    "                    loaders[\"validation\"], \n",
    "                    log_dir=Path(LINEAR_CLASSIFIER_MODELS_PATH, version, attack),\n",
    "                    tensor_dir=None, \n",
    "                    optimizer=None, \n",
    "                    adversarial_attack=None,\n",
    "                    criterion=nn.CrossEntropyLoss(),\n",
    "                    epochs=1, \n",
    "                    val_freq=1, \n",
    "                    batch_size=BATCH_SIZE,  \n",
    "                    lr=0.001, \n",
    "                    to_restore = {\"epoch\": 0, \"best_acc\": 0.}, \n",
    "                    n=4, \n",
    "                    avgpool_patchtokens=False, \n",
    "                    show_image=False)\n",
    "    \n",
    "    # Save adversarial Classifier\n",
    "    save_path = Path(LINEAR_CLASSIFIER_MODELS_PATH, version, attack)\n",
    "    \n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    save_file_log = f\"log_{attack}.pt\"\n",
    "    torch.save(loggers, Path(save_path, save_file_log))\n",
    "    \n",
    "    print(f'''Finished Training classifier on {attack}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on all adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = [x for x in loader_dict.keys()]\n",
    "\n",
    "for attack in attacks:\n",
    "    if attack == \"ori\":\n",
    "        pstr = \"#\"*30 + f''' evaluating adv_classifier trained on {attack} ''' + \"#\"*30\n",
    "        print(len(pstr)*\"#\")\n",
    "        print(pstr)\n",
    "        print(len(pstr)*\"#\")\n",
    "        adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                 num_labels=len(CLASS_SUBSET))\n",
    "        adv_classifier.to(DEVICE)\n",
    "\n",
    "        # load from checkpoint\n",
    "        log_dir = Path(LINEAR_CLASSIFIER_MODELS_PATH, attack)\n",
    "        to_restore={'epoch': 1}\n",
    "        utils.restart_from_checkpoint(\n",
    "            Path(log_dir, \"checkpoint.pth.tar\"),\n",
    "            run_variables=to_restore,\n",
    "            state_dict=adv_classifier\n",
    "        )\n",
    "\n",
    "        for applied_attack in attacks:\n",
    "\n",
    "            print(\">\"*5 + f\" {applied_attack} dataset \")\n",
    "            logger_dict, logger = validate_network(model, \n",
    "                                                   adv_classifier, \n",
    "                                                   loader_dict[applied_attack][\"validation\"], \n",
    "                                                   criterion=nn.CrossEntropyLoss(),\n",
    "                                                   tensor_dir=None, \n",
    "                                                   adversarial_attack=None, \n",
    "                                                   n=4, \n",
    "                                                   avgpool_patchtokens=False, \n",
    "                                                   path_predictions=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on newly generated attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = [x for x in loader_dict.keys()]\n",
    "\n",
    "for attack in attacks:\n",
    "    \n",
    "    pstr = \"#\"*30 + f''' evaluating adv_classifier trained on {attack} ''' + \"#\"*30\n",
    "    print(len(pstr)*\"#\")\n",
    "    print(pstr)\n",
    "    print(len(pstr)*\"#\")\n",
    "    \n",
    "    adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                             num_labels=len(CLASS_SUBSET))\n",
    "    adv_classifier.to(DEVICE)\n",
    "    \n",
    "    log_dir = Path(LINEAR_CLASSIFIER_MODELS_PATH, attack)\n",
    "    to_restore={'epoch': 1}\n",
    "    utils.restart_from_checkpoint(\n",
    "        Path(log_dir, \"checkpoint.pth.tar\"),\n",
    "        run_variables=to_restore,\n",
    "        state_dict=adv_classifier\n",
    "    )\n",
    "    \n",
    "    vits = ViTWrapper(model, adv_classifier, transform=None)\n",
    "\n",
    "    for applied_attack in attacks:\n",
    "        \n",
    "        if applied_attack == \"pgd\":\n",
    "            ev_attack = PGD(vits, eps=0.3, alpha=6/255, steps=15)\n",
    "        elif applied_attack == \"cw\":\n",
    "            ev_attack = CW(vits, c=10, lr=0.003, steps=30)\n",
    "        elif applied_attack == \"fgsm\":\n",
    "            ev_attack = FGSM(vits, eps=0.03)\n",
    "        else:\n",
    "            continue\n",
    " \n",
    "        print(\">\"*5 + f''' applying attack: {ev_attack} ''')\n",
    "        logger_dict, logger = validate_network(model, \n",
    "                                               adv_classifier, \n",
    "                                               loader_dict[\"ori\"][\"validation\"],\n",
    "                                               criterion=nn.CrossEntropyLoss(),\n",
    "                                               tensor_dir=None, \n",
    "                                               adversarial_attack=ev_attack,\n",
    "                                               n=4, \n",
    "                                               avgpool_patchtokens=False, \n",
    "                                               path_predictions=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on full pipeline with post-hoc as multiplexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean_classifier\n",
    "name=\"clean\"\n",
    "clean_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                    num_labels=len(CLASS_SUBSET))\n",
    "clean_classifier.to(DEVICE)\n",
    "\n",
    "log_dir = Path(LINEAR_CLASSIFIER_MODELS_PATH, \"ori\")\n",
    "to_restore={'epoch': 1}\n",
    "utils.restart_from_checkpoint(\n",
    "    Path(log_dir, \"checkpoint.pth.tar\"),\n",
    "    run_variables=to_restore,\n",
    "    state_dict=clean_classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load posthoc\n",
    "posthocs=[\"cw\", \"fgsm_06\", \"pgd_03\"]\n",
    "\n",
    "adv_models = [\"cw\", \"fgsm_06\", \"pgd_03\"]\n",
    "\n",
    "# Perform validation on clean dataset\n",
    "for post_model in posthocs:\n",
    "    \n",
    "    log_dir = Path(POSTHOC_MODELS_PATH, post_model)\n",
    "    \n",
    "    posthoc = LinearBC(1536)\n",
    "    posthoc.cuda()\n",
    "    to_restore={'epoch':3}\n",
    "    utils.restart_from_checkpoint(\n",
    "        Path(log_dir, \"checkpoint.pth.tar\"),\n",
    "        run_variables=to_restore,\n",
    "        state_dict=posthoc\n",
    "    )\n",
    "    \n",
    "    for adv_model in adv_models:\n",
    "        adv_classifier = LinearClassifier(linear_classifier.linear.in_features, \n",
    "                                          num_labels=len(CLASS_SUBSET))\n",
    "        adv_classifier.to(DEVICE)\n",
    "        \n",
    "        log_dir = Path(LINEAR_CLASSIFIER_MODELS_PATH, \"adv_model\")\n",
    "        to_restore={'epoch': 1}\n",
    "        \n",
    "        utils.restart_from_checkpoint(\n",
    "            Path(log_dir, \"checkpoint.pth.tar\"),\n",
    "            run_variables=to_restore,\n",
    "            state_dict=adv_classifier\n",
    "        )\n",
    "        \n",
    "        for attack, loaders in loader_dict.items():\n",
    "            pstr = \"#\"*30 + f''' Validating Posthoc: {post_model} and adv_classifier: {adv_model} on {attack} ''' + \"#\"*30\n",
    "            print(len(pstr)*\"#\")\n",
    "            print(pstr)\n",
    "            print(len(pstr)*\"#\")\n",
    "            log_dict, logger = validate_multihead_network(model, \n",
    "                                                          posthoc,\n",
    "                                                          adv_classifier,\n",
    "                                                          clean_classifier,\n",
    "                                                          loader_dict[attack][\"validation\"], \n",
    "                                                          tensor_dir=None, \n",
    "                                                          adversarial_attack=None, \n",
    "                                                          n=4, \n",
    "                                                          avgpool=False)\n",
    "            \n",
    "            # Save adversarial Classifier\n",
    "            save_path = Path(ADVERSARIAL_CLASSIFIER_MODEL_SAVE_PATH, version, \"benchmark\")\n",
    "            if not os.path.isdir(save_path):\n",
    "                os.makedirs(save_path)\n",
    "            save_file_log = f\"log_post{post_model}_adv{adv_model}_attack{attack}.pt\"\n",
    "            torch.save(logger, str(save_path) + \"/\" + save_file_log)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black Box on Multihead model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
