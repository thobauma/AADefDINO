{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This extension reloads external Python files\n",
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sys\n",
    "from torch.utils.data import random_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "from src.helpers.helpers import get_random_indexes, get_random_classes\n",
    "from src.model.dino_model import get_dino\n",
    "from src.model.train import *\n",
    "from src.model.data import *\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "username = getpass.getuser()\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "MAX_PATH = Path('/','cluster', 'scratch', 'mmathys', 'dl_data')\n",
    "# Path for intermediate outputs\n",
    "BASE_POSTHOC_PATH = Path(MAX_PATH, 'posthoc-fixed-labels/')\n",
    "#BASE_POSTHOC_PATH = Path(MAX_PATH, 'posthoc-subset/')\n",
    "\n",
    "# Original Dataset\n",
    "ORI_PATH = Path(DATA_PATH, 'ori_data/')\n",
    "CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "\n",
    "TR_PATH = Path(ORI_PATH, 'train/')\n",
    "TR_ORI_LABEL_PATH = Path(TR_PATH,'correct_labels.txt')\n",
    "TR_ORI_IMAGES_PATH = Path(TR_PATH,'images')\n",
    "\n",
    "VAL_PATH = Path(ORI_PATH, 'validation/')\n",
    "VAL_ORI_LABEL_PATH = Path(VAL_PATH,'correct_labels.txt')\n",
    "VAL_ORI_IMAGES_PATH = Path(VAL_PATH,'images')\n",
    "\n",
    "# DAmageNet\n",
    "DN_PATH = Path(DATA_PATH, 'damageNet')\n",
    "DN_LABEL_PATH = Path(DN_PATH, 'val_damagenet.txt')\n",
    "DN_IMAGES_PATH = Path(DN_PATH, 'images')\n",
    "DN_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'damagenet')\n",
    "DN_POSTHOC_LABEL_PATH = Path(DN_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "# PGD\n",
    "TR_PGD_PATH = Path(MAX_PATH, 'adversarial_data/pgd_06/train')\n",
    "TR_PGD_LABEL_PATH = TR_ORI_LABEL_PATH\n",
    "TR_PGD_IMAGES_PATH = Path(TR_PGD_PATH, 'images')\n",
    "TR_PGD_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'pgd/train/')\n",
    "TR_PGD_POSTHOC_LABEL_PATH = Path(TR_PGD_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "VAL_PGD_PATH = Path(MAX_PATH, 'adversarial_data/pgd_06/validation')\n",
    "VAL_PGD_LABEL_PATH = VAL_ORI_LABEL_PATH\n",
    "VAL_PGD_IMAGES_PATH = Path(VAL_PGD_PATH, 'images')\n",
    "VAL_PGD_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'pgd/validation/')\n",
    "VAL_PGD_POSTHOC_LABEL_PATH = Path(VAL_PGD_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "# CW\n",
    "TR_CW_PATH = Path(MAX_PATH, 'adversarial_data/cw/train')\n",
    "TR_CW_LABEL_PATH = TR_ORI_LABEL_PATH\n",
    "TR_CW_IMAGES_PATH = Path(TR_CW_PATH, 'images')\n",
    "TR_CW_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'cw/train/')\n",
    "TR_CW_POSTHOC_LABEL_PATH = Path(TR_CW_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "VAL_CW_PATH = Path(MAX_PATH, 'adversarial_data/cw/validation')\n",
    "VAL_CW_LABEL_PATH = VAL_ORI_LABEL_PATH\n",
    "VAL_CW_IMAGES_PATH = Path(VAL_CW_PATH, 'images')\n",
    "VAL_CW_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'cw/validation/')\n",
    "VAL_CW_POSTHOC_LABEL_PATH = Path(VAL_CW_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "# FGSM\n",
    "TR_FGSM_PATH = Path(MAX_PATH, 'adversarial_data/fgsm_06/train')\n",
    "TR_FGSM_LABEL_PATH = TR_ORI_LABEL_PATH\n",
    "TR_FGSM_IMAGES_PATH = Path(TR_FGSM_PATH, 'images')\n",
    "TR_FGSM_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'fgsm/train/')\n",
    "TR_FGSM_POSTHOC_LABEL_PATH = Path(TR_FGSM_POSTHOC_PATH, 'labels.csv')\n",
    "\n",
    "VAL_FGSM_PATH = Path(MAX_PATH, 'adversarial_data/fgsm_06/validation')\n",
    "VAL_FGSM_LABEL_PATH = VAL_ORI_LABEL_PATH\n",
    "VAL_FGSM_IMAGES_PATH = Path(VAL_FGSM_PATH, 'images')\n",
    "VAL_FGSM_POSTHOC_PATH = Path(BASE_POSTHOC_PATH, 'fgsm/validation/')\n",
    "VAL_FGSM_POSTHOC_LABEL_PATH = Path(VAL_FGSM_POSTHOC_PATH, 'labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If CLASS_SUBSET is specified, INDEX_SUBSET will be ignored. Set CLASS_SUBSET=None if you want to use indexes.\n",
    "# INDEX_SUBSET = get_random_indexes(number_of_images = 50000, n_samples=1000)\n",
    "# CLASS_SUBSET = get_random_classes(number_of_classes = 25, min_rand_class = 1, max_rand_class = 1001)\n",
    "\n",
    "\n",
    "CLASS_SUBSET = np.load(CLASS_SUBSET_PATH)\n",
    "# CLASS_SUBSET = CLASS_SUBSET[:2]\n",
    "\n",
    "INDEX_SUBSET = None\n",
    "NUM_WORKERS= 0\n",
    "PIN_MEMORY=True\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_paths = {\n",
    "            'cw':{ \n",
    "                'b':{\n",
    "                    'train':{\n",
    "                        'label':TR_ORI_LABEL_PATH,\n",
    "                        'images':TR_CW_IMAGES_PATH\n",
    "                    },\n",
    "                    'val':\n",
    "                    {\n",
    "                        'label':VAL_ORI_LABEL_PATH,\n",
    "                        'images':VAL_CW_IMAGES_PATH\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'ori':{\n",
    "                'b':{\n",
    "                    'train':{\n",
    "                        'label':TR_ORI_LABEL_PATH,\n",
    "                        'images':TR_ORI_IMAGES_PATH\n",
    "                    },\n",
    "                    'val':{\n",
    "                        'label':VAL_ORI_LABEL_PATH,\n",
    "                        'images':VAL_ORI_IMAGES_PATH\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'dn':{\n",
    "                'b':{\n",
    "                    'train':{\n",
    "                        'label':TR_CW_PATH,\n",
    "                        'images':None\n",
    "                    },\n",
    "                    'val':\n",
    "                    {\n",
    "                        'label':VAL_ORI_LABEL_PATH,\n",
    "                        'images':DN_IMAGES_PATH\n",
    "                    }\n",
    "                 }\n",
    "            },\n",
    "            'fgsm_06':{\n",
    "                'b':{\n",
    "                    'train':{\n",
    "                        'label':TR_ORI_LABEL_PATH,\n",
    "                        'images':TR_FGSM_IMAGES_PATH\n",
    "                    },\n",
    "                    'val':\n",
    "                    {\n",
    "                        'label':VAL_ORI_LABEL_PATH,\n",
    "                        'images':VAL_FGSM_IMAGES_PATH\n",
    "                    }\n",
    "                 }\n",
    "            },\n",
    "            'pgd_06':{\n",
    "                'b':{\n",
    "                    'train':{\n",
    "                        'label':TR_ORI_LABEL_PATH,\n",
    "                        'images':TR_PGD_IMAGES_PATH\n",
    "                    },\n",
    "                    'val':\n",
    "                    {\n",
    "                        'label':VAL_ORI_LABEL_PATH,\n",
    "                        'images':VAL_PGD_IMAGES_PATH\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "}\n",
    "\n",
    "datasets = ['ori', 'cw', 'pgd_06', 'fgsm_06']\n",
    "for ds in datasets:\n",
    "    ds_dict = datasets_paths[ds]\n",
    "    ds_dict['p'] = {\n",
    "        'train': { \n",
    "            'images': Path(BASE_POSTHOC_PATH, ds, 'train', 'images'),\n",
    "            'label': Path(BASE_POSTHOC_PATH, ds, 'train', 'labels.csv')\n",
    "        },\n",
    "        'val': { \n",
    "            'images': Path(BASE_POSTHOC_PATH, ds, 'val', 'images'),\n",
    "            'label': Path(BASE_POSTHOC_PATH, ds, 'val', 'labels.csv')\n",
    "        }\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_paths['cw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import DINO\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(dim, num_labels)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # linear layer\n",
    "        return self.linear(x)\n",
    "    \n",
    "model, dino_classifier = get_dino()\n",
    "\n",
    "linear_classifier = LinearClassifier(dino_classifier.linear.in_features, \n",
    "                         num_labels=len(CLASS_SUBSET))\n",
    "\n",
    "linear_classifier.load_state_dict(torch.load(\"/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes\" + \"/\" + \"clean.pt\"))\n",
    "linear_classifier.cuda()\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit([i for i in CLASS_SUBSET])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posthoc_forward_pass(model, classifier, datasets, datasets_paths):\n",
    "    logger_dict = {}\n",
    "    for ds in datasets:\n",
    "        ds_b = datasets_paths[ds]['b']\n",
    "        ds_p = datasets_paths[ds]['p']\n",
    "        \n",
    "        logger_dict[ds] = {}\n",
    "        \n",
    "        transform = ONLY_NORMALIZE_TRANSFORM\n",
    "        \n",
    "        if ds == 'ori':\n",
    "            transform = ORIGINAL_TRANSFORM\n",
    "            \n",
    "        for tv in ['train', 'val']:            \n",
    "            print(f'''images: {ds_b[tv]['images']}\\nlabel: {ds_b[tv]['label']}\\npred: {ds_p[tv]['label']}''')\n",
    "            \n",
    "            data_set = AdvTrainingImageDataset(img_folder=ds_b[tv]['images'], \n",
    "                                               labels_file_name=ds_b[tv]['label'], \n",
    "                                               #labels_file_name='/cluster/home/thobauma/deeplearning/data-mmathys/adversarial_data_test/cw/train/labels.txt',\n",
    "                                               transform=transform, \n",
    "                                               class_subset=CLASS_SUBSET,\n",
    "                                               index_subset=None,\n",
    "                                               label_encoder=label_encoder)\n",
    "            \n",
    "            data_loader = DataLoader(data_set, \n",
    "                                     batch_size=BATCH_SIZE, \n",
    "                                     num_workers=NUM_WORKERS, \n",
    "                                     pin_memory=PIN_MEMORY, \n",
    "                                     shuffle=False)\n",
    "            \n",
    "            print(f'''{ds}: {tv} {len(data_set)}''')\n",
    "            logger_dict[ds][tv] = validate_network(model=model, \n",
    "                                                   classifier=classifier, \n",
    "                                                   validation_loader=data_loader, \n",
    "                                                   criterion=nn.CrossEntropyLoss(), \n",
    "                                                   tensor_dir=ds_p[tv]['images'],\n",
    "                                                   adversarial_attack=None, \n",
    "                                                   n=4, \n",
    "                                                   avgpool_patchtokens=False, \n",
    "                                                   path_predictions=ds_p[tv]['label'],\n",
    "                                                   show_image=False)\n",
    "            \n",
    "    return logger_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = ['ori', 'pgd_06', 'fgsm_06', 'cw']\n",
    "logger_dict = posthoc_forward_pass(model,\n",
    "                                   linear_classifier, \n",
    "                                   data_sets, \n",
    "                                   datasets_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def merge_frames(frames, on_what=['file', 'true_labels'], how='left'):\n",
    "    merged_df = reduce(lambda left, right:pd.merge(left, right, on=on_what, how=how,  suffixes=('', '_drop')), frames)\n",
    "    merged_df.drop(merged_df.filter(regex='_drop$').columns.tolist(),axis=1, inplace=True)\n",
    "    return merged_df\n",
    "\n",
    "def get_merged_labels(datasets=['ori', 'cw', 'pgd_06', 'fgsm_06'], datasets_types=['train', 'val'], datasets_paths=datasets_paths, save_path=BASE_POSTHOC_PATH, get_df_dict = False):\n",
    "    df_data_types = {}\n",
    "    df_data = {}\n",
    "    for tv in datasets_types:\n",
    "        df_data[tv] = {}\n",
    "        for ds in datasets:\n",
    "            ds_dict = datasets_paths[ds]\n",
    "            df_data[tv][ds] = pd.read_csv(ds_dict['p'][tv]['label'])\n",
    "            df_data[tv][ds].rename(columns = {'pred_labels': ds+'_pred'}, inplace = True)\n",
    "            if ds != 'ori':\n",
    "                df_data[tv][ds] = pd.merge(df_data[tv][ds], df_data[tv]['ori'], on=['file', 'true_labels'], how='left')\n",
    "            df_data[tv][ds].to_csv(Path(BASE_POSTHOC_PATH, ds, tv, 'labels_merged.csv'), sep=\",\", index=None)\n",
    "        df_data_types[tv] = merge_frames(df_data[tv].values())\n",
    "        if save_path is not None:\n",
    "            df_data_types[tv].to_csv(Path(save_path,tv+'.csv'), sep=\",\", index=None)\n",
    "    if get_df_dict:\n",
    "        return df_data_types, df_data\n",
    "\n",
    "    return df_data_types\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_types, df_data = get_merged_labels(get_df_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_types['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['train']['cw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'cw'\n",
    "df = df_data['train'][name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in ['cw', 'pgd_06', 'fgsm_06']:    \n",
    "    df = df_data['train'][name]\n",
    "    print(f'''{name}''')\n",
    "    ldf = len(df)\n",
    "    print(f'''total data:             {ldf}''')\n",
    "    print(f'''correct pred:           {len(df[df['true_labels']==df['ori_pred']])},   {len(df[df['true_labels']==df['ori_pred']])/ldf}''')\n",
    "    print(f'''incorrect adv pred:     {len(df[df['true_labels']!=df[name+'_pred']])},   {len(df[df['true_labels']!=df[name+'_pred']])/ldf}''')\n",
    "    df_f = df[df['true_labels']==df['ori_pred']]\n",
    "    print(f'''number adv tuples:      {len(df_f[df_f['true_labels']!=df_f[name+'_pred']])},   {len(df_f[df_f['true_labels']!=df_f[name+'_pred']])/ldf}''')\n",
    "    print(f'''\\n''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_df = pd.read_csv('/cluster/home/thobauma/deeplearning/data-mmathys/adversarial_data_test/cw/train/labels.txt', sep=\" \", header=None)\n",
    "cw_df.columns=['file', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_stuff = pd.merge(cw_df, df_data['train']['cw'], how='left', suffixes=('', '_post'), on=['file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
