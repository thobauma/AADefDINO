{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install, Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ori', 'cw', 'fgsm_06', 'pgd_03']\n"
     ]
    }
   ],
   "source": [
    "# This extension reloads external Python files\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "from src.model.dino_model import get_dino\n",
    "from src.model.train import *\n",
    "from src.model.data import *\n",
    "from src.helpers.helpers import create_paths\n",
    "\n",
    "# seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = Path('/','cluster', 'scratch', 'thobauma', 'dl_data')\n",
    "MAX_PATH = Path('/','cluster', 'scratch', 'mmathys', 'dl_data')\n",
    "\n",
    "BASE_ADV_PATH = Path(MAX_PATH, 'adversarial_data_tensors')\n",
    "BASE_POSTHOC_PATH = Path(MAX_PATH, 'posthoc_tensors')\n",
    "\n",
    "ORI_PATH = Path(DATA_PATH, 'ori')\n",
    "CLASS_SUBSET_PATH = Path(ORI_PATH, 'class_subset.npy')\n",
    "CLASS_SUBSET = np.load(CLASS_SUBSET_PATH)\n",
    "\n",
    "ADV_DATASETS = ['cw', 'fgsm_06', 'pgd_03']\n",
    "DATASETS = ['ori', *ADV_DATASETS]\n",
    "print(DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATHS = create_paths(data_name='ori',\n",
    "                 datasets_paths=None,  \n",
    "                 initial_base_path=DATA_PATH, \n",
    "                 posthoc_base_path=BASE_POSTHOC_PATH, \n",
    "                 train_str='train', \n",
    "                 val_str='validation')\n",
    "for adv_ds in ADV_DATASETS:\n",
    "    DATA_PATHS = create_paths(data_name=adv_ds,\n",
    "                 datasets_paths=DATA_PATHS,  \n",
    "                 initial_base_path=BASE_ADV_PATH, \n",
    "                 posthoc_base_path=BASE_POSTHOC_PATH, \n",
    "                 train_str='train', \n",
    "                 val_str='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_SUBSET = None\n",
    "NUM_WORKERS= 0\n",
    "PIN_MEMORY=True\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import DINO\n",
    "Official repo: https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Model vit_small built.\n",
      "Embed dim 1536\n",
      "We load the reference pretrained linear weights.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],

   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(dim, num_labels)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # linear layer\n",
    "        return self.linear(x)\n",
    "    \n",
    "model, dino_classifier = get_dino()\n",
    "\n",
    "linear_classifier = LinearClassifier(dino_classifier.linear.in_features, \n",
    "                         num_labels=len(CLASS_SUBSET))\n",
    "\n",
    "linear_classifier.load_state_dict(torch.load(\"/cluster/scratch/mmathys/dl_data/adversarial_data/adv_classifiers/25_classes\" + \"/\" + \"clean.pt\"))\n",
    "linear_classifier.cuda()\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit([i for i in CLASS_SUBSET])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 1,
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posthoc_forward_pass(model, classifier, datasets, datasets_paths):\n",
    "    logger_dict = {}\n",
    "    for ds in datasets:\n",
    "        ds_init = datasets_paths[ds]['init']\n",
    "        ds_posthoc = datasets_paths[ds]['posthoc']\n",
    "        \n",
    "        logger_dict[ds] = {}\n",
    "        print(\"\\n\"+\"#\"*50 + f''' forwardpass for {ds} ''' + \"#\"*50)\n",
    "        \n",
    "        for tv in ['train', 'validation']:            \n",
    "            print(f'''images: {ds_init[tv]['images']}\\nlabel: {ds_init[tv]['label']}\\npred: {ds_posthoc[tv]['label']}''')\n",
    "            \n",
    "            if ds == 'ori':\n",
    "                transform = ORIGINAL_TRANSFORM\n",
    "                data_set = AdvTrainingImageDataset(img_folder=ds_init[tv]['images'], \n",
    "                                   labels_file_name=ds_init[tv]['label'], \n",
    "                                   #labels_file_name='/cluster/home/thobauma/deeplearning/data-mmathys/adversarial_data_test/cw/train/labels.txt',\n",
    "                                   transform=transform, \n",
    "                                   class_subset=CLASS_SUBSET,\n",
    "                                   index_subset=None,\n",
    "                                   label_encoder=label_encoder)\n",
    "                \n",
    "            else:\n",
    "                data_set = PosthocForwardDataset(img_folder=ds_init[tv]['images'], \n",
    "                                                 labels_file_name=ds_init[tv]['label'],\n",
    "                                                 index_subset=None, \n",
    "                                                 class_subset=None)\n",
    "            \n",
    "            data_loader = DataLoader(data_set, \n",
    "                                     batch_size=BATCH_SIZE, \n",
    "                                     num_workers=NUM_WORKERS, \n",
    "                                     pin_memory=PIN_MEMORY, \n",
    "                                     shuffle=False)\n",
    "            \n",
    "            print(f'''{ds}: {tv} {len(data_set)}''')\n",
    "            logger_dict[ds][tv] = validate_network(model=model, \n",
    "                                                   classifier=classifier, \n",
    "                                                   validation_loader=data_loader, \n",
    "                                                   criterion=nn.CrossEntropyLoss(), \n",
    "                                                   tensor_dir=ds_posthoc[tv]['images'],\n",
    "                                                   adversarial_attack=None, \n",
    "                                                   n=4, \n",
    "                                                   avgpool_patchtokens=False, \n",
    "                                                   path_predictions=ds_posthoc[tv]['label'],\n",
    "                                                   show_image=False)\n",
    "            \n",
    "    return logger_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################## forwardpass for ori ##################################################\n",
      "images: /cluster/scratch/thobauma/dl_data/ori/train/images\n",
      "label: /cluster/scratch/thobauma/dl_data/ori/train/labels.csv\n",
      "pred: /cluster/scratch/mmathys/dl_data/posthoc_tensors/ori/train/labels.csv\n",
      "ori: train 32181\n",
      "saving predictions to: /cluster/scratch/mmathys/dl_data/posthoc_tensors/ori/train/labels.csv\n",
      "Test:  [  0/503]  eta: 0:06:36  loss: 0.018451 (0.018451)  acc1: 98.437500 (98.437500)  acc5: 100.000000 (100.000000)  time: 0.787801  data: 0.518269  max mem: 497\n",
      "Test:  [ 20/503]  eta: 0:06:39  loss: 0.027935 (0.045099)  acc1: 98.437500 (98.586310)  acc5: 100.000000 (99.925595)  time: 0.828631  data: 0.572045  max mem: 498\n",
      "Test:  [ 40/503]  eta: 0:06:13  loss: 0.032479 (0.040204)  acc1: 98.437500 (98.818598)  acc5: 100.000000 (99.961890)  time: 0.786478  data: 0.527344  max mem: 498\n",
      "Test:  [ 60/503]  eta: 0:05:59  loss: 0.025912 (0.038865)  acc1: 98.437500 (98.847336)  acc5: 100.000000 (99.948770)  time: 0.820076  data: 0.558678  max mem: 498\n",
      "Test:  [ 80/503]  eta: 0:05:45  loss: 0.035542 (0.040067)  acc1: 98.437500 (98.784722)  acc5: 100.000000 (99.922840)  time: 0.830772  data: 0.570339  max mem: 498\n",
      "Test:  [100/503]  eta: 0:05:46  loss: 0.034375 (0.039740)  acc1: 98.437500 (98.808787)  acc5: 100.000000 (99.922649)  time: 1.034725  data: 0.770533  max mem: 498\n",
      "Test:  [120/503]  eta: 0:05:28  loss: 0.031955 (0.039233)  acc1: 98.437500 (98.799070)  acc5: 100.000000 (99.935434)  time: 0.841911  data: 0.578822  max mem: 498\n",
      "Test:  [140/503]  eta: 0:05:08  loss: 0.040873 (0.040711)  acc1: 98.437500 (98.781028)  acc5: 100.000000 (99.933511)  time: 0.811874  data: 0.552511  max mem: 498\n",
      "Test:  [160/503]  eta: 0:04:49  loss: 0.033023 (0.040173)  acc1: 98.437500 (98.806289)  acc5: 100.000000 (99.922360)  time: 0.804692  data: 0.544770  max mem: 498\n",
      "Test:  [180/503]  eta: 0:04:31  loss: 0.037386 (0.041641)  acc1: 100.000000 (98.817334)  acc5: 100.000000 (99.922307)  time: 0.801810  data: 0.536077  max mem: 498\n",
      "Test:  [200/503]  eta: 0:04:13  loss: 0.016139 (0.040372)  acc1: 100.000000 (98.880597)  acc5: 100.000000 (99.930037)  time: 0.797625  data: 0.538920  max mem: 498\n",
      "Test:  [220/503]  eta: 0:03:55  loss: 0.032956 (0.040393)  acc1: 98.437500 (98.882919)  acc5: 100.000000 (99.929299)  time: 0.813035  data: 0.555089  max mem: 498\n",
      "Test:  [240/503]  eta: 0:03:39  loss: 0.035491 (0.040495)  acc1: 98.437500 (98.878371)  acc5: 100.000000 (99.928683)  time: 0.832428  data: 0.569607  max mem: 498\n",
      "Test:  [260/503]  eta: 0:03:22  loss: 0.032896 (0.040897)  acc1: 98.437500 (98.862548)  acc5: 100.000000 (99.928161)  time: 0.821734  data: 0.559979  max mem: 498\n",
      "Test:  [280/503]  eta: 0:03:05  loss: 0.019946 (0.040754)  acc1: 100.000000 (98.876779)  acc5: 100.000000 (99.933274)  time: 0.816290  data: 0.550473  max mem: 498\n",
      "Test:  [300/503]  eta: 0:02:49  loss: 0.026400 (0.040732)  acc1: 100.000000 (98.899502)  acc5: 100.000000 (99.932517)  time: 0.868395  data: 0.614110  max mem: 498\n",
      "Test:  [320/503]  eta: 0:02:32  loss: 0.023149 (0.040270)  acc1: 98.437500 (98.899922)  acc5: 100.000000 (99.936721)  time: 0.817557  data: 0.562616  max mem: 498\n",
      "Test:  [340/503]  eta: 0:02:15  loss: 0.031171 (0.039894)  acc1: 100.000000 (98.923204)  acc5: 100.000000 (99.940433)  time: 0.820696  data: 0.566181  max mem: 498\n",
      "Test:  [360/503]  eta: 0:01:58  loss: 0.034042 (0.040548)  acc1: 98.437500 (98.913608)  acc5: 100.000000 (99.939404)  time: 0.803947  data: 0.550025  max mem: 498\n",
      "Test:  [380/503]  eta: 0:01:42  loss: 0.024533 (0.039969)  acc1: 100.000000 (98.941929)  acc5: 100.000000 (99.942585)  time: 0.835121  data: 0.569714  max mem: 498\n",
      "Test:  [400/503]  eta: 0:01:25  loss: 0.030407 (0.040291)  acc1: 98.437500 (98.944046)  acc5: 100.000000 (99.941552)  time: 0.827299  data: 0.568454  max mem: 498\n",
      "Test:  [420/503]  eta: 0:01:08  loss: 0.028359 (0.039828)  acc1: 100.000000 (98.964519)  acc5: 100.000000 (99.944329)  time: 0.764058  data: 0.508076  max mem: 498\n",
      "Test:  [440/503]  eta: 0:00:51  loss: 0.026680 (0.039847)  acc1: 100.000000 (98.961876)  acc5: 100.000000 (99.943311)  time: 0.771656  data: 0.512189  max mem: 498\n",
      "Test:  [460/503]  eta: 0:00:35  loss: 0.034918 (0.039994)  acc1: 98.437500 (98.962852)  acc5: 100.000000 (99.945770)  time: 0.837123  data: 0.578522  max mem: 498\n",
      "Test:  [480/503]  eta: 0:00:18  loss: 0.026761 (0.039841)  acc1: 100.000000 (98.970244)  acc5: 100.000000 (99.948025)  time: 0.821936  data: 0.559250  max mem: 498\n",
      "Test:  [500/503]  eta: 0:00:02  loss: 0.024324 (0.039531)  acc1: 98.437500 (98.970808)  acc5: 100.000000 (99.946981)  time: 0.845237  data: 0.582932  max mem: 498\n",
      "Test:  [502/503]  eta: 0:00:00  loss: 0.024324 (0.039695)  acc1: 98.437500 (98.965228)  acc5: 100.000000 (99.947174)  time: 0.833307  data: 0.573318  max mem: 498\n",
      "Test: Total time: 0:06:55 (0.825861 s / it)\n",
      "* Acc@1 98.965 Acc@5 99.947 loss 0.040\n",
      "images: /cluster/scratch/thobauma/dl_data/ori/validation/images\n",
      "label: /cluster/scratch/thobauma/dl_data/ori/validation/labels.csv\n",
      "pred: /cluster/scratch/mmathys/dl_data/posthoc_tensors/ori/validation/labels.csv\n",
      "ori: validation 1250\n",
      "saving predictions to: /cluster/scratch/mmathys/dl_data/posthoc_tensors/ori/validation/labels.csv\n",
      "Test:  [ 0/20]  eta: 0:00:16  loss: 0.070752 (0.070752)  acc1: 96.875000 (96.875000)  acc5: 100.000000 (100.000000)  time: 0.835665  data: 0.577456  max mem: 498\n",
      "Test:  [ 5/20]  eta: 0:00:13  loss: 0.086784 (0.106726)  acc1: 96.875000 (96.614583)  acc5: 100.000000 (100.000000)  time: 0.887795  data: 0.629491  max mem: 498\n",
      "Test:  [10/20]  eta: 0:00:09  loss: 0.087962 (0.096412)  acc1: 96.875000 (96.875000)  acc5: 100.000000 (99.857955)  time: 0.916877  data: 0.656127  max mem: 498\n",
      "Test:  [15/20]  eta: 0:00:04  loss: 0.087962 (0.092802)  acc1: 96.875000 (96.777344)  acc5: 100.000000 (99.804688)  time: 0.928792  data: 0.665463  max mem: 498\n",
      "Test:  [19/20]  eta: 0:00:00  loss: 0.071814 (0.082762)  acc1: 96.875000 (97.200000)  acc5: 100.000000 (99.840000)  time: 0.897761  data: 0.642879  max mem: 498\n",
      "Test: Total time: 0:00:17 (0.897987 s / it)\n",
      "* Acc@1 97.200 Acc@5 99.840 loss 0.083\n",
      "\n",
      "################################################## forwardpass for fgsm_06 ##################################################\n",
      "images: /cluster/scratch/mmathys/dl_data/adversarial_data_tensors/fgsm_06/train/images\n",
      "label: /cluster/scratch/mmathys/dl_data/adversarial_data_tensors/fgsm_06/train/labels.csv\n",
      "pred: /cluster/scratch/mmathys/dl_data/posthoc_tensors/fgsm_06/train/labels.csv\n",
      "fgsm_06: train 32181\n",
      "saving predictions to: /cluster/scratch/mmathys/dl_data/posthoc_tensors/fgsm_06/train/labels.csv\n",
      "Test:  [  0/503]  eta: 0:38:09  loss: 6.163318 (6.163318)  acc1: 6.250000 (6.250000)  acc5: 43.750000 (43.750000)  time: 4.551904  data: 4.284311  max mem: 498\n",
      "Test:  [ 20/503]  eta: 0:38:57  loss: 6.123169 (6.214812)  acc1: 7.812500 (8.482143)  acc5: 43.750000 (44.494048)  time: 4.853131  data: 4.570626  max mem: 498\n",
      "Test:  [ 40/503]  eta: 0:37:28  loss: 6.110985 (6.232229)  acc1: 7.812500 (8.574695)  acc5: 39.062500 (42.454268)  time: 4.875843  data: 4.592316  max mem: 498\n",
      "Test:  [ 60/503]  eta: 0:35:49  loss: 6.008188 (6.178087)  acc1: 7.812500 (8.504098)  acc5: 42.187500 (42.981557)  time: 4.839637  data: 4.555703  max mem: 498\n",
      "Test:  [ 80/503]  eta: 0:34:08  loss: 6.273994 (6.199174)  acc1: 7.812500 (8.487654)  acc5: 42.187500 (42.920525)  time: 4.817999  data: 4.536584  max mem: 498\n",
      "Test:  [100/503]  eta: 0:32:32  loss: 5.987711 (6.186135)  acc1: 9.375000 (8.663366)  acc5: 43.750000 (42.961015)  time: 4.855589  data: 4.569371  max mem: 498\n",
      "Test:  [120/503]  eta: 0:30:51  loss: 6.166484 (6.181565)  acc1: 7.812500 (8.690599)  acc5: 43.750000 (43.039773)  time: 4.777298  data: 4.494038  max mem: 498\n",
      "Test:  [140/503]  eta: 0:29:15  loss: 6.185832 (6.195626)  acc1: 7.812500 (8.665780)  acc5: 40.625000 (42.907801)  time: 4.852696  data: 4.568994  max mem: 498\n",
      "Test:  [160/503]  eta: 0:27:38  loss: 5.949533 (6.180064)  acc1: 7.812500 (8.666537)  acc5: 46.875000 (43.177407)  time: 4.822415  data: 4.541142  max mem: 498\n",
      "Test:  [180/503]  eta: 0:26:01  loss: 6.071629 (6.173894)  acc1: 7.812500 (8.632597)  acc5: 45.312500 (43.456492)  time: 4.825963  data: 4.543503  max mem: 498\n",
      "Test:  [200/503]  eta: 0:24:25  loss: 6.102489 (6.165732)  acc1: 7.812500 (8.729789)  acc5: 42.187500 (43.540112)  time: 4.847434  data: 4.564424  max mem: 498\n",
      "Test:  [220/503]  eta: 0:22:49  loss: 6.151655 (6.169012)  acc1: 7.812500 (8.738688)  acc5: 46.875000 (43.764140)  time: 4.896856  data: 4.616162  max mem: 498\n",
      "Test:  [240/503]  eta: 0:21:14  loss: 5.956027 (6.150067)  acc1: 9.375000 (8.849844)  acc5: 46.875000 (44.093620)  time: 4.882697  data: 4.601251  max mem: 498\n",
      "Test:  [260/503]  eta: 0:19:35  loss: 6.218965 (6.150981)  acc1: 9.375000 (8.878113)  acc5: 45.312500 (44.163075)  time: 4.730826  data: 4.453628  max mem: 498\n",
      "Test:  [280/503]  eta: 0:17:54  loss: 6.122562 (6.156047)  acc1: 9.375000 (8.907918)  acc5: 43.750000 (44.278247)  time: 4.595103  data: 4.316874  max mem: 498\n",
      "Test:  [300/503]  eta: 0:16:15  loss: 6.004921 (6.145701)  acc1: 9.375000 (8.970100)  acc5: 45.312500 (44.414452)  time: 4.597104  data: 4.320910  max mem: 498\n",
      "Test:  [320/503]  eta: 0:14:37  loss: 6.015760 (6.139264)  acc1: 9.375000 (9.000195)  acc5: 43.750000 (44.358450)  time: 4.622904  data: 4.350305  max mem: 498\n",
      "Test:  [340/503]  eta: 0:13:00  loss: 6.023014 (6.144090)  acc1: 7.812500 (8.967192)  acc5: 46.875000 (44.437317)  time: 4.705707  data: 4.433266  max mem: 498\n",
      "Test:  [360/503]  eta: 0:11:24  loss: 6.056786 (6.144690)  acc1: 9.375000 (8.972472)  acc5: 43.750000 (44.494460)  time: 4.711331  data: 4.436556  max mem: 498\n",
      "Test:  [380/503]  eta: 0:09:48  loss: 6.076167 (6.143154)  acc1: 7.812500 (8.932087)  acc5: 42.187500 (44.393865)  time: 4.838128  data: 4.563631  max mem: 498\n",
      "Test:  [400/503]  eta: 0:08:13  loss: 6.194893 (6.145805)  acc1: 9.375000 (8.895729)  acc5: 43.750000 (44.346166)  time: 4.835715  data: 4.560523  max mem: 498\n",
      "Test:  [420/503]  eta: 0:06:37  loss: 6.169906 (6.150094)  acc1: 7.812500 (8.810867)  acc5: 42.187500 (44.306710)  time: 4.857680  data: 4.579129  max mem: 498\n",
      "Test:  [440/503]  eta: 0:05:01  loss: 6.196063 (6.151898)  acc1: 7.812500 (8.758503)  acc5: 43.750000 (44.267290)  time: 4.819240  data: 4.543911  max mem: 498\n",
      "Test:  [460/503]  eta: 0:03:26  loss: 6.195980 (6.153002)  acc1: 10.937500 (8.798807)  acc5: 45.312500 (44.292299)  time: 4.886807  data: 4.610480  max mem: 498\n",
      "Test:  [480/503]  eta: 0:01:50  loss: 6.273192 (6.156397)  acc1: 7.812500 (8.751299)  acc5: 43.750000 (44.266502)  time: 4.905675  data: 4.623668  max mem: 498\n",
      "Test:  [500/503]  eta: 0:00:14  loss: 6.111912 (6.154139)  acc1: 9.375000 (8.801148)  acc5: 43.750000 (44.295783)  time: 5.036376  data: 4.752702  max mem: 498\n",
      "Test:  [502/503]  eta: 0:00:04  loss: 6.112590 (6.157193)  acc1: 7.812500 (8.781579)  acc5: 42.187500 (44.265250)  time: 4.939762  data: 4.658198  max mem: 498\n",
      "Test: Total time: 0:40:18 (4.807581 s / it)\n",
      "* Acc@1 8.782 Acc@5 44.265 loss 6.157\n",
      "images: /cluster/scratch/mmathys/dl_data/adversarial_data_tensors/fgsm_06/validation/images\n",
      "label: /cluster/scratch/mmathys/dl_data/adversarial_data_tensors/fgsm_06/validation/labels.csv\n",
      "pred: /cluster/scratch/mmathys/dl_data/posthoc_tensors/fgsm_06/validation/labels.csv\n",
      "fgsm_06: validation 1250\n",
      "saving predictions to: /cluster/scratch/mmathys/dl_data/posthoc_tensors/fgsm_06/validation/labels.csv\n",
      "Test:  [ 0/20]  eta: 0:01:40  loss: 6.089995 (6.089995)  acc1: 7.812500 (7.812500)  acc5: 43.750000 (43.750000)  time: 5.035820  data: 4.747051  max mem: 498\n",
      "Test:  [ 5/20]  eta: 0:01:15  loss: 6.406136 (6.519743)  acc1: 9.375000 (9.635417)  acc5: 43.750000 (44.791667)  time: 5.010678  data: 4.728120  max mem: 498\n",
      "Test:  [10/20]  eta: 0:00:50  loss: 6.406136 (6.305566)  acc1: 9.375000 (9.517045)  acc5: 45.312500 (45.454545)  time: 5.080170  data: 4.792235  max mem: 498\n",
      "Test:  [15/20]  eta: 0:00:25  loss: 6.262640 (6.192254)  acc1: 9.375000 (9.472656)  acc5: 43.750000 (45.605469)  time: 5.073564  data: 4.786459  max mem: 498\n",
      "Test:  [19/20]  eta: 0:00:04  loss: 6.342644 (6.285185)  acc1: 9.375000 (9.360000)  acc5: 43.750000 (45.120000)  time: 4.884581  data: 4.606879  max mem: 498\n",
      "Test: Total time: 0:01:37 (4.884824 s / it)\n",
      "* Acc@1 9.360 Acc@5 45.120 loss 6.285\n"
     ]
    }
   ],

   "source": [
    "logger_dict = posthoc_forward_pass(model,\n",
    "                                   linear_classifier, \n",
    "                                   DATASETS, \n",
    "                                   DATA_PATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Label files and store them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def merge_frames(frames, on_what=['file', 'true_labels'], how='left'):\n",
    "    merged_df = reduce(lambda left, right:pd.merge(left, right, on=on_what, how=how,  suffixes=('', '_drop')), frames)\n",
    "    merged_df.drop(merged_df.filter(regex='_drop$').columns.tolist(),axis=1, inplace=True)\n",
    "    return merged_df\n",
    "\n",
    "def get_merged_labels(datasets=DATASETS, datasets_types=['train', 'validation'], datasets_paths=DATA_PATHS, save_path=BASE_POSTHOC_PATH, get_df_dict = False):\n",
    "    df_data_types = {}\n",
    "    df_data = {}\n",
    "    for tv in datasets_types:\n",
    "        df_data[tv] = {}\n",
    "        for ds in datasets:\n",
    "            ds_dict = datasets_paths[ds]\n",
    "            df_data[tv][ds] = pd.read_csv(ds_dict['posthoc'][tv]['label'])\n",
    "            df_data[tv][ds].rename(columns = {'pred_labels': ds+'_pred'}, inplace = True)\n",
    "            if ds != 'ori':\n",
    "                df_data[tv][ds] = pd.merge(df_data[tv][ds], df_data[tv]['ori'], on=['file', 'true_labels'], how='left')\n",
    "            df_data[tv][ds].to_csv(Path(BASE_POSTHOC_PATH, ds, tv, 'labels_merged.csv'), sep=\",\", index=None)\n",
    "        df_data_types[tv] = merge_frames(df_data[tv].values())\n",
    "        if save_path is not None:\n",
    "            df_data_types[tv].to_csv(Path(save_path,tv+'.csv'), sep=\",\", index=None)\n",
    "    if get_df_dict:\n",
    "        return df_data_types, df_data\n",
    "\n",
    "    return df_data_types\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_types, df_data = get_merged_labels(get_df_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cw:\n",
      "    train:\n",
      "        total data:             32181\n",
      "        correct pred:           31848,   0.9896522792952364\n",
      "        incorrect adv pred:     32145,   0.9988813274913769\n",
      "        number adv tuples:      31812,   0.9885336067866132\n",
      "    validation:\n",
      "        total data:             1250\n",
      "        correct pred:           1215,   0.972\n",
      "        incorrect adv pred:     1249,   0.9992\n",
      "        number adv tuples:      1214,   0.9712\n",
      "\n",
      "fgsm_06:\n",
      "    train:\n",
      "        total data:             32181\n",
      "        correct pred:           31848,   0.9896522792952364\n",
      "        incorrect adv pred:     29355,   0.9121842080730866\n",
      "        number adv tuples:      29023,   0.9018675616046735\n",
      "    validation:\n",
      "        total data:             1250\n",
      "        correct pred:           1215,   0.972\n",
      "        incorrect adv pred:     1133,   0.9064\n",
      "        number adv tuples:      1098,   0.8784\n",
      "\n",
      "pgd_03:\n",
      "    train:\n",
      "        total data:             32181\n",
      "        correct pred:           31848,   0.9896522792952364\n",
      "        incorrect adv pred:     32181,   1.0\n",
      "        number adv tuples:      31848,   0.9896522792952364\n",
      "    validation:\n",
      "        total data:             1250\n",
      "        correct pred:           1215,   0.972\n",
      "        incorrect adv pred:     1250,   1.0\n",
      "        number adv tuples:      1215,   0.972\n"
     ]
    }
   ],
   "source": [
    "for name in ADV_DATASETS:   \n",
    "    print(f'''\\n{name}:''')\n",
    "    for tv in ['train', 'validation']: \n",
    "        print(f'    {tv}:')\n",
    "        df = df_data[tv][name]\n",
    "\n",
    "        ldf = len(df)\n",
    "        print(f'''        total data:             {ldf}''')\n",
    "        print(f'''        correct pred:           {len(df[df['true_labels']==df['ori_pred']])},   {len(df[df['true_labels']==df['ori_pred']])/ldf}''')\n",
    "        print(f'''        incorrect adv pred:     {len(df[df['true_labels']!=df[name+'_pred']])},   {len(df[df['true_labels']!=df[name+'_pred']])/ldf}''')\n",
    "        df_f = df[df['true_labels']==df['ori_pred']]\n",
    "        print(f'''        number adv tuples:      {len(df_f[df_f['true_labels']!=df_f[name+'_pred']])},   {len(df_f[df_f['true_labels']!=df_f[name+'_pred']])/ldf}''')\n"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
