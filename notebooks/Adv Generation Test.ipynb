{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import PIL\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# allow imports when running script from within project dir\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "\n",
    "# local\n",
    "from src.helpers.argparser import parser\n",
    "from src.model.dino_model import get_dino, ViTWrapper\n",
    "from src.model.data import *\n",
    "\n",
    "# Custom imports\n",
    "import torchattacks\n",
    "from torchattacks import *\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as pth_transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torchvision.models.resnet import ResNet, Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, dim, num_labels=1000):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.linear = nn.Linear(dim, num_labels)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # linear layer\n",
    "        return self.linear(x)\n",
    "    \n",
    "class CustomResNet(ResNet):\n",
    "    def __init__(self, classifier=None):\n",
    "        super(CustomResNet, self).__init__(block=Bottleneck, layers=[3, 4, 6, 3])\n",
    "        self.load_state_dict(torch.load(\"/cluster/scratch/jrando/resnet/resnet.pth\"))\n",
    "        del self.fc\n",
    "        \n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def _forward_impl(self, x):\n",
    "        # Normalize\n",
    "        transform = pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        x = transform(x)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        if self.classifier:\n",
    "            x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path('/cluster/scratch/thobauma/data/')\n",
    "ORI = DATA/'ori/filtered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set\n",
      "saving predictions to: /cluster/scratch/jrando/delete/adv/pgd_01/validation/labels.csv\n",
      "saving output tensors to: /cluster/scratch/jrando/delete/adv/pgd_01/validation/images\n",
      "\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([2], device='cuda:0')\n",
      "saving at /cluster/scratch/jrando/delete/adv/pgd_01/validation/images/ILSVRC2012_val_00031506\n",
      "Total elapsed time (sec): 0.12\n",
      "Accuracy against attack: 0.00 %\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = ORI/'train'\n",
    "VALIDATION_PATH = ORI/'validation'\n",
    "\n",
    "linear_classifier = LinearClassifier(2048, 9)\n",
    "linear_classifier.load_state_dict(torch.load(\"/cluster/scratch/thobauma/data/models/resnet/checkpoint.pth.tar\")['state_dict'])\n",
    "linear_classifier.to('cuda')\n",
    "\n",
    "model = CustomResNet(classifier=linear_classifier).to('cuda')\n",
    "\n",
    "atk = PGD(model, eps=0.01, alpha=(0.001*2)/3, steps=3)\n",
    "name='pgd_01'\n",
    "\n",
    "STORE_PATH = Path('/cluster/scratch/jrando/delete', 'adv', name)\n",
    "\n",
    "print('Validation set')\n",
    "\n",
    "val_dataset = AdvTrainingImageDataset(VALIDATION_PATH/'images', \n",
    "                              VALIDATION_PATH/'labels.csv', \n",
    "                              ORIGINAL_TRANSFORM, index_subset=[1000])\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "STORE_LABEL_PATH = Path(STORE_PATH, 'validation', 'labels.csv')\n",
    "STORE_LABEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "STORE_IMAGES_PATH = Path(STORE_PATH, 'validation', 'images')\n",
    "STORE_IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "correct = 0\n",
    "start = time.time()\n",
    "true_labels = []\n",
    "adv_labels = []\n",
    "names = []\n",
    "store_images = []\n",
    "\n",
    "print(f'''saving predictions to: {STORE_LABEL_PATH}''')\n",
    "print(f'''saving output tensors to: {STORE_IMAGES_PATH}\\n''')\n",
    "\n",
    "for images, labels, img_names in tqdm(val_loader):\n",
    "    images.cuda(non_blocking=True)\n",
    "    labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "    adv_images = atk(images, labels)\n",
    "    store_images.append(adv_images)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(adv_images)\n",
    "\n",
    "    _, pre = torch.max(outputs.data, 1)\n",
    "    print(pre)\n",
    "    print(labels)\n",
    "\n",
    "    correct += (pre == labels).sum()\n",
    "\n",
    "    for adv_img, img_name in zip(adv_images, img_names):\n",
    "        print(f\"saving at {Path(STORE_IMAGES_PATH, img_name)}\")\n",
    "        save_image(adv_img, fp=Path(STORE_IMAGES_PATH, img_name), format= \"PNG\")\n",
    "\n",
    "    true_labels.extend(labels.detach().cpu().tolist())\n",
    "    adv_labels.extend(pre.detach().cpu().tolist())\n",
    "    names.extend(img_names)\n",
    "\n",
    "print('Total elapsed time (sec): %.2f' % (time.time() - start))\n",
    "print('Accuracy against attack: %.2f %%' % (100 * float(correct) / len(val_loader.dataset)))\n",
    "\n",
    "data_dict = {'image': names, 'reduced_label':true_labels, name+'_pred':adv_labels}\n",
    "df = pd.DataFrame(data_dict)\n",
    "df['image'] = df['image'].str.split('.').str[0]\n",
    "df.to_csv(STORE_LABEL_PATH, sep=\",\")\n",
    "print(f'''\\n''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./test.tch\", \"wb\") as f:\n",
    "    torch.save(store_images[0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.6438, 0.6368, 0.6346,  ..., 0.5662, 0.5248, 0.5582],\n",
       "          [0.6324, 0.6245, 0.6232,  ..., 0.5967, 0.5624, 0.5740],\n",
       "          [0.6143, 0.6273, 0.6400,  ..., 0.6794, 0.6571, 0.6290],\n",
       "          ...,\n",
       "          [0.7486, 0.6650, 0.7241,  ..., 0.7032, 0.7551, 0.7406],\n",
       "          [0.7795, 0.7402, 0.7555,  ..., 0.8044, 0.8162, 0.7921],\n",
       "          [0.8449, 0.8394, 0.8566,  ..., 0.6850, 0.7608, 0.7440]],\n",
       "\n",
       "         [[0.5090, 0.5128, 0.4981,  ..., 0.4303, 0.4243, 0.4477],\n",
       "          [0.4896, 0.4817, 0.4798,  ..., 0.4732, 0.4364, 0.4574],\n",
       "          [0.4757, 0.4730, 0.4896,  ..., 0.5701, 0.5355, 0.5110],\n",
       "          ...,\n",
       "          [0.6368, 0.5626, 0.6127,  ..., 0.6093, 0.6626, 0.6714],\n",
       "          [0.6819, 0.6292, 0.6494,  ..., 0.6980, 0.7133, 0.6917],\n",
       "          [0.7620, 0.7503, 0.7666,  ..., 0.5553, 0.6292, 0.6341]],\n",
       "\n",
       "         [[0.5105, 0.5047, 0.4841,  ..., 0.4274, 0.4178, 0.4316],\n",
       "          [0.5004, 0.4853, 0.4833,  ..., 0.4658, 0.4410, 0.4343],\n",
       "          [0.4622, 0.4731, 0.4859,  ..., 0.5691, 0.5169, 0.4927],\n",
       "          ...,\n",
       "          [0.5974, 0.5416, 0.5894,  ..., 0.5763, 0.6645, 0.6852],\n",
       "          [0.6776, 0.6273, 0.6395,  ..., 0.6661, 0.7008, 0.6885],\n",
       "          [0.7629, 0.7617, 0.7744,  ..., 0.5246, 0.6110, 0.6375]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"/cluster/scratch/jrando/delete/adv/pgd_01/validation/images/ILSVRC2012_val_00031506\")\n",
    "img = img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONLY_NORMALIZE_TRANSFORM = pth_transforms.Compose([\n",
    "                                            pth_transforms.ToTensor(),\n",
    "                                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ONLY_NORMALIZE_TRANSFORM(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7783,  0.6789,  0.6403,  0.3051, -0.2316,  0.3395, -0.6651, -0.3711,\n",
       "         -0.8962]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test.unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /cluster/scratch/jrando/delete/adv/pgd_01/validation/images/ILSVRC2012_val_00032854 ./test_ong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
